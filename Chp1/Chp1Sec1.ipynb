{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a256ce-c27f-477e-b107-dd55bc2a7cb0",
   "metadata": {},
   "source": [
    "# Introduction to Partial Differential Equations\n",
    "---\n",
    "\n",
    "## Chapter 1: Preliminaries (Calculus, Linear Algebra, ODEs, and Python)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636d72a6-a6de-4621-8b3d-419f14adbf60",
   "metadata": {},
   "source": [
    "## Creative Commons License Information\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/80x15.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">Introduction to Partial Differential Equations: Theory and Computations</span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">Troy Butler</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>.<br />Based on a work at <a xmlns:dct=\"http://purl.org/dc/terms/\" href=\"https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations\" rel=\"dct:source\">https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540575c3-e170-4365-bff2-7836c2d2fde2",
   "metadata": {},
   "source": [
    "## Section 1.1: Calculus, Symbolic Computations, and Manufactured Solutions\n",
    "---\n",
    "\n",
    "This course makes extensive use of prerequisite knowledge from\n",
    "\n",
    "- [Multivariable Calculus](#calculus)\n",
    "  \n",
    "> This is what puts the \"partial\" in partial differential equations (PDEs). This is the focus of the current notebook.\n",
    "\n",
    "- (Basics of) Ordinary Differential Equations (ODEs)\n",
    "\n",
    "> This is fundamental to understanding numerical time-stepping procedures for spatial-temporal PDEs. We review important aspects of this in the [Chp1Sec3](Chp1Sec3.ipynb) and [Chp1Sec4](Chp1Sec4.ipynb) notebooks.\n",
    "\n",
    "3. (Basics of) Linear Algebra \n",
    "\n",
    "> We provide review in the [Chp1Sec5](Chp1Sec5.ipynb) notebook, which gives students more useful information related to Python and specifically using `numpy` and its linear algebra subpackage. Linear algebra provides many useful perspectives in terms of linear operators acting on vector spaces that motivate many conversations we will have throughout this course. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91408b9-c46a-4d2e-b72b-a0598d4f7855",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id=\"calculus\">Section 1.1.1: Summary of important differential calculus concepts</a>\n",
    "---\n",
    "\n",
    "- [Partial Derivatives (first, higher order)](https://en.wikipedia.org/wiki/Partial_derivative) are at the heart of PDEs. \n",
    "\n",
    "  These allow us to define and understand what a partial differential equation is and to verify a given function is in fact a solution by \"plugging it into\" the equation. This is particularly crucial when we verify code is performing as expected through the [**method of manufactured solutions**](#manufactured) solutions. \n",
    "\n",
    "Some notation review: \n",
    "\n",
    "- If $u:\\mathbb{R}^3\\to\\mathbb{R}$, we often write $u(x,y,z)$ and its first-order partial derivatives are often denoted by $\\partial_x u, \\partial_y u, $ and $\\partial_z u$ or more compactly as $u_x$, $u_y$, $u_z$. Other notations are also common (e.g., see https://en.wikipedia.org/wiki/Partial_derivative).\n",
    "\n",
    "- The [del or nabla](https://en.wikipedia.org/wiki/Del) operator, $\\nabla$, is a vector differential operator applied to scalar-valued or vector-valued functions: $\\nabla u$ (defining the gradient), $\\nabla \\times \\vec{u}$ (defining the curl), and $\\nabla \\cdot \\vec{u}$ (defining the divergence) where $u$ is a scalar-valued function and $\\vec{u}$ is a vector-valued function. We do not use the curl in this class, so we summarize only the gradient and divergence below.\n",
    "\n",
    "  - On $\\mathbb{R}^n$, the differential operator $\\nabla$ and [gradient](https://en.wikipedia.org/wiki/Gradient) of $u:\\mathbb{R}^n\\to\\mathbb{R}$ are defined as\n",
    "<br><br>\n",
    "$$\n",
    "    \\nabla := \\begin{bmatrix}\n",
    "                \\partial_{x_1} \\\\\n",
    "                \\partial_{x_2} \\\\\n",
    "                \\vdots \\\\\n",
    "                \\partial_{x_n}\n",
    "                \\end{bmatrix}, \\ \\text{ and } \\ \n",
    "      \\nabla u := \\left(\\begin{array}{c}\n",
    "                \\partial_{x_1}u \\\\\n",
    "                \\partial_{x_2}u \\\\\n",
    "                \\vdots \\\\\n",
    "                \\partial_{x_n}u\n",
    "                \\end{array}\\right), \\ \\text{respectively.}\n",
    "$$\n",
    "<br><br>\n",
    "  - For $\\vec{u}:\\mathbb{R}^n\\to\\mathbb{R}^n$, its [divergence](https://en.wikipedia.org/wiki/Divergence) is given by\n",
    "<br><br>\n",
    "$$\n",
    "    \\nabla \\cdot \\vec{u} :=  \\sum_{i=1}^n \\partial_{x_i}\\vec{u}_i\n",
    "$$\n",
    "<br><br>\n",
    "    where  $\\vec{u}_i$ denotes the $i$th component of the vector $\\vec{u}$.\n",
    "<br><br>\n",
    "\n",
    "- [Taylor's theorem](https://en.wikipedia.org/wiki/Taylor%27s_theorem) allows us to understand why finite difference schemes are valid numerical schemes for approximating solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51eac43-1059-42fc-b101-af635bbc6ffe",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.1.2: Symbolic differentiation in Python\n",
    "---\n",
    "\n",
    "We make use of the [`sympy`](https://www.sympy.org/en/index.html) package in Python to symbolically differentiate/integrate and manipulate functions via simplification or factoring routines. \n",
    "\n",
    "This allows us to (1) check any of our by-hand work, and (2) get results quicker!\n",
    "\n",
    "Below, we focus just on the differentiation aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d980450-d436-4584-9cf0-e383dc50b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp  # First we need to import sympy, we use the standard sp abbreviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e058f07-a324-4ab3-9c1f-710cb63bb010",
   "metadata": {},
   "source": [
    "Below we create symbolic variables `x` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e433e87-2385-4a17-a10a-20c7002019d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sp.symbols('x, y')  # Creating x and y as symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e59b18-d0a7-4dfd-9a9a-353c5bb1247f",
   "metadata": {},
   "source": [
    "Now we use symbolic functions available within `sympy` to create the symbolic function for $$u=e^{-x^2}\\sin(y) + x^2 - y + xy.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cd2e58-753f-4c83-8589-5eed884a418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = sp.exp(-x**2) * sp.sin(y) + x**2 - y + x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf4396-c0ce-4637-be57-c1ffd34057a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "u  # The standard output for a symbolic function is \"pretty\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5b35d7-8c2a-4dcf-812f-a21be15c45b2",
   "metadata": {},
   "source": [
    "We now use sympy to compute the first-order partial derivatives of u with respect to $x$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7115e3f2-0999-4160-a8d8-b74b72744ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "u.diff(x,1)  # The x indicates the variable to differentiate with respect to and 1 indicates the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8037103d-3c0b-4983-a7e2-d002262413f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "u.diff(y,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0338cc90-4cc7-43c4-aa68-9d600425e835",
   "metadata": {},
   "source": [
    "Higher order derivative computations are just as easily computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e1399-005a-416d-830c-7b3ab7bd37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "u.diff(x,5)  # This computes a 5th order derivative. Who wants to do this by hand?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef00eefe-d5bc-4a25-b64c-6abecbbd94cc",
   "metadata": {},
   "source": [
    "Since a vector is a type of matrix, we can use symbolic matrices to represent gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e90fc-d4e9-42b7-9312-e154acf073d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_grad = sp.Matrix([u.diff(x,1), u.diff(y,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bad51a-e9e3-4307-a756-3e7998cd7565",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a2cb16-7118-4aec-9120-f7cc2193eaad",
   "metadata": {},
   "source": [
    "`sympy` matrices have a Jacobian method attribute that can also be used to produce the gradient as follows. Note the use of the transpose at the end since a Jacobian computes a matrix of first-order derivatives for a vector-valued function where each row is the gradient of the corresponding component in the vector-valued function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbdcf90-17a0-4f59-8192-93abeba5c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.Matrix([u]).jacobian(sp.Matrix([x, y])).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5f57be-a123-4357-a48f-3bd475b3b640",
   "metadata": {},
   "source": [
    "#### Some special characters\n",
    "\n",
    "What about Greek letters used as coefficients or their own variables? Luckily, there is the [`abc`](https://docs.sympy.org/latest/modules/abc.html) module within `sympy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5367980c-abf6-4566-8111-34eb893408bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy.abc import pi, rho, alpha, kappa, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58d30bf-f345-4077-9bcc-874896773555",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = sp.exp(-kappa*x**2) * sp.sin(pi*y) + rho*x**2 - alpha*y + beta*x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f20af-00e5-4cef-948d-d1ce385f39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc8762-f08f-4037-81c1-a3dc96635961",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.Matrix([v]).jacobian(sp.Matrix([x, y])).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3965eb2-76ce-40b3-b897-c8695354e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.Matrix([v]).jacobian(sp.Matrix([x, y, pi, rho, alpha, kappa, beta])).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52912c-e809-4042-883e-ad1b30871bf1",
   "metadata": {},
   "source": [
    "---\n",
    "#### Student activity\n",
    "---\n",
    "\n",
    "Consider the following two functions\n",
    "\n",
    "$$\n",
    "    f(x,y,z) = xy^2z^3 \\ \\text{ and } \\ g(x,y,z) =  \\sin(x)\\cos(y)\\tan(z) - 3^x.\n",
    "$$\n",
    "\n",
    "Use `sympy` to create symbolic functions for the above two functions and compute their first-order partial derivatives as well as their gradients. Several blank code cells are below to get you started, but students should feel free to make more as needed as well as adding Markdown cells for notes.\n",
    "\n",
    "*Hint: First you will need to create a new symbolic variable z.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff1e444-96d7-451c-8bf3-6e487904d680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbca0ad-90bd-4da5-846e-1facbc94c9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab76f77-c017-41c8-8129-c0fa41a57d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17da2d81-2770-405e-a72f-222940ea71d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c67c635b-28bd-4818-beec-f7b4dc6f21cb",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.1.3: The Laplacian\n",
    "---\n",
    "\n",
    "The **Laplace operator** (often referred to simply as the **Laplacian**) is denoted by $\\Delta$ and defined on $\\mathbb{R}^n$ by\n",
    "$$\n",
    "\\Delta := \\nabla^2 := \\nabla\\cdot \\nabla= \\sum_{i=1}^n \\partial_{x_i}^2.\n",
    "$$\n",
    "\n",
    "From this definition, it follows that\n",
    "\n",
    "$$\n",
    "    \\nabla \\cdot \\nabla u = \\nabla \\cdot (\\nabla u)= \\Delta u.\n",
    "$$\n",
    "\n",
    "In other words, the Laplacian of a scalar-valued function $u:\\mathbb{R}^n\\to\\mathbb{R}$ is defined by the divergence of the gradient of $u$.\n",
    "\n",
    "Let's take the Laplacian of the `u` defined as a symbolic function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822da92-87fb-4bda-902f-d5b933c68d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = sp.exp(-x**2) * sp.sin(y) + x**2 - y + x*y  # This is just in case we edited u above. It is not necessary to redefine u everytime we want to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1f621-4d5d-4f7b-a0fd-63552678f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Del_u = u.diff(x,2) + u.diff(y,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa4193-4a6c-4341-883e-e258c15b3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Del_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec305804-f8d1-41bc-a854-a079f272f52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A slick way (good for high dimensions)\n",
    "\n",
    "Del_u = 0  # initialize the Del_u function as a 0\n",
    "for var in [x, y]:  # loop through each variable to differentiate\n",
    "    Del_u += u.diff(var, 2)  # add the second derivative of u with respect to given variable to Del_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29472207-8d26-497e-be28-f1f33ba43bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Del_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c7aeba-9793-405c-b23a-7d937d95a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = sp.exp(-kappa*x**2) * sp.sin(pi*y) + rho*x**2 - alpha*y + beta*x*y  # Just in case v was redefined above when exploring. Not necessary otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3dbcda-c331-4326-9277-46251a813fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Del_v = 0\n",
    "for var in [x, y, rho, beta, alpha, kappa]:\n",
    "    Del_v += v.diff(var, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c2588a-56b1-4b95-b1f3-81e52bc06b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "Del_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20dfe1c-8752-48f3-a68a-24af6b415fa8",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.1.4: Our first manufactured solution\n",
    "---\n",
    "\n",
    "In Chapter 2, we will consider solutions to Poisson's problem defined by $-\\Delta u = f$ on a given domain with some specified boundary conditions.\n",
    "\n",
    "For now, we remove the complication of the boundary conditions so that we focus just on $-\\Delta u=f$ being satisfied on $\\mathbb{R}^n$ for some specified $n$.\n",
    "\n",
    "In general, we use numerical methods to solve PDEs. By \"solve\" we mean produce a numerical estimate to $u$ at some set of points discretizing the domain. We do this because in general we cannot produce closed-form solutions to PDEs and must rely on numerical methods. However, we want to be able to *test* code under conditions where we *know* what the solutions are to make sure that the code is producing reasonable approximations to these known solutions. If this is not the case, then we would certainly *not* trust the code to produce useful estimates of solutions in cases where we have no idea what the solution is!\n",
    "\n",
    "This leads us to what is commonly referred to as the ***method of manufactured solutions.***\n",
    "\n",
    "The idea is quite simple. We want to solve a problem of a particular type such as $-\\Delta u = f$ numerically. The \"data\" of the problem (here, the $f$) will change in each particular instance, and the \"solution\" of the problem (here, the $u$) and its numerical approximation will subsequently change as the data varies from instance to instance. To verify the code is working properly via the method of manufactured solutions, we *start* with a solution. This may strange, but it is quite intuitive. Pick a $u$, any $u$, and as long as it satisfies any constraints of the problem (e.g., boundary conditions), you can then plug it into the differential equation to figure out what $f$ would have produced such a $u$. Now, you know the data $f$ that should be \"fed\" into the code to check if it produces a numerical approximation to the $u$ that you manufactured. That is it. This is the method.\n",
    "\n",
    "Sometimes it is easier said than done because the constraints of the problem (e.g., boundary and initial conditions) may seemingly force you to \"throw\" away the function $u$ you wanted to use. We will see how to deal with this in Chapter 2 by simply \"adjusting\" the $u$ (usually through the inclusion of a linear function to adjust its boundary values) to create a function that does serve as a manufactured solution.\n",
    "\n",
    "This illustrates a good use of a symbolic toolbox because we can ensure that all the derivative computations are done correctly (as long as we coded them correctly). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff5f5f-0fd4-4e9e-bca7-d82a9f5fa659",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = - Del_u  # The process of defining the data associated with a manufactured solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63686c5-4f3f-42ff-9ecd-6da3affd5039",
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9640b7b7-4647-49da-83a5-d57e04bb7cba",
   "metadata": {},
   "source": [
    "Yes, it was as easy as what was done above. Now, we know exactly what function $f$ to put into any purported solver for $-\\Delta u =f$ on $\\mathbb{R}^2$. We can then check the output of the purported solver against what we know $u$ to be.\n",
    "\n",
    "This is also *incredibly useful* when verifying rates of convergence of numerical methods (something we will study in the [Chp1Sec2](Chp1Sec2.ipynb) notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc1010d-ec32-4328-8a5a-c6b75c020320",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.1.5: Some more `sympy` fun\n",
    "---\n",
    "\n",
    "`sympy` also has an [`integrals` module](https://docs.sympy.org/latest/modules/integrals/integrals.html) for computing indefinite and definite integrals as well as integral transforms (such as Laplace and Fourier transforms). \n",
    "\n",
    "We show some examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ceaa9-f409-434b-8a49-4cffe39c76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that indefinite integrals do not include the constant of integration \n",
    "\n",
    "sp.integrate(x**2 + x*y - y, x)  # Determines an antiderivative of the function $x^2+xy-y$ with respect to x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08da959-45a1-41bb-9353-efcc16152e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.integrate(x**2 + x*y - y, y)  # Integrates with respect to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56611fd-4dab-47ef-9d75-924e2c24c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.integrate(x**2 + x*y - y, (x, 0, 1))  # A definite integral, with respect to x, from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7805b7d9-1e69-435a-93fe-f262b6558a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about an iterated integral?\n",
    "\n",
    "sp.integrate(sp.integrate(x**2 + x*y - y, (x, 0, 1)), (y, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e943c4-6f45-4b85-b664-d23ce7f057a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating an iterated integral via a for-loop\n",
    "\n",
    "f = x**2 + x*y - y\n",
    "integrand = f\n",
    "for var, limits in zip([x,y], [(0,1), (2,3)]):\n",
    "    integrand = sp.integrate(integrand, (var, limits[0], limits[1]))  \n",
    "\n",
    "# The integral is the final integrand\n",
    "integral = integrand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e34789-2498-4613-88b7-89591072977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "integral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f41f1ef-5752-474a-89da-289c7bd3b5cc",
   "metadata": {},
   "source": [
    "---\n",
    "#### Navigation:\n",
    "\n",
    "- [Previous](Chp1Sec0.ipynb)\n",
    "\n",
    "- [Next](Chp1Sec2.ipynb)\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
