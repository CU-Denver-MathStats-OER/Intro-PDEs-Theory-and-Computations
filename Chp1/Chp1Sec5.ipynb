{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd25bd22-7547-4b00-88c3-1dfa93970212",
   "metadata": {},
   "source": [
    "# Introduction to Partial Differential Equations\n",
    "---\n",
    "\n",
    "## Chapter 1: Preliminaries (Calculus, Linear Algebra, ODEs, and Python)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de98101-2944-4b6a-84e0-6fc3dbbbdf34",
   "metadata": {},
   "source": [
    "## Want to use Colab? [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations/blob/main/Chp1/Chp1Sec5.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93232e-4d07-487c-83c5-5e6302878cc1",
   "metadata": {},
   "source": [
    "## Prepping the environment for interactive plots in Colab\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bca151-4031-4058-91b3-c9b523d1fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab - installing missing packages')\n",
    "    !pip install ipympl\n",
    "    from IPython.display import clear_output\n",
    "    clear_output()\n",
    "    exit()\n",
    "else:\n",
    "    print('Not running on CoLab - assuming environment has necessary packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48710945-f73a-4efc-bef5-5bba438afad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1857772e-72b7-4ac3-9786-198aa0f8b6bf",
   "metadata": {},
   "source": [
    "## Creative Commons License Information\n",
    "---\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/80x15.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">Introduction to Partial Differential Equations: Theory and Computations</span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">Troy Butler</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>.<br />Based on a work at <a xmlns:dct=\"http://purl.org/dc/terms/\" href=\"https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations\" rel=\"dct:source\">https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5280af96-8274-402a-a5b1-24973fb22799",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1.5: Linear Algebra and Differential Equations\n",
    "---\n",
    "\n",
    "As we come to the end of Chapter 1, we make some important connections between linear algebra and differential equations that serve us well throughout the rest of this course. Along the way, we review some useful linear algebra concepts and how to use the [`linalg` sublibrary of `numpy`](https://numpy.org/doc/stable/reference/routines.linalg.html) to perform some useful calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77eaca1-b1b2-4654-a74b-e239f097f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import matrix_rank\n",
    "import numpy.linalg as linalg  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bae3f1a-6fdc-462c-8ca5-2696520c026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb918fa3-ded5-4fa9-b56d-bd46cf81c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfc33d2-4b2d-4402-af1e-93d980e89266",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.5.1: A high-level overview of linear algebra concepts\n",
    "---\n",
    "\n",
    "Note that we could allow the vectors and matrices discussed below to have complex components, but we are limiting ourselves to real spaces for now. \n",
    "The extension to complex spaces is straightforward in most cases since $\\mathbb{C}$ is [isometrically isomorphic](https://en.wikipedia.org/wiki/Isometry) to $\\mathbb{R}^2$ (i.e., they are \"basically the same\" and the more significant differences arise when considering behavior of functions on these spaces). \n",
    "\n",
    "The primary focus is on reviewing linear algebra properties related to the [Banach space](https://en.wikipedia.org/wiki/Banach_space) $\\mathbb{R}^n$ (recall that a Banach space is a complete normed linear space). However, we will also discuss/present ideas in the context of some polynomial spaces of finite order (representing a finite-dimensional function space) and also make a few connections to useful perspectives we take when studying PDEs throughout this course. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beec242a-79c6-4bde-93ed-9214f092f070",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.5.2: Linear Independence\n",
    "--- \n",
    "\n",
    "A set of vectors $V=\\{v_i\\}_{i=1}^k\\subset\\mathbb{R}^n$ is **linearly independent** if the only linear combination that produces the $n$-dimensional zero vector is the trivial linear combination, i.e., for $\\{c_i\\}_{i=1}^k\\subset\\mathbb{R}$\n",
    "\n",
    "$$\n",
    "    \\sum_{i=1}^k c_i v_i = 0\\in\\mathbb{R}^n \\Longleftrightarrow c_i = 0 \\ \\forall 1\\leq i\\leq k.\n",
    "$$\n",
    "\n",
    "Otherwise, the set of vectors is **linearly dependent**.\n",
    "\n",
    "Other important concepts related to a set of vectors $V$ are\n",
    "\n",
    "  - The **span** of a set of vectors $V$ is a subspace of $\\mathbb{R}^n$ defined by the set of all possible linear combinations of vectors in $V$, i.e.,\n",
    "  <br><br>\n",
    "  $$\n",
    "      \\operatorname{span} V := \\left\\{\\sum_{i=1}^k c_i v_k \\in\\mathbb{R}^n \\, : \\, c_i\\in\\mathbb{R} \\ \\forall 1\\leq i\\leq k\\right\\}.\n",
    "  $$\n",
    "  <br><br>\n",
    "  - A linearly independent spanning set for a subspace is called a **basis**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cdaca4-cc7f-4747-9107-612af96c2320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The span of a single vector in $\\mathbb{R}^2$ is a vector subspace conceptualized\n",
    "# by a line through the origin pointing in both the positive and negative directions\n",
    "# of the vector. Any vector that \"lays\" upon this line can be used as a basis for\n",
    "# this subspace.\n",
    "def plot_span_vector_2d(v_x, v_y, xlim=[-5, 5], ylim=[-5, 5], fignum=0):\n",
    "    plt.figure(fignum)\n",
    "    plt.clf()\n",
    "    \n",
    "    # Define a range of constant multiples of the vector\n",
    "    cs = np.linspace(-50, 50, 100)\n",
    "    # Plot the span\n",
    "    plt.plot(cs*v_x, cs*v_y)  \n",
    "    # Plot the vector\n",
    "    plt.arrow(0, 0, v_x, v_y, \n",
    "              head_width=0.25, width=0.1, color='r', alpha=1)\n",
    "    \n",
    "    # Various plotting properties\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.axvline(0, ls=':', c='k')\n",
    "    plt.axhline(0, ls=':', c='k')\n",
    "    plt.show()\n",
    "    plt.tight_layout()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eda0561-fe38-4234-824c-02a00238d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out \n",
    "\n",
    "%matplotlib widget\n",
    "interact_manual(plot_span_vector_2d, \n",
    "                v_x = widgets.BoundedFloatText(value=-2, \n",
    "                                               min=-2,\n",
    "                                               max=2,\n",
    "                                               step=0.1,\n",
    "                                               description='x-direction of vector'),\n",
    "                v_y = widgets.BoundedFloatText(value=1, \n",
    "                                               min=-2,\n",
    "                                               max=2,\n",
    "                                               step=0.1,\n",
    "                                               description='y-direction of vector'),\n",
    "                xlim = fixed([-5, 5]),\n",
    "                ylim = fixed([-5, 5]),\n",
    "                fignum = fixed(0)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8dadc8-3301-4068-959e-288a65fa1134",
   "metadata": {},
   "source": [
    "---\n",
    "#### Theorem: Any collection of $n+1$ vectors in $\\mathbb{R}^n$ form a linearly dependent set.\n",
    "---\n",
    "\n",
    "**Proof:**\n",
    "Let $e_i$ denote the standard basis vectors in $\\mathbb{R}^n$ for $i=1,2,\\ldots, n$. \n",
    "\n",
    "Let $V=\\{v_1, v_2, \\ldots, v_{n+1}\\}$ denote a set of $n+1$ vectors from $\\mathbb{R}^n$. Assume that this set is linearly independent.\n",
    "\n",
    "For $v_1$, there exists constants $\\alpha_i$, for $i=1,2,\\ldots, n$, such that\n",
    "$$\n",
    "   \\large v_1 = \\sum_{i=1}^n \\alpha_ie_i.\n",
    "$$\n",
    "Assume without loss of generality that $\\alpha_1\\neq 0$. \n",
    "Then, \n",
    "$$\n",
    "    \\large e_1 = \\frac{1}{\\alpha_1}\\left(v_1 - \\sum_{i=2}^n \\alpha_i e_i\\right).\n",
    "$$\n",
    "This implies that $\\{v_1, e_2, e_3, \\ldots, e_n\\}$ spans $\\mathbb{R}^n$. \n",
    "Thus, for $v_2$, there exists (new) constants $\\alpha_i$, for $i=1,2,\\ldots, n$, such that\n",
    "$$\n",
    "    \\large v_2 = \\alpha_1 v_1 + \\sum_{i=2}^n \\alpha_i e_i.\n",
    "$$\n",
    "Since the set $V$ is linearly independent, we must have that at least one $\\alpha_i\\neq 0$ for an $i=2,3,\\ldots, n$. \n",
    "Again, without loss of generality, assume that $\\alpha_2 \\neq 0$, and as before arrive at the conclusion that $\\{v_1, v_2, e_3, e_4, \\ldots, e_n\\}$ is a spanning set for $\\mathbb{R}^n$.\n",
    "\n",
    "We can repeat this argument $n$ times until we have replaced each $e_i$ with an associated $v_i$ so that $\\{v_1,v_2,\\ldots, v_n\\}$ forms a spanning set for $\\mathbb{R}^n$.\n",
    "However, this immediately implies that $v_{n+1}$ is a linear combination of these vectors, which contradics the assumption of linear independence. $\\Box$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794805c2-5780-4057-a590-db0200077cac",
   "metadata": {},
   "source": [
    "---\n",
    "#### Spaces of polynomials are finite-dimensional vector spaces\n",
    "---\n",
    "\n",
    "Let $\\mathcal{P}_n$ denote the space of all real-valued polynomials of order less than or equal to $n$, i.e., \n",
    "\n",
    "$$\n",
    "    \\mathcal{P}_n := \\{ a_0 + a_1x + \\cdots + a_n x^n \\, : \\, a_i\\in\\mathbb{R}, 0\\leq i\\leq n\\}.\n",
    "$$\n",
    "\n",
    "A standard problem in linear algebra is to show that $\\mathcal{P}_n$ is an $(n+1)$-dimensional vector space. \n",
    "\n",
    "Each polynomial function is then viewed as a vector in the space, and a useful basis is given by the set of $n+1$ monomials defined by $V = \\{ 1, x, x^2, \\ldots, x^n\\}$ (note that $1 = x^0$). In other words, this is a set of linearly independent vectors whose span is $\\mathcal{P}_n$. \n",
    "\n",
    "For a specific example, consider $\\mathcal{P}_2$ and note $\\{1, x, x^2\\}$ is the smallest set needed to describe every single quadratic polynomial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c8790b-5f3b-47d2-8012-e3caac30e34c",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.5.3: Solvability of a linear system\n",
    "---\n",
    "\n",
    "Suppose $A\\in\\mathbb{R}^{k\\times n}$ and we are interested in the solvability of the linear system given by\n",
    "\n",
    "$$\n",
    "    Ax=b, \n",
    "$$\n",
    "\n",
    "where $x\\in\\mathbb{R}^n$ and $b\\in\\mathbb{R}^k$. We refer to the vector $b$ as the \"data\" and $x$ as the \"solution\" to this system. \n",
    "\n",
    "Let $\\operatorname{Cols}(A)$ denote the set of $k$-dimensional vectors defined by the columns of $A$, i.e., $\\operatorname{Cols}(A) = \\{ v_i\\in\\mathbb{R}^k\\, : \\, v_i = i\\text{th column of }A\\}$. \n",
    "\n",
    "The vector subspace of $\\mathbb{R}^k$ defined by $\\operatorname{span} \\operatorname{Cols}(A)$ is called the column space of $A$. \n",
    "\n",
    "- In order for a solution to $Ax=b$ to exist, we need $b\\in \\operatorname{span} \\operatorname{Cols}(A) $. In other words, $b$ must exist in the column space of $A$.\n",
    "\n",
    "- $\\operatorname{Cols}(A)$ may not form a basis to the space of admissible data (i.e., the data for which there exists a solution to the system) because the columns of $A$ may not be linearly independent. \n",
    "\n",
    "- The dimension of the column space is called the **rank** of the matrix.\n",
    "\n",
    "In this course, we focus primarily on linear differential equations and use $L$ to denote the linear differential operator. The differential equation is then written as $Lu=f$ where $f$ is the \"data\" and $u$ is the solution we are after. Note the immediate symmetry between the notation $Ax=b$ and $Lu=f$. When we use finite difference approximations in place of derivatives, $Lu=f$ is often rewritten as $Av=b$ where $v$ is a finite-dimensional vector with components representing the approximation to $u$ at a particular grid point in the spatial domain. The solvability of $Lu=f$ is often directly tied to the solvability of $Av=b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e9318-0aef-45f1-aa78-10093a7ecd93",
   "metadata": {},
   "source": [
    "Below, we give examples of square matrices defined in terms of vectors and explore the linear dependence or independence of these vectors by investigating the rank of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43020168-b00d-447b-b666-b3e91f8bb1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are linearly dependent vectors used to construct a matrix\n",
    "v1 = np.array([1, 2, 3])\n",
    "v2 = np.array([4, 5, 6])\n",
    "v3 = np.array([7, 8, 9])\n",
    "\n",
    "A = np.array([v1, v2, v3]).transpose()  # transpose interchanges rows and columns\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a205b8-6e41-4c8b-835d-9e5fc1620859",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_rank(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc23bb0-64c5-4cc6-a606-13f99460eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we will see in Chapter 2, the following vectors are inspired by a \n",
    "# centered finite difference approximation used to discretize $-u''=f$\n",
    "u1 = np.array([2, -1, 0])\n",
    "u2 = np.array([-1, 2, -1])\n",
    "u3 = np.array([0, -1, 2])\n",
    "\n",
    "A = np.array([u1, u2, u3]).transpose()\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4e7e54-c8a9-49ae-b8b0-a2ceb8b2ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix_rank(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55556a94-af05-4bca-9fce-e938a4699898",
   "metadata": {},
   "source": [
    "The above example hints at the fact that the way in which we discretize $-u''=f$ in this course should lead to a solvable linear system of equations because the subsequent matrix-vector equation defined by this discretization will involve a nonsingular matrix. We define these concepts below more carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528a247a-62fa-4ea9-9d15-c3e8d6b11e79",
   "metadata": {},
   "source": [
    "---\n",
    "#### Polynomial interpolation\n",
    "---\n",
    "\n",
    "Consider $\\mathcal{P}_1$, which is the space of all linear real-valued functions of the form $a_0+a_1x$. Recall that we can fit a line through any two points. Thus, if we are given points $\\{(x_0, y_0), (x_1, y_1)\\}$, we can seek the $a_0$ and $a_1$ defining the line that passes through these points by solving the linear system defined by\n",
    "\n",
    "$$\n",
    "    \\begin{pmatrix}\n",
    "        1 & x_0 \\\\\n",
    "        1 & x_1 \\\\\n",
    "    \\end{pmatrix}\n",
    "    \\begin{pmatrix}\n",
    "        a_0 \\\\\n",
    "        a_1\n",
    "    \\end{pmatrix} \n",
    "    =\n",
    "    \\begin{pmatrix}\n",
    "        y_0 \\\\\n",
    "        y_1\n",
    "    \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Note that as long as $x_0\\neq x_1$, the matrix above will always have rank 2 because the two columns will always be linearly independent. Moreover, the column space will always be $\\mathbb{R}^2$ as long as $x_0\\neq x_1$, which implies we can always find a solution to the system. This is consistent with our recollection that we can fit a line through any two points. \n",
    "\n",
    "What if $x_0=x_1$? \n",
    "\n",
    "- The matrix will have rank 1 because the two columns are now linearly dependent (the second column will just be equal to $x_0$ times the first column in this case). One of two things will happen as a result. \n",
    "\n",
    "  - If $y_0=y_1$, then there are an infinite number of solutions because there are an infinite number of linear functions that pass through the point $(x_0,y_0)$. \n",
    "  \n",
    "  - However, if $y_0\\neq y_1$, then the data vector is no longer in the span of the column space of the matrix, so there is *no* solution. While you may object to this since you can envision a *vertical* line through the points $(x_0,y_0)$ and $(x_0, y_1)$, you need to recall that this is *not* a function (it fails the aptly named \"vertical line test\") and does not belong to the space $\\mathcal{P}_1$.\n",
    "  \n",
    "We can of course generalize the above ideas to $\\mathcal{P}_n$ where any set of $n+1$ points in $\\mathbb{R}^2$ with distinct $x$-coordinates can be used to uniquely define a polynomial of order $n$ that interpolates (i.e., \"passes through\") this data (notice how the $y$-coordinates of these points do in fact define the so-called \"data\" vector). It is important to note that if $n\\geq 2$, it is possible that the function in $\\mathcal{P}_n$ that interpolates a set of $n+1$ data points is given by a polynomial of order $k<n$. For example, suppose $n=10$ but the $11$ points we are given all lay upon a *horizontal* line, then $a_1=a_2=\\cdots=a_{10}=0$ and only $a_0$ has the potential to be non-zero (it will be equal to whatever the $y$-values of the data points are, which must all agree if the data do in fact lay upon a horizontal line). \n",
    "\n",
    "Why bring this up? Well, certain types of data that are \"restricted\" (in a sense) often lead to solutions that are themselves restricted to a particular smaller-dimensional subspace of the vector space defining all possible solutions to the problem. This matters in PDEs where the problem $Lu=f$ may be investigated for a class of data $f$ parameterized in some way (e.g., to model typically expected variations in source terms one will encounter in a laboratory or field setting). \n",
    "\n",
    "One final note: What if we are given more than $n+1$ data points for determining a function in $\\mathcal{P}_n$ that interpolates this data? Again assuming that the $x$-coordinates are all distinct, then there are two cases. The solution will either not exist (there is no interpolant) or a unique solution will exist in which case more data are given than is necessary (the problem is solvable but over-determined). The rank of the matrix we form cannot grow beyond $n+1$ because there are only $n+1$ columns no matter how many data points we consider. However, we often turn to *regression* to find a polynomial of *best fit* defined by minimizing the sum square error when this occurs (assuming, of course, that the data are now \"noisy\" and shouldn't be interpolated at all in which case we often prefer much more than $n+1$ data points). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2faecc-a32a-4b63-b9f2-bac4d92de3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f433b3b3-a727-4cbe-9f9b-a2b5eb34da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class polynomial_interpolant(object):\n",
    "    def __init__(self, data):\n",
    "        '''\n",
    "        Initialize object for constructing a polynomial interpolant through \n",
    "        the given array of data points.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : numpy array of shape (n+1, 2)\n",
    "            The (x,y) points where each of the points defines a row in the array\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.n = len(data)-1\n",
    "        self.construct_poly()\n",
    "        \n",
    "    def order(self):\n",
    "        print('Polynomial order is ', self.n)\n",
    "    \n",
    "    def compute_poly_coeffs(self):\n",
    "        self.A = np.zeros((self.n+1, self.n+1))\n",
    "        self.A[:,0] = np.ones(self.n+1)\n",
    "        for i in range(1, self.n+1):\n",
    "            self.A[:,i] = self.data[:,0]**i\n",
    "        self.poly_coeffs = np.linalg.solve(self.A, self.data[:,1])\n",
    "        \n",
    "    def construct_poly(self):\n",
    "        self.compute_poly_coeffs()\n",
    "        x = sym.symbols('x')\n",
    "        self.poly = 0\n",
    "        for i in range(self.n+1):\n",
    "            self.poly += self.poly_coeffs[i]*x**i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae8d12d-82e5-4bef-8d2e-cda76a5fc3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly1 = polynomial_interpolant(np.random.uniform(low=-5, high=5, size=(3,2)))\n",
    "poly1.poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265665c-4d70-4d93-b6c0-3e8480c452a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sym.lambdify(sym.symbols('x'), poly1.poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd0545-a2ec-4e78-9a08-115d92e6e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.figure(1)\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, p(x))\n",
    "\n",
    "plt.scatter(poly1.data[:,0], poly1.data[:,1], marker='s', color='r', s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca8a4b4-9cd1-402f-a3df-41398cf6e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly2 = polynomial_interpolant(np.random.uniform(low=-5, high=5, size=(5,2)))\n",
    "poly2.poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46812981-1a21-4f28-9282-31776da4598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sym.lambdify(sym.symbols('x'), poly2.poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00384a-296b-47ac-9397-596d70497d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.figure(2)\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, p(x))\n",
    "\n",
    "plt.scatter(poly2.data[:,0], poly2.data[:,1], marker='s', color='r', s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af51aa-72e4-4c3c-a145-d6cf477aaab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3240a423-32fa-48b7-9509-8c8cedb503c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbe132d9-3bfc-4ee4-acdd-64ea89bc06b1",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.5.4: Singular and Nonsingular Matrices\n",
    "---\n",
    "\n",
    "We only use the terms singular and nonsingular matrices to refer to square matrices $A\\in\\mathbb{R}^{n\\times n}$. \n",
    "\n",
    "If matrix $A$ has a multiplicative inverse $A^{-1}$ so that $A^{-1}A= I$ where $I$ is the $n\\times n$ identity matrix, then $A$ is said to be nonsingular. \n",
    "Otherwise, we say that $A$ is singular.\n",
    "\n",
    "   - If $A$ is nonsingular, then $Ax=b$ can be solved for all $b\\in\\mathbb{R}^n$ and the solution is unique.\n",
    "\n",
    "   - If $A$ is nonsingular, then $Ax=b$ either has no solution or an infinite number of solutions. It will have an infinite number of solutions if $b$ is in the span of the columns of $A$.\n",
    "   \n",
    "   - The **rank** of a matrix is the number of linearly independent columns (or rows) in the matrix (and the column and row ranks are exactly the same, which is why we just say rank in general). The rank of a nonsingular $n\\times n$ matrix is $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebdafaf-7a2c-426d-80b5-7d93d2ab986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = np.array([[2, -1, 0], [-1, 2, -1], [0, -1, 2]])\n",
    "A2 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "print('A1 = \\n', A1)\n",
    "print('\\nThe rank of A1 is ', matrix_rank(A1))\n",
    "print('\\nThe inverse of A1 is \\n\\n', np.linalg.inv(A1))\n",
    "\n",
    "print('\\nA2 = \\n', A2)\n",
    "print('\\nThe rank of A2 is \\n', matrix_rank(A2))\n",
    "print('\\nThe inverse of A2 is (uh-oh)\\n\\n', np.linalg.inv(A2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fdec1e-7620-480c-8169-e1830ab8fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = np.array([1, 2, 1])\n",
    "\n",
    "x1 = np.linalg.solve(A1, b1)\n",
    "print(x1)\n",
    "\n",
    "# check solution, should be equal to b1\n",
    "print(np.dot(A1, x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91b007-6aad-4564-a8f2-9356ed3c290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = np.array([1, 4, 7])  # This is the first column of A2, so a solution should exist\n",
    "\n",
    "x2 = np.linalg.solve(A2, b2)  # But this will produce an error\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd0cd58-a74a-4ab4-b1d2-83a90486b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.linalg.lstsq(A2, b2, rcond=None)[0]  # What if we try to find the vector of \"best fit\" with least squares?\n",
    "print(x2)\n",
    "\n",
    "# check solution, should be equal to b2\n",
    "print(np.dot(A2, x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b966054f-ece5-45e2-b7df-f4b21b22c07d",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.5.5: Eigenvalues and Eigenvectors\n",
    "---\n",
    "\n",
    "This again only makes sense if we are discussing square matrices $A\\in\\mathbb{R}^{n\\times n}$ (the eigenvalues and eigenvectors may in fact be complex valued even if $A$ is not). We say that $\\lambda$ is an eigenvalue of $A$ if there exists a nonzero vector $x$ such that \n",
    "$$\n",
    "\\large Ax=\\lambda x.\n",
    "$$\n",
    "\n",
    "- The span of all eigenvectors associated with an eigenvalue is called the ***eigenspace***. \n",
    "\n",
    "- Using eigenvectors to form a basis for a linear operator is a very convenient way to form a basis for the solution space that makes solving problems very straightforward. In particular, if an operator is self-adjoint (sometimes we say symmetric when referring to real-valued matrices, which just means $A=A^\\top$), then all the eigenvalues are real and the corresponding eigenvectors form an orthogonal set. In this case, it is very easy to solve problems involving the linear operator using the eigenvectors as a basis for the solution space. \n",
    " \n",
    "When studying a linear differential equation written as $Lu=f$, we often step back and look for eigenvalues and eigenfunctions (remember functions are types of vectors) of the operator $L$, i.e.,  $Lu=\\lambda u$, where $u$ also is required to satisfy any potential boundary conditions. It turns out that the eigenfunctions we encounter in this class correspond to solving $Lu=f$ via Fourier series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5db35-37cf-4bdd-b6fa-8c96e3efcb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals1, eig_vecs1 = np.linalg.eig(A1)\n",
    "eig_vals2, eig_vecs2 = np.linalg.eig(A2)\n",
    "\n",
    "print('Eig. values of A1, \\n', eig_vals1)\n",
    "print('\\nEig. vectors of A1 arranged as a matrix, \\n', eig_vecs1)\n",
    "\n",
    "print('\\nEig. values of A2, \\n', eig_vals2)\n",
    "print('\\nEig. vectors of A2 arranged as a matrix, \\n', eig_vecs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d5254d-6d1d-4f9f-90fe-c997691a4609",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.dot(A1, eig_vecs1))\n",
    "\n",
    "print()\n",
    "\n",
    "print(np.dot(eig_vecs1, np.diag(eig_vals1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b59d01-6d1e-475c-8f53-1c43c9c9a278",
   "metadata": {},
   "source": [
    "---\n",
    "#### Theorem: Let $(\\lambda, x)$ be an eigenvalue/eigenvector pair associated with a matrix $A\\in\\mathbb{R}^{n\\times n}$ and $\\alpha\\in\\mathbb{R}$, then $\\gamma=1+\\alpha\\lambda$ and $x$ defines an eigenvalue/eigenvector pair for the matrix $B=I+\\alpha A$. \n",
    "---\n",
    "\n",
    "**Proof:**\n",
    "$$\n",
    "\\begin{align}\n",
    "    Bx  = & (I+\\alpha A)x \\\\\n",
    "        = & Ix + \\alpha Ax \\\\\n",
    "        = & x + \\alpha \\lambda x \\\\\n",
    "        = & (1+\\alpha\\lambda)x \\\\\n",
    "        = & \\gamma x. \\ \\Box\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8361c5a3-47e3-4c58-9dcb-dfc462010004",
   "metadata": {},
   "source": [
    "---\n",
    "#### Corollary: Let $(\\lambda, x)$ be an eigenvalue/eigenvector pair associated with a matrix $A\\in\\mathbb{R}^{n\\times n}$ and $\\alpha\\in\\mathbb{R}$, then $\\gamma = \\alpha + \\lambda$ and $x$ defines an eigenvalue/eigenvector pair for the matrix $B=\\alpha I + A$.\n",
    "---\n",
    "\n",
    "The proof is a straightforward modification of the prior argument. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173b0bf8-4e40-4902-b222-712217cd2b15",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.5.6: Euclidean Inner Product and the Associated Norm\n",
    "---\n",
    "\n",
    "We write the Euclidean inner product of two vectors $x,y\\in\\mathbb{R}^n$ as\n",
    "\n",
    "$$\n",
    "\\large (x,y) = \\sum_{j=1}^n x_jy_j,\n",
    "$$\n",
    "\n",
    "and the associated (Euclidean) norm is defined by \n",
    "\n",
    "$$\n",
    " \\large   \\|x\\| = (x,x)^{1/2}\n",
    "$$\n",
    "\n",
    "While all norms are equivalent on a finite-dimensional vector space, the Euclidean norm (also called the 2-norm) is the only one *induced* by an inner product. An inner product structure is very useful to have on a vector space because it provides two things: a natural way to define a norm and a geometric notion of \"angles\" between vectors. In other words, we can define and investigate orthogonality (i.e., whether or not two vectors are \"perpendicular\" to each other).\n",
    "\n",
    "- Two vectors $x$ and $y$ are ***orthogonal*** if $(x,y)=0$.\n",
    "\n",
    "- A set of vectors $V=\\{v_1, v_2, \\ldots, v_k\\}$ is an ***orthogonal set*** if $(v_i,v_j)=0$ for all $i\\neq j$. If, in addition, $||v_i||=1$ for all $i=1,2,\\ldots, k$, then the set is called ***orthonormal***.\n",
    "\n",
    "  - Of all the properties that a norm satisfies, perhaps the two most important are the triangle inequality\n",
    "<br><br>\n",
    "$$\n",
    "\\large \\|x+y\\| \\leq \\|x\\| + \\|y\\|\n",
    "$$\n",
    "<br><br>\n",
    "and the Cauchy-Schwarz inequality for any inner-product induced norm\n",
    "<br><br>\n",
    "$$\n",
    "\\large (x,y)\\leq \\|x\\|\\cdot\\|y\\|.\n",
    "$$\n",
    "\n",
    "Note that the Cauchy-Schwartz inequality only holds for inner-product induced norms (i.e., on an inner product space where we use the inner-product induced norm), but the triangle inequality holds on any normed linear vector space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22665720-a86d-4a60-bc4e-c029ee7e0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u1)\n",
    "print(u2)\n",
    "print(np.dot(u1, u2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5783fa6-f345-4632-92cd-74baf95ef62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u1)\n",
    "print(u3)\n",
    "print(np.dot(u1, u3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d0003-e59e-4a81-bced-9ecf8d82875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u2)\n",
    "print(u3)\n",
    "print(np.dot(u2, u3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccadafda-c6d0-4067-9d90-592cbaf1b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v1)\n",
    "print(v3)\n",
    "print(np.dot(v1, v3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b299b-74c6-4d9c-b80f-a204c5ed5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.norm(u1))\n",
    "\n",
    "print(np.linalg.norm(u2))\n",
    "\n",
    "print(np.linalg.norm(v1))\n",
    "\n",
    "print(np.linalg.norm(v3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c858c2f8-aa4d-4d84-a764-918c0020ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Cauchy-Schwarz inequality\n",
    "print(np.dot(u1, u3))\n",
    "print(np.linalg.norm(u1)*np.linalg.norm(u3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca597cb-e34c-46b4-90ce-62bfd28b3de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.dot(v1, v3))\n",
    "print(np.linalg.norm(v1)*np.linalg.norm(v3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85603644-2f26-44c8-bca5-865e8aca527a",
   "metadata": {},
   "source": [
    "---\n",
    "#### Theorem: The Pythagorean theorem can be generalized to inner product spaces. \n",
    "---\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Let $x,y\\in\\mathbb{R}^n$ be orthogonal. Then,\n",
    "\\begin{eqnarray}\n",
    "   ||x+y||^2 &=& (x+y,x+y) \\\\\n",
    "              &=& (x,x) + \\underbrace{(x,y)}_{=0} + \\underbrace{(y,x)}_{=0} + (y,y) \\\\\n",
    "              &=& (x,x) + (y,y) \\\\\n",
    "              &=& ||x||^2 + ||y||^2 \\ \\Box\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf1665-9a42-4a3e-b342-c6f819e9ae2a",
   "metadata": {},
   "source": [
    "---\n",
    "#### Theorem: A set of othonormal vectors forms a linearly independent set. \n",
    "---\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Let $V=\\{v_1,v_2,\\ldots, v_k\\}\\subset\\mathbb{R}^n$ be a set of orthonormal vectors and let $\\alpha_i$ for $1\\leq i\\leq k$ be any constants such that\n",
    "$$\n",
    "    \\alpha_1 v_1 + \\alpha_2 v_2 + \\cdots + \\alpha_k v_k = 0.\n",
    "$$\n",
    "Consider any $1\\leq j\\leq k$, take the inner product of both sides of the above equation with $v_j$. \n",
    "The orthormality of the vectors implies\n",
    "$$\n",
    "    \\alpha_j = 0.\n",
    "$$\n",
    "Since the $j\\in\\{1,2,\\ldots, k\\}$ was arbitrary, this shows that the only linear combination of the vectors in $V$ that produces the zero vector is in fact the trivial linear combination. \n",
    "Thus, the vectors $V$ are linearly independent. $\\Box$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b63baa2-bdd0-4a57-ab61-495386bd8cea",
   "metadata": {},
   "source": [
    "---\n",
    "#### Theorem: An orthonormal set in $\\mathbb{R}^n$ is a basis for $\\mathbb{R}^n$ and the (unique) coefficients we use to write a vector $z\\in\\mathbb{R}^n$ as a linear combination of this orthonormal set are easily determined via inner products. \n",
    "---\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Let $Y=\\{y_1,y_2,\\ldots, y_n\\}\\subset\\mathbb{R}^n$ be an orthonormal set.\n",
    "Then, by part (j), this is a linearly independent set of $n$ vectors.\n",
    "Thus, it is a basis for $\\mathbb{R}^n$ by a standard result in linear algebra.\n",
    "Finally, for any $z\\in\\mathbb{R}^n$, there exists constants $\\{c_1,c_2,\\ldots, c_n\\}\\subset\\mathbb{R}$ such that\n",
    "$$\n",
    "    z = \\sum_{j=1}^n c_jy_j.\n",
    "$$\n",
    "These constants can easily be determined by taking the inner product of both sides of the equation with respect to $y_k$ for $1\\leq k\\leq n$ and exploiting the orthormality of the set to reveal that\n",
    "$$\n",
    "    (z, y_k) = c_k \\ \\forall \\ 1\\leq k\\leq n. \\ \\Box\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d0bbe6-cb5b-4861-9761-8b8ca4fba681",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.5.7 Positive Definite Matrices\n",
    "---\n",
    "\n",
    "If $A\\in\\mathbb{R}^{n\\times n}$ is symmetric and $x^\\top Ax>0$ for all nonzero $x\\in\\mathbb{R}^n$, then $A$ is said to be ***positive definite***. We often write SPD to denote a symmetric positive definite matrix. \n",
    "If $x^\\top Ax\\geq 0$ for all nonzero $x\\in\\mathbb{R}^n$, then $A$ is said to be ***positive semidefinite***.\n",
    "\n",
    "- A SPD matrix is nonsingular.\n",
    "\n",
    "- A symmetric matrix is also positive definite if and only if all th e eigenvalues are real and strictly positive. If some of the eigenvalues are zero but all others are positive, then the symmetric matrix is positive semidefinite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd074eb-c123-4d8d-ab80-213084798c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple way to check symmetry\n",
    "print(A1 - np.transpose(A1))\n",
    "print(A2 - A2.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3926dea1-a521-453c-b290-e7bd88d1db71",
   "metadata": {},
   "source": [
    "---\n",
    "#### Theorem: The sum of SPD matrices is also SPD.\n",
    "---\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Suppose $A,B\\in\\mathbb{R}^{n\\times n}$ are SPD and let $C=A+B$.\n",
    "Then, \n",
    "\n",
    "$$\n",
    "    C^\\top = (A+B)^\\top = A^\\top + B^\\top = A+B = C,\n",
    "$$\n",
    "\n",
    "so $C$ is symmetric.\n",
    "Let $x\\in\\mathbb{R}^n$ be nonzero.\n",
    "Then,\n",
    "\n",
    "$$\n",
    "    x^\\top Cx = x^\\top(A+B)x = x^\\top Ax + x^\\top Bx > 0,\n",
    "$$\n",
    "\n",
    "so $C$ is positive definite. $\\Box$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daacc070-6d28-48d1-9917-c9034ff82445",
   "metadata": {},
   "source": [
    "---\n",
    "#### Theorem: Let $A\\in\\mathbb{R}^{n\\times n}$ be nonsingular and $B=A^\\top A$, then $B$ is SPD.\n",
    "---\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Let $A\\in\\mathbb{R}^{n\\times n}$ be nonsingular and $B=A^\\top A$. Then,\n",
    "\n",
    "$$\n",
    "    B^\\top = (A^\\top A)^\\top = A^\\top (A^\\top)^\\top = A^\\top A = B,\n",
    "$$\n",
    "\n",
    "so $B$ is symmetric.\n",
    "Let $x\\in\\mathbb{R}^n$ be nonzero, then\n",
    "\n",
    "$$\n",
    "    x^\\top B x = x^\\top A^\\top A x = (Ax, Ax) = ||Ax||^2.\n",
    "$$\n",
    "\n",
    "Since $A$ is nonsingular and $x\\neq 0$, then $Ax\\neq 0$, so $||Ax||^2>0$. \n",
    "Thus, \n",
    "\n",
    "$$\n",
    "    x^\\top B x > 0, \n",
    "$$\n",
    "\n",
    "which implies $B$ is positive definite. $\\Box$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaca1d7-1899-4e32-819a-d8d0a9d5634b",
   "metadata": {},
   "source": [
    "---\n",
    "## Navigation:\n",
    "\n",
    "- [Previous](https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations/blob/main/Chp1/Chp1Sec4.ipynb)\n",
    "\n",
    "- [Next](https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations/blob/main/Chp1/Chp1Sec6.ipynb)\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
