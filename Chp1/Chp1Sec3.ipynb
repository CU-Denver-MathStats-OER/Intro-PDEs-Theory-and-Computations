{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9288fb0-2309-43e6-b904-1e6c01c16bd4",
   "metadata": {},
   "source": [
    "# Introduction to Partial Differential Equations\n",
    "---\n",
    "\n",
    "## Chapter 1: Preliminaries (Calculus, Linear Algebra, ODEs, and Python)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57613e6-2d8a-48a2-9bf2-381d41b3631c",
   "metadata": {},
   "source": [
    "## Want to use Colab? [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations/blob/main/Chp1/Chp1Sec3.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15db0875-7359-4d27-9f48-b7b42ef5ff7f",
   "metadata": {},
   "source": [
    "## Prepping the environment for interactive plots in Colab\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e18a28d-a89b-499d-a404-34661dc52e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab - installing missing packages')\n",
    "    !pip install ipympl\n",
    "    from IPython.display import clear_output\n",
    "    clear_output()\n",
    "    exit()\n",
    "else:\n",
    "    print('Not running on CoLab - assuming environment has necessary packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed99697-f939-41cd-b2a6-d144dfe98518",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca0ab6-476e-49c9-9df7-8e86a8371648",
   "metadata": {},
   "source": [
    "## Creative Commons License Information\n",
    "---\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/80x15.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">Introduction to Partial Differential Equations: Theory and Computations</span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">Troy Butler</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>.<br />Based on a work at <a xmlns:dct=\"http://purl.org/dc/terms/\" href=\"https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations\" rel=\"dct:source\">https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a231f-a555-40ef-b530-2211c2b2e7ce",
   "metadata": {},
   "source": [
    "## Section 1.3: Numerical Estimation of an ODE\n",
    "---\n",
    "\n",
    "![Taylor series of exp](https://upload.wikimedia.org/wikipedia/commons/6/62/Exp_series.gif \"Taylor polynomial approximations of the exponential function\")\n",
    "\n",
    "The most commonly used finite difference schemes are derived by manipulating [Taylor series expansions](https://en.wikipedia.org/wiki/Taylor_series) (the above gif shows various Taylor polynomial approximations to $e^x$ obtained by truncating its Taylor series expansion). This *usually* means it is straightforward to derive the [*local truncation error*](https://en.wikipedia.org/wiki/Truncation_error_(numerical_integration)) and prove convergence (which, for a finite difference scheme, usually means that we prove the scheme is both *consistent* and *stable*, which then implies convergence of the scheme). \n",
    "\n",
    "> Consistency is related to the truncation error and is usually straightforward to prove. Proving stability is usually an exercise in patience and perseverance. Stability concepts are discussed in the [next notebook](https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations/blob/main/Chp1/Chp1Sec4.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd28545f-d605-4f4d-9863-79750a2f5a91",
   "metadata": {},
   "source": [
    "We review the derivation of the forward Euler method in this notebook.\n",
    "\n",
    "Some key takeaways are:\n",
    "\n",
    "- The forward Euler method (or FE method for short) is an explicit method, which means it is easy to implement for problems where you want to evolve a solution in time quickly.\n",
    "\n",
    "- However, the time steps required for numerical stability may be so small as to render the method useless for even simple problems. \n",
    "\n",
    "- The forward Euler method is often used as a *first attempt* at gaining some numerical insight into a solution that varies in time. \n",
    "\n",
    "- For improved stability and accuracy with larger time steps, you will often turn to either multistage explicit methods (e.g., the popular 4-stage Runge-Kutta method) or implicit methods (e.g., backward Euler or Crank-Nicolson). \n",
    "\n",
    "<mark>The best takeaway from this notebook is the way in which convergence is studied **numerically**</mark>. This is related to what is shown in the [previous notebook](https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations/blob/main/Chp1/Chp1Sec2.ipynb).\n",
    "\n",
    "- Numerical studies of convergence are useful for either confirming the theory in practice, revealing some error we did not anticipate (either in the theoretical development or in the implementation), or for helping us develop the theory when the numerical analysis is a bit elusive. We develop some initial insight into this interplay of numerics and theory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a169db-2ca2-4606-870a-b5af3b61d239",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.3.1: Recalling [Taylor Series](https://en.wikipedia.org/wiki/Taylor_series) and [Taylor's Theorem](https://en.wikipedia.org/wiki/Taylor%27s_theorem)\n",
    "---\n",
    "\n",
    "For a *smooth* (i.e., infinitely differentiable) function $g(t)$, its Taylor series expanded about $t=a$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    g(t) &= g(a)+\\frac {g'(a)}{1!} (t-a)+ \\frac{g''(a)}{2!} (t-a)^2+\\frac{g'''(a)}{3!}(t-a)^3+ \\cdots \\\\\\\\\n",
    "     & = \\sum_{n=0} ^ {\\infty} \\frac {g^{(n)}(a)}{n!} (t-a)^{n}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "When $a=0$, this is often referred to as the Maclaurin series (but we'll stick with referring to this as a Taylor series). \n",
    "\n",
    "**Remarks:**\n",
    "\n",
    "- The equality of the infinite series to $g(t)$ is understood to mean that the limit of the sequence of partial sums (i.e., the Taylor polynomials $T_k(t) := \\sum_{n=0} ^ {k} \\frac {g^{(n)}(a)}{n!} (t-a)^{n}$) converges pointwise to $g(t)$as $k\\to \\infty$ on some interval (which may even be $\\mathbb{R}$), and the convergence is uniform on every compact subset (i.e., sets of the form $[c,d]$ for some real numbers $c<d$) of the convergence interval.\n",
    "\n",
    "  - Recall from the previous notebook that uniform convergence means convergence in the $\\sup$-norm metric.\n",
    "\n",
    "Of course, we *never* in practice construct or compute infinite series. Who would engage in such madness? In practice, we make do with a particular Taylor polynomial (i.e., some truncation of a Taylor series defining a polynomial of a specific degree) and utilize Taylor's theorem to understand the error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc32470-6e34-4468-b5af-126d11424c44",
   "metadata": {},
   "source": [
    "---\n",
    "#### Taylor's Theorem\n",
    "\n",
    "Let $k\\geq 1$ be an integer and let the function $g:\\mathbb{R}\\to\\mathbb{R}$ be $k$ times differentiable at the point $a\\in\\mathbb{R}$. Then, there exists a function $h_k:\\mathbb{R}\\to\\mathbb{R}$ such that\n",
    "\n",
    "$$\n",
    "    g(t) = T_k(t) + h_k(t)(t-a)^k, \\ \\text{ and } \\ \\lim_{t\\to a}h_k(x)=0.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Remarks:**\n",
    "\n",
    "- The polynomial $T_k(t)$ appearing above is the $k$th order Taylor polynomial we defined above. \n",
    "\n",
    "- The remainder term is\n",
    "<br><br>\n",
    "$$\n",
    "    R_k(t) = g(t) - T_k(t) =  h_k(t)(t-a)^k = o(|t-a|^k) \\ \\text{ as } t\\to a.\n",
    "$$\n",
    "<br><br>\n",
    "Here, $R_k(t) = o(|t-a|^k)$ as $t\\to a$ means that $\\lim_{t\\to a} \\left|\\dfrac{R_k(t)}{|t-a|^k}\\right| = 0$. \n",
    "<br><br>\n",
    "Thus, Taylor's theorem describes the polynomial (given by $T_k$) that is the *asymptotic best fit* to the function $g(t)$ as $t\\to a$.\n",
    "\n",
    "- Note that this theorem *fails* to give an actual form for the remainder term $R_k(t)$ because we do not know the form of $h_k(t)$. The theorem only states there exists such a function $h_k(t)$ but does not state what it actually is other than it must have a specific asymptotic property. This is unsatisfying.\n",
    "\n",
    "- <mark>The real power of approximating a function with a Taylor polynomial where an explicit remainder term can be formulated and analyzed occurs when we use stronger *regularity* assumptions on the function.</mark>\n",
    "\n",
    "Wait, what does regularity mean? In PDEs, we often refer to the regularity of the function when discussing just *how smooth* the function is. If we assume that just *one more derivative* of $g$ exists so that it is $k+1$ times differentiable on the open interval defined by $a$ and $t$ and that the $k$th derivative is continuous on the closed interval defined by $a$ and $t$, then the remainder term $R_k(t)$ takes the form\n",
    "\n",
    "$$\n",
    "    R_k(t) = \\frac { g^{(k+1)} (\\xi) } {(k+1)!} (t-a)^{k+1}, \n",
    "$$\n",
    "\n",
    "where $\\xi$ is some number between $a$ and $t$ that is guaranteed to exist by the Mean Value Theorem for derivatives.\n",
    "\n",
    "In practice, we do not know what $\\xi$ is, but if we can determine a bound $g^{(k+1)}(\\xi)$ on the interval defined by $(a-r, a+r)$ for some $r>0$ (and call such a bound $M$), then we can bound the remainder term as follows\n",
    "\n",
    "$$\n",
    "    \\left| R_k(t) \\right| \\leq \\frac{M r^{k+1}}{(k+1)!}, \n",
    "$$\n",
    "\n",
    "This then defines a computable bound on the error for the Taylor polynomial for any $t\\in(a-r, a+r)$. Moreover, the bound is $\\mathcal{O}(r^{k+1})$ which means that $R_k\\sim \\mathcal{O}(r^{k+1})$. Subsequently, $\\frac{R_k}{r^m}\\sim\\mathcal{O}(r^{k+1-m})$ for $1\\leq m\\leq k$, which is useful for understanding the rates of convergence we will encounter with finite difference schemes.\n",
    "\n",
    "Let's look at a simple example involving $e^t$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b30162f-d266-4225-be4b-c15537a29a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import factorial as fac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43996a14-b8d8-4e01-aedb-5f1e92abf249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Taylor polynomial for e^t\n",
    "\n",
    "def exponential_Taylor(t, a, k):\n",
    "    v = np.exp(a) * np.ones(len(t))\n",
    "    for i in range(1, k+1):\n",
    "        v += np.exp(a) * (t-a)**i / fac(i)\n",
    "    return v\n",
    "\n",
    "# Define the remainder bound\n",
    "def exponential_remainder(t, a, k):\n",
    "    r = np.max(np.abs(t-a))\n",
    "    R_k = np.exp(np.max(t)) * r**(k+1) / fac(k+1)\n",
    "    return R_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ec358-70ab-4d84-a6ce-0a818883335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_exponential(t, a, k, num=0):\n",
    "    plt.figure(num = num)\n",
    "    plt.clf()\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(t, np.exp(t), 'b', label='$e^t$')\n",
    "    plt.plot(t, exponential_Taylor(t, a, k), 'r:', label='$T_' + str(k) + '(t)$')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(t, exponential_remainder(t, a, k)*np.ones(len(t)), 'b', label='Remainder Bound')\n",
    "    plt.plot(t, np.abs(np.exp(t) - exponential_Taylor(t, a, k)), 'r:', label='abs. error = $|e^t - T_' + str(k) + '(t)|$')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2079720e-2779-417f-a152-657f0a6b0ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fad0946-5dfd-4283-b021-9c7e11e2de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out \n",
    "\n",
    "%matplotlib widget\n",
    "interact_manual(plot_exponential, \n",
    "            t = fixed(np.linspace(-1, 1, 50)),\n",
    "            a = fixed(-0.5),  # Try -0.5, 0, and 0.5\n",
    "            k = widgets.IntSlider(value=1, min=1, max=10, step=1),\n",
    "            num = fixed(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97c065-ef14-49c2-a651-c847ba8128d3",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.3.2: A prototypical IVP and the Forward Euler Method\n",
    "---\n",
    "\n",
    "In the remainder (no pun intended) of this notebook, we consider 1st order IVPs of the form\n",
    "\n",
    "$$\n",
    "    \\begin{cases}\n",
    "        u'(t) &= f(t, u(t)), \\ t>t_0 \\\\\n",
    "        u(t_0) &= u_0,\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "where $f=f(t, u)$ is a given function that may result in the IVP being nonlinear. \n",
    "\n",
    "For simplicity, we assume $t_0=0$ in everything that follows in this notebook.\n",
    "\n",
    "While we provide a detailed derivation and analysis of the forward Euler method below, conceptually, it is rather simple to describe as replacing $u'(t)$ with a first-order forward finite difference approximation at any given time, keeping the right-hand side evaluated at the current time $t$, and solving for $u(t+\\Delta t)$ (where $\\Delta t$ is used to denote the step-size in the finite difference formula). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b98ca19-3a8e-480d-86ce-031c31461dbb",
   "metadata": {},
   "source": [
    "---\n",
    "#### Deriving the [Forward Euler Method](https://en.wikipedia.org/wiki/Euler_method)\n",
    "---\n",
    "\n",
    "Suppose $u$ is twice continuously differentiable around $t$ and that $\\Delta t>0$ is chosen such that $t+\\Delta t$ is \"around\" $t$, then by Taylor's theorem and the explicit representation of the remainder term, we have\n",
    "\n",
    "$$\n",
    "    u(t+\\Delta t) = u(t) + u'(t)\\Delta t + \\frac{u''(\\xi)(\\Delta t)^2}{2}\n",
    "$$\n",
    "\n",
    "for some $\\xi\\in[t, t+\\Delta t]$. Some people prefer to write this as\n",
    "\n",
    "$$\n",
    "    u(t+\\Delta t) = u(t) + u'(t)\\Delta t + \\frac{u''(t+\\xi)(\\Delta t)^2}{2}\n",
    "$$\n",
    "\n",
    "where now $\\xi\\in[0,\\Delta t]$.\n",
    "\n",
    "By rearranging terms, we have that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    u'(t) &= \\frac{u(t+\\Delta t) - u(t)}{\\Delta t} +  \\frac{u''(t+\\xi)\\Delta t}{2} \\\\\\\\\n",
    "          &= \\frac{u(t+\\Delta t) - u(t)}{\\Delta t} + \\mathcal{O}(\\Delta t).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let $t_m:= m\\Delta t$ and $v_m$ denote the approximations we will construct to $u(t_m)$ for $m=0, 1, 2, \\ldots$. \n",
    "\n",
    "At $m=0$, we choose $v_0=u(t_0)=u_0$ to produce an approximation with zero error.\n",
    "\n",
    "What should we do for $m>0$?\n",
    "\n",
    "Well, let's explore what we know about $u(t)$. We know it satisfies the IVP, and we know that $\\dfrac{u(t+\\Delta t) - u(t)}{\\Delta t}$ approximates $u'(t)$ for any $t$, so we have that\n",
    "\n",
    "$$\n",
    "    \\frac{u(t_{m+1}) - u(t_m)}{\\Delta t} \\approx u'(t_m) = f(t_m, u(t_m))\n",
    "$$\n",
    "\n",
    "is a \"good\" approximation if $\\Delta t$ is sufficiently small. This seems like a decent way of recursively defining our approximations to $u(t_m)$ with $v_m$ by simply substituting the $v_m$ into this approximation of the differential equation to get\n",
    "\n",
    "$$\n",
    "    \\frac{v_{m+1}-v_m}{\\Delta t} = f(t_m, v_m), \n",
    "$$\n",
    "\n",
    "which we can solve for $v_{m+1}$ to get\n",
    "\n",
    "$$\n",
    "    v_{m+1} = v_m + \\Delta t f(t_m, v_m).\n",
    "$$\n",
    "\n",
    "Thus, as soon as we have $v_0$, we can compute $v_1$, from which we can compute $v_2$, and so on to produce a sequence of $v_m$ values that approximate $u(t_m)$ for any $m=0,1,2,\\ldots$. \n",
    "\n",
    "This is called the Forward Euler method. \n",
    "\n",
    "**Remarks:**\n",
    "\n",
    "- This method sucks. Well, it is okay. Nah, it sucks. It is easy to implement and straightforward to analyze, but overall it is not great even on simple problems. It is usually our first attempt just to get some initial transitory information on how a state variable evolves over very short time periods, but we typically prefer to use other methods.\n",
    "\n",
    "- While we have derived an approximation method above, we have not yet analyzed whether it is any good or not. We just used some reasoning/rationalization to justify some substitutions that \"seemed reasonable\" as we went along. This is fine for developing a method based on some mathematical reasoning/intuition, but we still need to perform some sort of rigorous analysis of this approach.\n",
    "\n",
    "First, let's explore some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b25ad-6b56-4a46-816f-71610cddc044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_Euler(f, Delta_t, n, u_0):\n",
    "    # Assuming f is passed as a function of (t,u)\n",
    "    v = np.zeros(n)\n",
    "    v[0] = u_0\n",
    "    for i in range(1,n):\n",
    "        v[i] = v[i-1] + Delta_t * f(i*Delta_t, v[i-1])\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10219c8f-58b6-4b77-9f8b-37c91f34ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A \"trivial\" example for u'=u, so f(t,u)=u\n",
    "f = lambda t, u: u  \n",
    "\n",
    "u_0 = 1\n",
    "\n",
    "u = lambda t: u_0*np.exp(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24473eb-5417-47a6-9d7a-caae97d202b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forward_Euler(f, u, t_f, Delta_t, num=1):\n",
    "    # Assuming f is a function of (u,t) and u is a function of t\n",
    "    \n",
    "    plt.figure(num)\n",
    "    plt.clf()\n",
    "    \n",
    "    n = int(t_f/Delta_t)\n",
    "    t = np.linspace(0, t_f, n+1)\n",
    "    \n",
    "    plt.plot(t, forward_Euler(f, t[1]-t[0], n+1, u(0)), 'r:', \n",
    "             marker='s', label='Approx. soln')\n",
    "    \n",
    "    plt.plot(t, u(t), 'b', label='Exact soln')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ff416-e23f-47af-99c7-fdf1fdcc43f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out \n",
    "\n",
    "%matplotlib widget\n",
    "interact_manual(plot_forward_Euler, \n",
    "            f = fixed(f),\n",
    "            u = fixed(u),\n",
    "            t_f = fixed(1),\n",
    "            Delta_t = widgets.FloatText(value=0.1),\n",
    "            num = fixed(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75006718-d27a-4c03-84f7-8e9f670ef64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A nonlinear example\n",
    "f = lambda t, u: t*u*(u-2)\n",
    "\n",
    "u_0 = 1.99\n",
    "\n",
    "u = lambda t: 2*u_0 / (u_0+(2-u_0)*np.exp(t**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6142ffd2-c16a-4cb5-b616-532fcf998926",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out \n",
    "\n",
    "%matplotlib widget\n",
    "interact_manual(plot_forward_Euler, \n",
    "            f = fixed(f),\n",
    "            u = fixed(u),\n",
    "            t_f = widgets.FloatText(4),\n",
    "            Delta_t = widgets.FloatText(value=0.1),  # Try 0.25, 0.5, and 1\n",
    "            num = fixed(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26c122-f281-413a-bbfb-f9abaea5418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A nonlinear example\n",
    "f = lambda t, u: t*u*(u-2)\n",
    "\n",
    "u_0 = 2.01\n",
    "\n",
    "u = lambda t: 2*u_0 / (u_0+(2-u_0)*np.exp(t**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b1d60-6404-4492-9646-d8ccf42b1bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out \n",
    "\n",
    "%matplotlib widget\n",
    "interact_manual(plot_forward_Euler, \n",
    "            f = fixed(f),\n",
    "            u = fixed(u),\n",
    "            t_f = widgets.FloatText(2.5),\n",
    "            Delta_t = widgets.FloatText(value=0.1),  # Try 0.01 a\n",
    "            num = fixed(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5e6c5-a4a6-4aaa-aaee-3263f1584943",
   "metadata": {},
   "source": [
    "**What if we did not necessarily know what $u$ is, so we wanted to improve the `plot_forward_Euler` function to allow for the option of $u$ being unknown?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e24161-2295-40cf-843e-7bd8577e891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forward_Euler_improved(f, t_f, Delta_t, u_0, u=None, num=1):  # All keyworded parameters have to come at the end\n",
    "    # Assuming f is a function of (u,t) and u is a function of t\n",
    "    \n",
    "    plt.figure(num)\n",
    "    plt.clf()\n",
    "    \n",
    "    n = int(t_f/Delta_t)\n",
    "    t = np.linspace(0, t_f, n+1)\n",
    "    \n",
    "    plt.plot(t, forward_Euler(f, t[1]-t[0], n+1, u_0), 'r:', \n",
    "             marker='s', label='Approx. soln')\n",
    "    \n",
    "    if u is not None:\n",
    "        plt.plot(t, u(t), 'b', label='Exact soln')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c968f9aa-c2ca-4b00-ac7f-000d34f3cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f out \n",
    "\n",
    "%matplotlib widget\n",
    "interact_manual(plot_forward_Euler_improved, \n",
    "            f = fixed(lambda t, u: u*np.sin(u*t)),  # Put any f you want here whether you know u or not. \n",
    "            u = fixed(None),  # Who cares what the exact solution $u$ is? Not me!\n",
    "            u_0 = fixed(1),  # Put any IC you want here whether you know $u$ or not. \n",
    "            t_f = widgets.FloatText(5),\n",
    "            Delta_t = widgets.FloatText(value=0.1),  # Try 0.01\n",
    "            num = fixed(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae162187-a44e-4c20-9f2c-26eb011a412e",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.3.3: An initial error analysis and ROC analysis\n",
    "---\n",
    "\n",
    "So far, we have only explored the quality of the FE method in a qualitative sense by examining how the plots compare between the approximations $\\{v_m\\}$ and the exact solutions.\n",
    "\n",
    "We now explore the errors a bit more systematically.\n",
    "\n",
    "Returning to the trivial example of $u'(t)=u(t)$ with $u(0)=1$ that was numerically explored above, we see that since $f(t,u(t))=u(t)$ that the FE method produces the following formula for the $v_m$ approximations\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    v_{m+1} &= v_m + \\Delta t v_m \\\\\\\\\n",
    "            &= (1+\\Delta t) v_m.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Using induction and the fact that $v_0=u(0)=1$, we have that for any $m=0, 1, 2,\\ldots$, \n",
    "\n",
    "$$\n",
    "    v_m = (1+\\Delta t)^m.\n",
    "$$\n",
    "\n",
    "Suppose we want to study the error at a final time $t_f$. For the sake of simplicity, we assume $t_f$ is a multiple of $\\Delta_t$, i.e., $t_f=M\\Delta t$ for some $M\\in\\mathbb{N}$. Then, we are interested in \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    v_M &= (1+\\Delta t)^M \\\\\\\\\n",
    "        &= (1+\\Delta t)^{t_f/\\Delta t}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The actual solution $u(t_f)=e^{t_f}$, so the error in $v_M$ as a function of $\\Delta t$ is given by\n",
    "\n",
    "$$\n",
    "    E(\\Delta t) = \\left| e^{t_f} - (1+\\Delta t)^{t_f/\\Delta t} \\right|.\n",
    "$$\n",
    "\n",
    "The question becomes: Is $(1+\\Delta t)^{t_f/\\Delta t}$ ever a good approximation to $e^{t_f}$ for \"small\" values of $\\Delta t$? \n",
    "\n",
    "To answer this question, we first recall from calculus the [limit characterization of the exponential function](https://en.wikipedia.org/wiki/Characterizations_of_the_exponential_function) (which is something we also saw in the previous notebook):\n",
    "\n",
    "$$\n",
    "    \\lim_{n\\to\\infty} \\left(1+\\frac{x}{n}\\right)^n = e^x, \n",
    "$$\n",
    "\n",
    "which we can use a change of variables of $\\epsilon=x/n$ (so $n=x/\\epsilon$) to rewrite as\n",
    "\n",
    "$$\n",
    "    \\lim_{\\epsilon\\to 0} \\left(1+\\epsilon\\right)^{x/\\epsilon} = e^x.\n",
    "$$\n",
    "\n",
    "Now, we see for small $\\Delta t$ that $(1+\\Delta t)^{t_f/\\Delta t}\\approx e^{t_f}$, and in fact that\n",
    "\n",
    "$$\n",
    "    E(\\Delta t) \\to 0 \\text{ as } \\Delta t\\to 0.\n",
    "$$\n",
    "\n",
    "Let's explore this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc6f2a-38ad-447a-98d1-3cee5803bb42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an array of Delta_ts that decrease by an order of magnitude\n",
    "\n",
    "Delta_ts = np.logspace(-1, -7, 7)\n",
    "print(Delta_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8379c-3789-450e-bb1c-4476820e8972",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_f = 1  # can choose this to be any positive number\n",
    "\n",
    "abs_errors = np.abs(np.exp(t_f)-(1+Delta_ts)**(t_f/Delta_ts))\n",
    "print(abs_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ac48e-6280-4308-9bd8-c12647ae1fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plt.figure(4)\n",
    "plt.loglog(Delta_ts, abs_errors, 'b')\n",
    "\n",
    "line_params = np.polyfit(np.log(Delta_ts), np.log(abs_errors), 1)\n",
    "\n",
    "plt.title(r'Rate of Convergence of $E(\\Delta t)\\approx${:1.3f}'.format(line_params[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c4ebd6-8679-486f-8b0f-1845f7cb36f4",
   "metadata": {},
   "source": [
    "**Exploring the ROC for the nonlinear example.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d220d-ed62-4d33-9261-21d098fd1aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda t, u: t*u*(u-2)\n",
    "\n",
    "u_0 = 2.01\n",
    "\n",
    "u = lambda t: 2*u_0 / (u_0+(2-u_0)*np.exp(t**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5fedaa-04f3-47d4-91e8-6eb99bb79571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Delta_ts = np.logspace(-1, -7, 7)\n",
    "print(Delta_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b86d39b-2041-4032-8958-f9b7013438b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code cell may take a few moments to run\n",
    "\n",
    "t_f = 1  # can choose this to be any positive number\n",
    "\n",
    "abs_errors = np.zeros(len(Delta_ts))\n",
    "for i, Delta_t in zip(range(len(Delta_ts)), Delta_ts):\n",
    "    abs_errors[i] = np.abs(u(t_f)-forward_Euler(f, Delta_t, int(1/Delta_t)+1, u_0)[-1])\n",
    "print(abs_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b1bf9-1be9-4b2c-a850-1cc035270d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plt.figure(4)\n",
    "plt.loglog(Delta_ts, abs_errors, 'b')\n",
    "\n",
    "line_params = np.polyfit(np.log(Delta_ts), np.log(abs_errors), 1)\n",
    "\n",
    "plt.title(r'Rate of Convergence of $E(\\Delta t)\\approx${:1.3f}'.format(line_params[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304f4606-5f65-438f-a723-2b70040c077b",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.3.4: Exploring the errors in more depth\n",
    "---\n",
    "\n",
    "<mark>**Step 1: Establish the Local Truncation Error (LTE)**</mark>\n",
    "\n",
    "Let $\\tau_m$ denote the Local Truncation Error (LTE) of the FE method at one-step of the method using the *exact* solution. From the above derivation of the FE method, we see that \n",
    "\n",
    "$$\n",
    "    \\large \\tau_m = \\frac{1}{2}(\\Delta t) u''(\\xi)\n",
    "$$\n",
    "\n",
    "where $\\xi\\in(t_m,t_{m+1})$.\n",
    "\n",
    "Generally, we will seek a ***bound on $u''(\\xi)$ over $[0,t_f]$ in order to remove dependency of $\\tau_m$ on $m$*** (notice that the dependence is subtle because $\\xi\\in(t_m,t_{m+1})$. How do we do this? Well, $u'=f(t,u)$ which means that $u''$ can be written in terms of derivatives of $f$ and $f$ is known. Of course, if $f$ is not differentiable, then we are not going to have much luck proving convergence results or at least not proving rates of convergence.\n",
    "\n",
    "Since in this particular case we know that $u(t)=e^t$ is the exact solution to this simple IVP, we have that $u''(t)=e^t$ so that $u''(\\xi)=e^\\xi$, which implies \n",
    "\n",
    "$$\n",
    "    \\large \\tau_m = \\frac{\\Delta t}{2}e^\\xi.\n",
    "$$\n",
    "\n",
    "Since $e^t$ is a monotonically increasing function, we have that $e^\\xi\\leq e^{t_f}$, which implies that\n",
    "\n",
    "$$\n",
    "    \\large |\\tau_m| \\leq \\frac{\\Delta t}{2} e^{t_f}, \\ \\text{ for } \\ 0\\leq (m+1)\\Delta t\\leq t_f.\n",
    "$$\n",
    "\n",
    "Here, we wrote $0\\leq (m+1)\\Delta t\\leq t_f$ because the truncation error $\\tau_m$ is defined for the method stepping from $t_m$ to $t_{m+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f08b8-ca09-4ac5-bbdf-1fdb41fa51a6",
   "metadata": {},
   "source": [
    "<mark>**Step 2: Determine a formula for how the error evolves over time**</mark>\n",
    "\n",
    "Write $E_m=u_m-v_m$ to denote the error at time $t_m=m\\Delta t$, then \n",
    "\n",
    "$$\n",
    "    \\large u_{m+1} = u_m + \\Delta t u_m + \\Delta t \\tau_m = (1+\\Delta t)u_m + \\Delta t\\tau_m.\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "    \\large E_{m+1}=u_{m+1}-v_{m+1} = (1+\\Delta t)u_m + \\Delta t \\tau_m - (1+\\Delta t)v_m. \n",
    "$$\n",
    "\n",
    "By factoring, we have that\n",
    "\n",
    "$$\n",
    "    \\large E_{m+1} = (1+\\Delta t)E_m + \\Delta t \\tau_m. \n",
    "$$\n",
    "\n",
    "Since $u_0 = 1 = v_0$, we clearly have $E_0=0$.\n",
    "\n",
    "Why did we use $E_m$ for the error term instead of $e_m$ as in the previous notebook? Well, $e$ is being used here for the exponential function, and we want to avoid \"overloading\" the notation $e$ in this current notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82339bda-f07f-4496-8d41-3d844f43919a",
   "metadata": {},
   "source": [
    "<mark>**Step 3: Use induction to determine a bound for $E_m$ at each $m$**</mark>\n",
    "\n",
    "It is often helpful to determine the first few bounds before applying induction.\n",
    "\n",
    "For $m=1$, we have that \n",
    "\n",
    "$$\n",
    "    \\large |E_1| \\leq \\Delta t \\tau_1 \\leq \\frac{(\\Delta t)^2}{2} e^{t_f}\n",
    "$$\n",
    "\n",
    "so for $m=2$, we have that\n",
    "\n",
    "$$\n",
    "    \\large |E_2| \\leq (1+\\Delta t) \\frac{(\\Delta t)^2}{2} e^{t_f}+ \\frac{(\\Delta t)^2}{2} e^{t_f}.\n",
    "$$\n",
    "\n",
    "Then, for $m=3$, we have that\n",
    "\n",
    "$$\n",
    "    \\large |E_3| \\leq (1+\\Delta t)^2 \\frac{(\\Delta t)^2}{2} e^{t_f} + (1+\\Delta t)\\frac{(\\Delta t)^2}{2} e^{t_f} + \\frac{(\\Delta t)^2}{2} e^{t_f}.\n",
    "$$\n",
    "\n",
    "We then see from induction that\n",
    "\n",
    "$$\n",
    "    \\large |E_m| \\leq \\frac{(\\Delta t)^2}{2}e^{t_f} \\sum_{i=0}^{m-1} (1+\\Delta t)^i.\n",
    "$$\n",
    "\n",
    "The sum is a partial sum of a geometric series, which can easily be calculated to show that\n",
    "\n",
    "$$\n",
    "    \\large |E_m| \\leq \\frac{(\\Delta t)^2}{2} e^{t_f} \\left[\\frac{(1+\\Delta t)^m - 1}{\\Delta t}\\right], \n",
    "$$\n",
    "\n",
    "which gives\n",
    "\n",
    "$$\n",
    "    \\large |E_m| \\leq \\frac{\\Delta t}{2} e^{t_f} \\left[(1+\\Delta t)^m - 1\\right].\n",
    "$$\n",
    "\n",
    "We now use the inequality $1+\\Delta t\\leq e^{\\Delta t}$ and observe that $(e^{\\Delta t})^m = e^{m\\Delta t} = e^{t_m}$ to write this as\n",
    "\n",
    "$$\n",
    "    \\large |E_m| \\leq \\frac{\\Delta t}{2} e^{t_f}\\left(e^{t_m}-1\\right).\n",
    "$$\n",
    "\n",
    "**Remarks:**\n",
    "\n",
    "- If we compare this to our computed errors from above (which we do below), we notice this bound is too large. Well, it is a *bound* not an estimate, so keep that in mind.\n",
    "\n",
    "- When we do not know the solution, which is often the case in practice, we would replace the $e^{t_f}$ in the bound for $|E_m|$ with $\\sup_{0\\leq t\\leq t_f} |u''(t)|$ and use the relationship of $u''(t)=\\frac{d}{dt} f(t,u(t)) = (\\partial_t f) + (\\partial_u f)u' = (\\partial_t f) + (\\partial_u f)f$ to try and get an idea of the size of the second derivative, which may only allow us to make local approximations to the bound of error at each time step instead of a global bound that is useful over all time steps.\n",
    "\n",
    "- If we replace the $e^{t_f}$ with $\\sup_{0\\leq t\\leq t_f} |u''(t)|$, then this is also a bound derived *a priori* of the computed solution. We simply used the form of the problem to get at this error bound. It is *common* for a priori derived error bounds and estimates of these error bounds to be *orders of magnitude* too large.\n",
    "\n",
    "- If an error bound can be orders of magnitude too large, then is it good for anything? Well, yes, it is good for proving convergence and establishing rates of convergence (e.g., under the assumption that the second derivative of $u$ is bounded on $[0, t_f]$, then <mark>we have that the method is $\\mathcal{O}(\\Delta t)$</mark> according to the error bound). But, a priori error bounds are just bad for producing any kind of reliable estimate of the actual error. This is not what their purpose is though. A posteriori error estimation is really the desired tool for producing accurate estimates of errors in computed quantities from a numerical solution, but this is a topic that is best studied in more depth in an advanced PDEs course where we can apply some functional analysis tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7497c9-7d2d-40a5-a52a-5e8c0feb1402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an array of Delta_ts that decrease by an order of magnitude\n",
    "\n",
    "Delta_ts = np.logspace(-1, -7, 7)\n",
    "print(Delta_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd24ab14-1777-40ba-a321-1a4ac7c03a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_f = 1  # can choose this to be any positive number\n",
    "\n",
    "abs_errors = np.abs(np.exp(t_f)-(1+Delta_ts)**(t_f/Delta_ts))\n",
    "print('Absolute value of errors at time t_f\\n')\n",
    "print(abs_errors)\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "error_bds = Delta_ts/2*np.exp(t_f)*(np.exp(t_f)-1)\n",
    "print('\\nError bound at time t_f\\n')\n",
    "print(error_bds)\n",
    "\n",
    "print('-'*50)\n",
    "print('\\nRatio of error bound to actual magnitude of errors\\n')\n",
    "print(error_bds/abs_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc438f2f-df83-4a9b-8621-6235465eec8e",
   "metadata": {},
   "source": [
    "---\n",
    "## Navigation:\n",
    "\n",
    "- [Previous](https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations/blob/main/Chp1/Chp1Sec2.ipynb)\n",
    "\n",
    "- [Next](https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations/blob/main/Chp1/Chp1Sec4.ipynb)\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
