{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62eec4c8-1bca-4653-8518-b52cbdc2c551",
   "metadata": {},
   "source": [
    "# Introduction to Partial Differential Equations\n",
    "---\n",
    "\n",
    "\n",
    "## Chapter 1: Preliminaries (Calculus, Linear Algebra, ODEs, and Python)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caa5460-1b82-444c-b58e-b1a6528255e0",
   "metadata": {},
   "source": [
    "## Section 1.2: Approximations and Rates of Convergence\n",
    "---\n",
    "\n",
    "The purpose of this section is to develop computational approaches to investigating the **rates of convergence** of schemes under the condition that the *exact* solution is available.\n",
    "\n",
    "The situation is a common one. We have a type of problem we wish to approximately solve using an algorithm. We implement the algorithm in code. We wish to verify that code *works*. To verify that code works, we typically mean two things. First, we check that the code produces an approximation to a *known* solution when the parameters/data for the problem are properly set up for the code. Second, we check that the approximations *converge* to the solution as various algorithmic parameters are refined. \n",
    "\n",
    "In some cases, a theoretical rate of convergenge is known or provable when certain assumptions are met about the parameters/data for the problem. However, this is not always the case, or it may be very difficult to obtain such theoretical rates of convergence. \n",
    "\n",
    "The methods studied in this notebook can be used to investigate the rate of convergence of a method even if a theoretical rate of convergence is not known. The numerically observed rates of convergence can provide useful insight into what the theoretical rates of convergence may be under various assumptions about the setup of the problem.\n",
    "\n",
    "In the end, we are utilizing the **method of manufactured solutions** which is a verification approach for code. This was discussed briefly in the previous notebook.\n",
    "\n",
    "We will reference the method of manufactured solutions many more times throughout the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702384ee-0b28-48d5-9096-60cc4f257c75",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.2.1: Definitions and Notation\n",
    "---\n",
    "\n",
    "**Sequence**. Let $X$ be a set. For $m\\in\\mathbb{Z}$, a sequence is an ordered set of terms from $X$ denoted by $x_m, x_{m+1}, x_{m+2}, \\ldots$ (with some terms possibly repeated a finite or infinite amount of times). We often use the more compact notation $(x_n)_{n=m}^\\infty\\subset X$ or $\\{x_n\\}_{n=m}^\\infty\\subset X$ to denote this set. When $m=1$ or it is understood from context what $m$ is, then we often denote the sequence even more simply as $(x_n)$ or $\\{x_n\\}$.\n",
    "\n",
    "**[Banace space](https://en.wikipedia.org/wiki/Banach_space)**. A Banach space is a complete normed vector space. Complete means that every Cauchy sequence converges and normed vector space means there exists a norm $\\|\\cdot\\|$ for computing the length of vectors and the distance between two vectors $x,y\\in X$ can be computed as $\\|x-y\\|$.\n",
    "\n",
    "**Convergence of Sequences**. Let $X$ be a Banach space and $(x_n)\\subset X$ and $x\\in X$. We say that $(x_n)$ converges to $x$ if for every $\\epsilon>0$ there exists an integer $N$ such that for all integers $n\\geq N$, \n",
    "\n",
    "$$ \\large \\|x_n-x \\| < \\epsilon.$$\n",
    "\n",
    "*Notes:*\n",
    "- If $(x_n)$ converges to $x$, then we write either \n",
    "\n",
    "$$\\large \\lim_{n\\to\\infty} x_n=x, \\ \\text{ or more simply } \\ x_n\\to x.$$\n",
    "\n",
    "- In the definition, $N$ can actually be a real number, but $n$ must be an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ff1a3-f8ca-4f1f-8567-dccd736af381",
   "metadata": {},
   "source": [
    "---\n",
    "#### Examples of Banach spaces and convergent sequences\n",
    "---\n",
    "\n",
    "There are [many examples](https://en.wikipedia.org/wiki/Banach_space#Examples_2) of Banach spaces. Below, we summarize some of the more common/familiar ones. \n",
    "\n",
    "- $\\mathbb{R}^k$ for any $k\\in\\mathbb{N}$ equipped with any norm is an examples of a Banach space. The typical norm is the Euclidean norm (i.e, the 2-norm) where if $x\\in \\mathbb{R}^k$, and $x^{(i)}$ denotes the $i$th component of $x$ for $1\\leq i\\leq k$,  then \n",
    "<br><br>\n",
    "$$\n",
    "    \\| x\\|_2 = \\left(\\sum_{i=1}^k |x^{(i)}|^2\\right)^{1/2}\n",
    "$$\n",
    "<br>\n",
    "  \n",
    "  - Note that when $k=1$, this reduces to $\\mathbb{R}$ and the norm reduces to the standard absolute value.\n",
    "  \n",
    "  - Any norm will do in $\\mathbb{R}^k$ because [all norms are equivalent on finite dimensional vector spaces](https://proofwiki.org/wiki/Norms_on_Finite-Dimensional_Real_Vector_Space_are_Equivalent).\n",
    "  \n",
    "  - A norm we often use in this class is the $\\sup$-norm (sometimes called the $\\infty$-norm) defined by\n",
    "  <br>\n",
    "$$\n",
    "    \\| x\\|_\\infty = \\sup_{1\\leq i\\leq k} | x^{(i)}| = \\max_{1\\leq i\\leq k} | x^{(i)}|\n",
    "$$\n",
    "<br>\n",
    "    note that we can interchange $\\sup$ and $\\max$ when the sets are finite.\n",
    "    \n",
    "We explore how to compute the norms of finite-dimensional vectors by creating some 1-d arrays in [`numpy`](https://numpy.org/) and using the [`norm` method within the `linalg` subpackage of `numpy`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8099ae1e-b39b-489c-addc-9318ad048008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Now we can create arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb4f79e-a798-4f4a-980b-a7bfcf1af3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, -2, 3, -4, 5, -6])  # A 1-d array of length 6 representing a 6-dimensional vector\n",
    "\n",
    "x.shape  # This will display (6,) meaning we think of this 1-d array as a 6-dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05d256-79a1-4373-863b-0ca2ac71e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(x)  # The default is the Euclidean/2-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce69307-c3ed-42c2-a675-d3a3876cc9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(x, ord=2)  # The ord keyword specifies the type of norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039faadc-d703-42fb-8297-185cea1412ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(x, ord=np.inf)  # Use the built-in np.inf constant to specify the infinity-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94107375-69f5-4ddb-90f4-6d8227270093",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(x, ord=1)  # This is the 1-norm defined by the sum of the absolute value of the components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dfb9b1-e17d-4fad-93e2-e26ea93dab1a",
   "metadata": {},
   "source": [
    "Below, we show how to create a vector with random entries and do some fancing printing of code by displaying Markdown formatted text as output from a code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5070d95-a5bf-45af-97b2-260d611802a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows us to print fancy looking outputs from code by defining a printmd function\n",
    "# to display Markdown formatted outputs.\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c8948-341f-4aa0-b84c-741c2ac9e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below we create a random 11-dimensional vector where each component \n",
    "# is drawn from a N(mu, sigma^2) distribution\n",
    "mu = 5\n",
    "sigma = 10\n",
    "y = np.random.normal(loc = mu, scale = sigma, size=(11,))\n",
    "\n",
    "# Now we print y and the 2- and infinity-norms\n",
    "print('~'*100)\n",
    "print('y = ', y)\n",
    "print('~'*100)\n",
    "# The r in beginning of the printmd(r ...) is used to treat the string as a raw string that treats\n",
    "# the backslashes as literal characters. This is useful and sometimes necessary when trying to \n",
    "# display nice looking LaTeX style outputs\n",
    "printmd(r'$\\| y\\|_2 \\approx$ {:.5f}'.format(np.linalg.norm(y)))\n",
    "printmd(r'$\\| y\\|_\\infty \\approx$ {:.5f}'.format(np.linalg.norm(y, ord=np.inf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaa41ac-170f-4b9f-8eef-b5fafa705611",
   "metadata": {},
   "source": [
    "- $\\mathcal{C}([a,b])$ is commonly used to denote continuous real-valued functions defined on an interval $[a,b]$ where $a<b$ are real numbers. For instance, $\\mathcal{C}([0,1])$ is the space of all continuous real-valued functions defined on the interval $[0,1]$.\n",
    "\n",
    "  - This becomes a Banach space when equipped with the $\\sup$-norm metric, denoted by $\\|\\cdot\\|_\\infty$ and defined for any $f\\in\\mathcal{C}([a,b])$ as\n",
    "<br><br>\n",
    "$$\n",
    "      \\| f\\|_\\infty := \\sup_{a\\leq x\\leq b} |f(x)|.\n",
    "$$\n",
    "\n",
    "  - Note that by the extreme value theorem, we can replace the $\\sup$ by a $\\max$ since a continuous real-valued function defined on a closed and bounded interval must achieve its maximum and minimum values.\n",
    "  \n",
    "Unlike the previous examples of $\\mathbb{R}^k$ where each vector is finite-dimensional so that computing the norms on a computer is possible, the vectors in $\\mathcal{C}([a,b])$ (yes, the elements of this space are functions and functions are types of vectors) are intrinsically infinite-dimensional. \n",
    "\n",
    "We can compute an estimate of the $\\sup$-norm metric of any $f\\in\\mathcal{C}([a,b])$ by creating an array of points in $[a,b]$, evaluating the function at each of the points in this array, and then treating this array of points like a finite-dimensional vector for which it is trivial to compute the $\\sup$-norm. \n",
    "\n",
    "We do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0347a77-37d0-4217-a939-58d663e60e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create an array of points to discretize [a,b]\n",
    "# Students are encouraged to change these values of variables below.\n",
    "a = 0 \n",
    "b = 1\n",
    "x = np.linspace(a, b, num=100)  # Creates evenly spaced numbers from a to b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc83f944-c00b-4d8a-b7b4-450fd73bf2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a function\n",
    "f = lambda x : x**2 - np.exp(x)*np.sin(np.pi*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de09886c-f717-4776-a913-2c23b2f9364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(f(x), ord=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df09c9e3-b132-42fa-a027-ee6617034e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # Now we can make some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b371f3e-f359-4e8e-afac-3b7b9b683af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.figure(0)\n",
    "plt.plot(x, f(x), label='$f(x)$')\n",
    "plt.plot(x, np.abs(f(x)), ls='-.', label='$|f(x)|$')\n",
    "plt.plot(x, np.linalg.norm(f(x), ord=np.inf)*np.ones(len(x)),\n",
    "         ls = ':', label=r'$\\approx \\|f(x)\\|_\\infty$')\n",
    "plt.legend(fontsize=18)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d3675-e74b-4520-8868-525ee72ed154",
   "metadata": {},
   "source": [
    "---\n",
    "#### Back to some definitions and notation\n",
    "---\n",
    "\n",
    "**Rate of Convergence (ROC)**. Let $X$ be a Banach space and $x_n\\to x\\in X$. We say that $x_n\\to x$ with the rate $\\alpha\\in\\mathbb{R}$ if there exists a constant $c\\in\\mathbb{R}$ (not depending on $n$) such that\n",
    "\n",
    "$$ \\large \\|x_n-x\\| \\leq c\\left(\\frac{1}{n}\\right)^\\alpha. $$\n",
    "   \n",
    "- If $\\alpha=1$, we say that the rate of convergence is either first-order or linear.\n",
    "\n",
    "- If $\\alpha=2$, we say that the rate of convergence is either second-order of quadratic.\n",
    "You should get the idea for $\\alpha > 2$.\n",
    "\n",
    "**Superlinear Convergence**. We say that a sequence $(x_n)\\subset X$ converges superlinearly towards $x\\in X$ if there is a positive sequence of real numbers $(c_n)\\subset\\mathbb{R}$ such that $c_n\\to 0$ and $\\|x_n-x\\| \\leq \\dfrac{c_n}{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e1118-1d63-4dc2-a504-0ec32b82f755",
   "metadata": {},
   "source": [
    "**The $\\mathcal{O}$-Notation (\"Big-Oh\" Notation)**. <mark>(This is not a standard definition of the notation, but it is useful for the purposes of this class.)</mark> Let $X$ be a Banach space, $(x_n)\\subset X$, and $(y_n)$ a sequence of real numbers with $y_n\\geq 0$ for all $n$. If there is a finite constant $c$, not depending on $n$, such that \n",
    "\n",
    "$$\\large \\|x_n\\| \\leq cy_n \\ \\ \\ \\forall \\ n\\geq 1,$$\n",
    "\n",
    "we say that the sequence $(x_n)$ is of order $(y_n)$, and we write, \n",
    "\n",
    "$$\\large x_n=\\mathcal{O}(y_n).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ddc573-da08-43ca-a1bd-82f54bcc2ec8",
   "metadata": {},
   "source": [
    "---\n",
    "#### Connecting $\\mathcal{O}$-notation with ROC and practical estimation of ROC\n",
    "---\n",
    "\n",
    "<mark>When a sequence converges, it is useful to conceptualize each element in the sequence as an approximation to the limit.</mark> \n",
    "\n",
    "When the space is a Banach space, then we can make sense of $x_n-x$ as some approximation to the zero vector. In other words, we think of the ***sequence of errors*** $(e_n):=(x_n-x)$ defined by each approximation subtracted from the limit. We are then interested in the ROC of $\\| e_n \\|\\to 0$. It is sometimes convenient to directly define the error sequence as $(e_n)=(\\|x_n-x\\|)$ so that the error vectors are transformed into a sequence of real numbers that should converge to zero. \n",
    "\n",
    "It turns out that many useful methods for estimating solutions have a ROC that is expressed in terms of $\\mathcal{O}(h(n)^\\alpha)$ where $n$ defines something related to the \"effort\" used in the method and $h(n)$ maps this to some sort of \"discretization\" parameter. In this class, we will often see that $n$ refers to the number of interior points used to discretize an interval like $[0,1]$ and $h(n)=\\dfrac{1}{n+1}$ is the spacing between these interior points. Methods are often referred to as first- or second-order if $\\alpha=1$ or $\\alpha=2$. \n",
    "\n",
    "This still does not address how we actually estimate the ROC. We do this by taking logarithms of both sides of the inequality in the definition of ROC with $h(n)$ replacing $1/n$ above to yield\n",
    "    \n",
    "$$\n",
    "    \\large \\log \\|x_n-x\\| \\leq \\alpha \\log \\left(c h(n)\\right) = \\alpha\\log c + \\alpha \\log\\left(h(n)\\right). \n",
    "$$\n",
    "\n",
    "With this, we can perform regression using ``polyfit`` within ``numpy`` (see https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html for more info) to fit a line to the logarithm of a few terms from the sequences $\\|x_n-x\\|$ and $h(n)$ and take the slope of that line to get an estimate of the ROC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42100cb8-55ab-4c9d-9191-3eefaa902d4e",
   "metadata": {},
   "source": [
    "**Before we estimate ROC.** An important point is that we first must determine the limit of $(x_n)$, which is not always obvious. Plots are useful tools if the Banach space can be easily visualized. However, we often must resort to ***the method of manufactured solutions*** to numerically investigate or verify ROCs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc5e262-a73c-4b93-8c7c-71519673b0ff",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.2.2: Examples of real-valued sequences converging to zero\n",
    "---\n",
    "\n",
    "Below, we show how to estimate the rate of convergence for some sequences that converge to zero.\n",
    "\n",
    "We know that \n",
    "\n",
    "- $x_n = \\sqrt{\\dfrac{1}{n}} \\Rightarrow x_n = \\mathcal{O}\\left(\\left(\\dfrac{1}{n}\\right)^{1/2}\\right)$\n",
    "\n",
    "- $x_n = \\sin(1/n) \\approx 1/n$ if $n$ is sufficiently large since $\\sin(\\theta)\\approx \\theta$ if $|\\theta|\\ll 1$ (i.e., if $\\theta$ is close to zero). Thus, $x_n=\\sin(1/n) = \\mathcal{O}\\left(\\left(\\dfrac{1}{n}\\right)\\right)$.\n",
    "\n",
    "- By a similar argument as the above example, $x_n = \\sqrt{\\dfrac{1}{n}} \\sin^2(1/n) = \\mathcal{O}\\left(\\left(\\dfrac{1}{n}\\right)^{5/2}\\right)$.\n",
    "\n",
    "\n",
    "We explore these in code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094808d-fe66-4ad7-9f69-b4e81586d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50  # Define the number of terms to compute for each sequence\n",
    "n = np.linspace(1, N, num=N)  # Create an array of integers from 1 to N\n",
    "\n",
    "x1 = np.sqrt(1./n)  # Seq. 1\n",
    "x2 = np.sin(1./n)  # Seq. 2\n",
    "x3 = np.sqrt(1./n) * np.sin(1./n)**2  # Seq. 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15017c77-c445-4bc3-a825-b8d2a810d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "fig = plt.figure(1, figsize=(5, 15))  \n",
    "\n",
    "ax1 = fig.add_subplot(3, 1, 1)  # 3x1 array - subplot 1\n",
    "ax2 = fig.add_subplot(3, 1, 2)  # 3x1 array - subplot 2\n",
    "ax3 = fig.add_subplot(3, 1, 3)  # 3x1 array - subplot 3\n",
    "\n",
    "ax1.set_title('$x_n = \\sqrt{1/n}$')\n",
    "ax2.set_title('$x_n = \\sin(1/n) $')\n",
    "ax3.set_title('$x_n = \\sqrt{1/n}\\sin^2(1/n)$')\n",
    "\n",
    "ax1.set_xlim(1, N)\n",
    "ax2.set_xlim(1, N)\n",
    "ax3.set_xlim(1, N)\n",
    "\n",
    "ax1.scatter(n, x1)\n",
    "ax2.scatter(n, x2)\n",
    "ax3.scatter(n, x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e31280-52c8-4f8f-83c1-de3be8a3e090",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lim = 0  # limit for each sequence\n",
    "\n",
    "# Remember to always take the absolute value before computing the \n",
    "# log of errors since errors can be positive or negative\n",
    "x1_errors = np.abs(x1 - x_lim)\n",
    "x2_errors = np.abs(x2 - x_lim)\n",
    "x3_errors = np.abs(x3 - x_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928727df-594d-44a1-9fee-ebd54172bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we append a [0] to the end of the following polyfit calls, then we only return the slopes\n",
    "ROC_1 = np.polyfit(np.log(1./n), np.log(x1_errors), 1)[0]\n",
    "ROC_2 = np.polyfit(np.log(1./n), np.log(x2_errors), 1)[0]\n",
    "ROC_3 = np.polyfit(np.log(1./n), np.log(x3_errors), 1)[0]\n",
    "\n",
    "print(\"ROC for $x_1$ = \", ROC_1)\n",
    "print(\"ROC for $x_2$ = \", ROC_2)\n",
    "print(\"ROC for $x_3$ = \", ROC_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd87d89-59f1-42d1-85a0-0cab0dd3b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "fig = plt.figure(2, figsize=(5, 15))\n",
    "\n",
    "ax1 = fig.add_subplot(3, 1, 1)  # 3x1 array - subplot 1\n",
    "ax2 = fig.add_subplot(3, 1, 2)  # 3x1 array - subplot 2\n",
    "ax3 = fig.add_subplot(3, 1, 3)  # 3x1 array - subplot 3\n",
    "\n",
    "axs = [ax1, ax2, ax3]\n",
    "ROCs = [ROC_1, ROC_2, ROC_3]\n",
    "errors = [x1_errors, x2_errors, x3_errors]\n",
    "sequence_strs = [r'$\\sqrt{\\dfrac{1}{n}}$', r'$\\sin(1/n)$', r'$\\sqrt{\\dfrac{1}{n}} \\sin^2(1/n)$']\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].loglog(1./n, errors[i], label='log of $|x_n - 0|$')\n",
    "    \n",
    "    slope_str = \"{:1.2f}\".format(ROCs[i])\n",
    "    \n",
    "    axs[i].loglog(1./n, (1./n * (errors[i][0]+1))**ROCs[i],  # Note the formula used to vertically offset this plot from the prior\n",
    "               'r:', label='slope = ' + slope_str)\n",
    "    \n",
    "    title_str = sequence_strs[i] + r' has  ROC $\\approx$' + slope_str\n",
    "    \n",
    "    axs[i].set_title(title_str)\n",
    "    axs[i].set_xlabel('log of $1/n$')\n",
    "    axs[i].legend(loc='upper left', shadow=True)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8ea72e-c17e-4eaa-a74d-c8354552acf0",
   "metadata": {},
   "source": [
    "---\n",
    "#### ROC with $h=\\dfrac{1}{n}$\n",
    "---\n",
    "\n",
    "When discretizing PDEs, we often let $h$ denote a spatial discretization parameter in a particular direction. \n",
    "If we use \"regular\" discretizations, then this means we specify a number of points in each direction to define a grid where the spacing of points in a particular direction is uniform. \n",
    "We often use $n$ to refer to the number of interior points in a direction, which means that the spacing between the grid points is given by $h=\\dfrac{c}{n+1}$ where $c$ is the length of the direction. Usually $c=1$ when the length of the direction is normalized to unity. \n",
    "Thus, as $n\\to\\infty$, $h\\to 0$, so all the definitions are easily changed by making the substitutions of $h$ for $1/n$ or $1/(n+1)$ (whichever makes the most sense for the given problem) and defining the ROC as $h\\to 0$ instead of as $n\\to\\infty$.\n",
    "\n",
    "We now estimate the ROC as $h\\to 0$ for the prior sequences where $h=1/n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca47812b-9cf9-4ce9-b89a-c3f6a7f92a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1/n\n",
    "\n",
    "ROC_1 = np.polyfit(np.log(h), np.log(x1_errors), 1)[0]\n",
    "ROC_2 = np.polyfit(np.log(h), np.log(x2_errors), 1)[0]\n",
    "ROC_3 = np.polyfit(np.log(h), np.log(x3_errors), 1)[0]\n",
    "\n",
    "printmd(\"ROC for $x_1$ = {:5f}\".format(ROC_1))\n",
    "printmd(\"ROC for $x_2$ = {:5f}\".format(ROC_2))\n",
    "printmd(\"ROC for $x_3$ = {:5f}\".format(ROC_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e12a4a-ed63-4513-9f20-a951c752166f",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.2.3: Estimating ROC for derivative approximations to smooth functions\n",
    "---\n",
    "\n",
    "What is a ***smooth function***?\n",
    "\n",
    "> A smooth function is as smooth as we need it to be. In other words, it has as many derivatives as is required by the analysis.\n",
    "\n",
    "Below, we show simple [finite difference formulas](https://en.wikipedia.org/wiki/Finite_difference) for estimating derivatives of a ***smooth*** function of a single variable. \n",
    "They are all derived via simple Taylor series expansions. We make use of these throughout the course.\n",
    "\n",
    "- A (first-order) forward finite difference scheme for $f'$: $\\displaystyle \\frac{f(x+h) - f(x)}{h} = f'(x) + O(h)$\n",
    "\n",
    "- A (first-order) backward finite difference scheme for $f'$: $\\displaystyle \\frac{f(x) - f(x-h)}{h} = f'(x) + O(h)$\n",
    "\n",
    "- A (second-order) centered finite difference scheme for $f'$: $\\large \\frac{f(x+h) - f(x-h)}{h} = f'(x) + O(h^2)$\n",
    "\n",
    "- A (second-order) centered finite difference scheme for $f''$: $\\large \\frac{f(x+h) - 2f(x) + f(x-h)}{h^2} = f''(x) + O(h^2)$\n",
    "\n",
    "Below, we derive the first and third ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da5a16-dc41-4303-91e7-36b2d851394a",
   "metadata": {},
   "source": [
    "---\n",
    "#### Derivation of (first-order) forward finite difference scheme for $f'$\n",
    "---\n",
    "By the [Taylor series formula](https://en.wikipedia.org/wiki/Taylor_series), we have\n",
    "\n",
    "$$\n",
    "    \\large f(x+h) = f(x) + f'(x)h + f''(x)\\frac{h^2}{2} + {\\cal O}(h^3), \n",
    "$$\n",
    "\n",
    "which we use to solve for $f'(x)$ to derive the particular finite difference scheme. We show the steps below.\n",
    "\n",
    "$$ \\large\n",
    "\\begin{align}\n",
    " f(x+h) - f(x) &= f'(x)h + f''(x)\\frac{h^2}{2} + {\\cal O}(h^3) \\\\ \\\\ \n",
    " \\Rightarrow \\frac{f(x+h)-f(x)}{h} &= \\underbrace{f'(x) + + f''(x)\\frac{h}{2} + \\frac{{\\cal O}(h^3)}{h}}_{\\text{Notice the impact of dividing through by $h$}} \\\\ \\\\\n",
    " \\Rightarrow \\frac{f(x+h) - f(x)}{h} &= f'(x) + \\underbrace{{\\cal O}(h)}_{=f''(x)\\frac{h}{2}} + \\underbrace{{\\cal O}(h^2)}_{=\\frac{{\\cal O}(h^3)}{h}} \\\\ \\\\\n",
    " \\Rightarrow \\frac{f(x+h) - f(x)}{h} &= f'(x) + {\\cal O}(h)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Above, we used the fact that if $\\alpha>1$, then ${\\cal O}(h) + {\\cal O}(h^\\alpha) = {\\cal O}(h)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde0b05-0850-48c7-acf0-709150aea3f2",
   "metadata": {},
   "source": [
    "---\n",
    "#### Derivation of (second-order) centered finite difference scheme for $f'$\n",
    "---\n",
    "\n",
    "By the Taylor series formula we have both\n",
    "\n",
    "$$\n",
    " \\large   f(x+h) = f(x) + f'(x)h + f''(x)\\frac{h^2}{2} + f'''(x)\\frac{h^3}{6}+{\\cal O}(h^4),\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    " \\large   f(x-h) = f(x) - f'(x)h + f''(x)\\frac{h^2}{2} - f'''(x)\\frac{h^3}{6}+{\\cal O}(h^4).\n",
    "$$\n",
    "\n",
    "Then, we compute $f(x+h)-f(x-h)$ from which it follows that\n",
    "\n",
    "$$\n",
    " \\large   f(x+h) - f(x-h) = 2f'(x)h + \\underbrace{f'''(x)\\frac{h^3}{3}}_{={\\cal O}(h^3)} + h.o.t.\n",
    "$$\n",
    "\n",
    "and dividing by $2h$ to both sides gives the result since ${\\cal O}(h^3)/(2h) = {\\cal O}(h^2)$. Above, \"$h.o.t.$\" stands for \"higher order terms\" to denote terms that are ${\\cal O}(h^\\alpha)$ for some $\\alpha>3$ that are subsequently dropped in the analysis. $\\Box$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878412b9-10e7-421c-ab8e-9f0a65308772",
   "metadata": {},
   "source": [
    "---\n",
    "#### Numerical verification of ROC for finite difference schemes (and some plotting and widget exploration)\n",
    "---\n",
    "\n",
    "Below, we see the following:\n",
    "\n",
    "- How to implement all four of the finite difference schemes shown above and numerically investigate the ROC for all of them.\n",
    "\n",
    "- How to make use of various plotting/labeling techniques/tricks that are, in general, useful to know.\n",
    "\n",
    "- Issues involving numerical ROC studies that arise from finite-precision arithmetic (i.e., the limitations of using a computer). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affe6a84-e276-497b-b515-e5a63eb32d79",
   "metadata": {},
   "source": [
    "<mark>**First, we show the remarkably simple implementations of the various finite difference methods from above.**</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5108c64c-32ca-45c2-a003-f033d467610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pay attention to the parentheses in the code. Common coding mistakes involve not getting\n",
    "# parentheses correct when implementing formulas/equations with fractions in code.\n",
    "\n",
    "def fp_forward(f, x, h=0.01):  # The forward FD (FFD) approx to $f'$\n",
    "    return (f(x+h)-f(x))/h\n",
    "\n",
    "def fp_backward(f, x, h=0.01):  # The backward FD (BFD) approx to $f'$\n",
    "    return (f(x)-f(x-h))/h\n",
    "\n",
    "def fp_centered(f, x, h=0.01):  # The centered FD (CFD) approx to $f'$\n",
    "    return (f(x+h)-f(x-h))/(2*h)  # Notice the parentheses around (2*h) in the denominator (a common mistake is to omit these)\n",
    "\n",
    "def fpp_centered(f, x, h=0.01):  # The centered FD (CFD2) approx to $f''$\n",
    "    return (f(x+h)-2*f(x)+f(x-h))/(h**2)  # The parentheses in the denominator are not necessary but improve the readability of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3758a4f-1640-4f00-b836-a1fa287b6de8",
   "metadata": {},
   "source": [
    "<mark>**Next, we use `sympy` to manufacture a solution to test these methods.**</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badd01eb-f184-4133-b51f-937774d2fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sympy to manufacture a solution\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ae9b6-e3ca-41ef-802f-e8b611d94f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A manufactured solution\n",
    "x = sp.symbols('x')\n",
    "\n",
    "u = sp.exp(-x**2)*sp.sin(3*sp.pi*x)\n",
    "u_p = u.diff(x, 1)\n",
    "u_pp = u.diff(x, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af3a0b5-cac2-446e-b13c-b4555c7cd474",
   "metadata": {},
   "outputs": [],
   "source": [
    "u  # Look at $u$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbc340b-3ba7-41d3-98f3-094627ae6ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_p  # Look at $u'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7feb20e-d459-4254-a065-9af1e5daf1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pp  # Look at $u''$. Who would want to compute this by hand?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6888d49d-3aa5-478c-bb87-f207befdc69f",
   "metadata": {},
   "source": [
    "<mark>**We need to transform the symbolic functions into computable functions.**</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f929d0e-18a0-4643-bfdb-fb5592d26671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to transform the symbolic functions into functions we can evaluate (and in particular evaluate on numpy arrays).\n",
    "# To this end, we use the lambdify function within sympy which turns a symbolic function into one we can evaluate.\n",
    "\n",
    "u_eval = sp.lambdify(x, u) \n",
    "u_p_eval = sp.lambdify(x, u_p)\n",
    "u_pp_eval = sp.lambdify(x, u_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1edf12-3588-4670-aab2-6dda0297e55d",
   "metadata": {},
   "source": [
    "<mark>**We can now plot the function $u$ and its derivatives $u'$ and $u''$ along with their finite difference approximations. Note the use of the `sympy` method `latex` used to create \"fancy\" labels for the titles of the plots. Also note that way in which we create some of the labels for plots with \" \" instead of ' '.**</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d771af6-d71b-4757-ab25-0150ed2b05fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now plot the function and its derivatives (and derivative approximations)\n",
    "%matplotlib widget\n",
    "\n",
    "fig = plt.figure(3, figsize=(8, 12))\n",
    "\n",
    "ax1 = fig.add_subplot(3, 1, 1)  # 3x1 array - subplot 1\n",
    "ax2 = fig.add_subplot(3, 1, 2)  # 3x1 array - subplot 2\n",
    "ax3 = fig.add_subplot(3, 1, 3)  # 3x1 array - subplot 3\n",
    "\n",
    "xs = np.linspace(0, 1, 100)\n",
    "h = 0.1\n",
    "\n",
    "ax1.plot(xs, u_eval(xs))\n",
    "ax1.set_title('$' + sp.latex(u) + '$')  # A neat trick for turning a symbolic function into a LaTeX string for labeling\n",
    "\n",
    "ax2.plot(xs, u_p_eval(xs), label=\"$u'$\")  # Notice the use of \" \" for the label so that the ' can be used within the label\n",
    "ax2.plot(xs, fp_forward(u_eval, xs, h=h), ls=':', label='FFD approx.')\n",
    "ax2.plot(xs, fp_backward(u_eval, xs, h=h), ls='-.', label='BFD approx.')\n",
    "ax2.plot(xs, fp_centered(u_eval, xs, h=h), ls='--', label='CFD approx.')\n",
    "ax2.set_title('$' + sp.latex(u_p) + '$')\n",
    "ax2.legend(fontsize=14)\n",
    "\n",
    "ax3.plot(xs, u_pp_eval(xs), label=\"$u''$\")\n",
    "ax3.plot(xs, fpp_centered(u_eval, xs, h=h), ls=':', label='CFD2 approx.')\n",
    "ax3.set_title('$' + sp.latex(u_pp) + '$')\n",
    "ax3.legend(fontsize=14)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e550515b-c77b-4f79-99a1-d05abbf64f74",
   "metadata": {},
   "source": [
    "<mark>**This is a good place to use widgets for greater interactivity so that we can explore the impact of changing the `h` parameter in the finite difference functions. Observe what happens to the second derivative approximation as soon as `h` is around $10^{-8}$. Why do you think this is? Why does it take longer for issues to arise in the first-order derivative approximations?**</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa244f3b-5dc4-4545-86a5-5590b6b1686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for widgets!\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d26c8-9d6c-42c3-bc45-f981e86fc37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_derivs(f, xs, h, f_p=None, f_pp=None, fignum=4):\n",
    "    fig = plt.figure(fignum, figsize=(8, 8))\n",
    "    plt.clf()\n",
    "    \n",
    "    ax1 = fig.add_subplot(2, 1, 1)  \n",
    "    ax2 = fig.add_subplot(2, 1, 2) \n",
    "\n",
    "    if f_p is not None:\n",
    "        ax1.plot(xs, f_p(xs), label=\"$u'$\")  \n",
    "    ax1.plot(xs, fp_forward(f, xs, h=h), ls=':', label='FFD approx.')\n",
    "    ax1.plot(xs, fp_backward(f, xs, h=h), ls='-.', label='BFD approx.')\n",
    "    ax1.plot(xs, fp_centered(f, xs, h=h), ls='--', label='CFD approx.')\n",
    "    ax1.legend(fontsize=14)\n",
    "\n",
    "    if f_pp is not None:\n",
    "        ax2.plot(xs, f_pp(xs), label=\"$u''$\")\n",
    "    ax2.plot(xs, fpp_centered(f, xs, h=h), ls=':', label='CFD2 approx.')\n",
    "    ax2.legend(fontsize=14)\n",
    "    \n",
    "    plt.show()  # Needed in a function \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016043a0-957b-4b95-9388-f104ab5ff1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "interact_manual(plot_derivs,\n",
    "                f = fixed(u_eval),\n",
    "                f_p = fixed(u_p_eval),\n",
    "                f_pp = fixed(u_pp_eval),\n",
    "                xs = fixed(np.linspace(0, 1, 100)),\n",
    "                h = widgets.FloatLogSlider(value=0.1, base=10, max=-1, min=-16, step=1),\n",
    "                fignum = fixed(4)\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef47e835-6e6e-45ae-be05-799f3271b2be",
   "metadata": {},
   "source": [
    "<mark>**Now we numerically explore the ROC.**</mark>\n",
    "\n",
    "Note that the theoretical ROC is defined pointwise. Is this ROC *uniform* across the domain $[0,1]$ for the function studied above?\n",
    "\n",
    "In other words, given a function $f$ that is continuously differentiable, let $f'_h$ and $f''_h$ denote some finite difference approximations to $f'$ and $f''$, respectively. Suppose at any particular $x$-value that $f'_h(x)$ is an $\\alpha$-order approximation to $f'(x)$ (i.e., $|f'_h(x) - f'(x)| = \\mathcal{O}(h^\\alpha)$) and $f''_h(x)$ is a $\\beta$-order approximation to $f''(x)$ (i.e., $|f''_h(x) - f''(x)|=\\mathcal{O}(h^\\beta)$). Is it the case that $\\|f'_h - f'\\|_\\infty = \\mathcal{O}(h^\\alpha)$ and $\\|f''_h-f''\\|_\\infty = \\mathcal{O}(h^\\beta)$?\n",
    "\n",
    "This is not as straightforward a question as it might appear because pointwise convergence does not, in general, imply uniform convergence (consider, for example, $f_n(x) = x^n$ for each $n\\in\\mathbb{N}$ on $[0,1]$ which converges pointwise, but not uniformly, to a function that is equal to $0$ for $x\\in[0,1)$ and equal to $1$ for $x=1$). \n",
    "\n",
    "We numerically investigate this below.\n",
    "\n",
    "***Note that `n=4` is the largest value that can be chosen to achieve numerical ROC that match what we expect from the theory for each method. Why is this?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c7a40-5436-48dc-8ca6-336f5577b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(0, 1, 200)  # Discretization of [0,1]\n",
    "\n",
    "# Try n=4, 5, 6, 7, and 8 below\n",
    "n = 5  # Defines 10^{-n} smallest h value and also the number of orders of magnitude of h to consider from 10^{-n} to 10^{-1}\n",
    "hs = np.logspace(start=-n, stop=-1, num=n)  \n",
    "print(hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56c8e60-fae0-4a5e-8cdc-21bef8c86487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the error arrays that hold the sup-norm estimates of errors for various h\n",
    "errors_FFD = np.zeros(n)\n",
    "errors_BFD = np.zeros(n)\n",
    "errors_CFD = np.zeros(n)\n",
    "errors_CFD2 = np.zeros(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13fde9-211e-4d1e-99f5-d08fe081bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    errors_FFD[i] = np.linalg.norm(fp_forward(u_eval, xs, hs[i]) - u_p_eval(xs), ord=np.inf)\n",
    "    errors_BFD[i] = np.linalg.norm(fp_backward(u_eval, xs, hs[i]) - u_p_eval(xs), ord=np.inf)\n",
    "    errors_CFD[i] = np.linalg.norm(fp_centered(u_eval, xs, hs[i]) - u_p_eval(xs), ord=np.inf)\n",
    "    errors_CFD2[i] = np.linalg.norm(fpp_centered(u_eval, xs, hs[i]) - u_pp_eval(xs), ord=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7ee45-e1bb-42df-ba3b-58b159f019bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Uniform_ROC_FFD = np.polyfit(np.log(hs), np.log(errors_FFD), 1)[0]\n",
    "Uniform_ROC_BFD = np.polyfit(np.log(hs), np.log(errors_BFD), 1)[0]\n",
    "Uniform_ROC_CFD = np.polyfit(np.log(hs), np.log(errors_CFD), 1)[0]\n",
    "Uniform_ROC_CFD2 = np.polyfit(np.log(hs), np.log(errors_CFD2), 1)[0]\n",
    "\n",
    "print(\"Uniform ROC for FFD = {:5f}\".format(Uniform_ROC_FFD))\n",
    "print(\"Uniform ROC for BFD = {:5f}\".format(Uniform_ROC_BFD))\n",
    "print(\"Uniform ROC for CFD = {:5f}\".format(Uniform_ROC_CFD))\n",
    "print(\"Uniform ROC for CFD2 = {:5f}\".format(Uniform_ROC_CFD2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba1daa-25da-435e-9f52-bfbd5ad6105b",
   "metadata": {},
   "source": [
    "<mark>**Plots are always very useful for helping us understand things. It is always worth it to try and visualize data.**</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce72a2-dd1e-4fec-95c3-b4c703e87bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plt.figure(5)\n",
    "# Feel free to change which error is plotted below although the CFD2 and CFD ones are the most interesting\n",
    "# Look at this plot if n is 5 or more above and it is the CFD2 error being plotted (or 7 or more for the CFD error)\n",
    "plt.loglog(hs, errors_CFD2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d65b9a2-c700-460d-aace-45caad10b34e",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1.2.4: An alternative ROC computation and detecting finite precision arithmetic issues \n",
    "---\n",
    "\n",
    "An alternative approach to computing the ROC by fitting a line to the \"loglog\" of all the data of normed errors vs. discretization parameters is to *iteraticaly track* how the ROC is evolving. This is, in essence, what is discussed on the Wiki page https://en.wikipedia.org/wiki/Rate_of_convergence.\n",
    "\n",
    "Put simply, we can compute the slopes of individual line segments connecting consecutive points in the loglog space of normed errors vs. discretization parameter. This will often reveal what, if any, magnitudes of discretization parameters result in significant finite precision arithmetic errors, but there is a catch (we will see this below).\n",
    "\n",
    "First, we recall that if $\\|e_h\\| = \\mathcal{O}(h^\\alpha)$, then there is a $c$, independent of $h$, such that $\\|e_h\\| = c\\left(h^\\alpha\\right)$, which implies\n",
    "\n",
    "$$\n",
    "    \\log \\|e_h\\| = \\log c + \\alpha \\log h.\n",
    "$$\n",
    "\n",
    "This subsequently implies for $h_1\\neq h_2$ that\n",
    "\n",
    "$$\n",
    "    \\log \\|e_{h_1} \\| - \\log \\| e_{h_2} \\| = \\alpha (\\log h_1 - \\log h_2).\n",
    "$$\n",
    "\n",
    "Since $\\log a - \\log b = \\log (a/b)$, we then have\n",
    "\n",
    "$$\n",
    "    \\alpha = \\frac{\\log\\left(\\frac{e_{h_1}}{e_{h_2}}\\right)}{\\log\\left(\\frac{h_1}{h_2}\\right)}.\n",
    "$$\n",
    "\n",
    "We show this in code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed73467-d428-4835-b077-950fbec02f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_ROC_estimates = np.log(errors_CFD2[1:-1]/errors_CFD2[0:-2]) / np.log(hs[1:-1]/hs[0:-2])\n",
    "\n",
    "print(iterative_ROC_estimates)  # Recall that the hs array is in increasing order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b68a28-66b6-4df1-a92c-e41b26bbff24",
   "metadata": {},
   "source": [
    "---\n",
    "#### The catch\n",
    "---\n",
    "\n",
    "This approach of iteratively monitoring $\\alpha$ may lead to incorrect ROC inferences. Or, at the very least, it is possible that it misleads one as to the ROC. In practice, it is important to look at the \"limiting behavior\" of the sequence of $\\alpha$ values computed using the method above (assuming that issues from finite precision arithmetic have been avoided).\n",
    "\n",
    "As an example, consider $e_h = |h\\log(h)|$ as $h\\to 0$ (and it should be clear we really mean $h\\downarrow 0$). \n",
    "\n",
    "First, does the limit even exist? Rewrite $e_h$ and observe that the limit is a so-called indeterminate form\n",
    "\n",
    "$$\n",
    " \\large   e_h = \\left|\\frac{\\log h}{1/h}\\right| \\to \\frac{\\infty}{\\infty} \\ \\ \\text{ as } h\\to 0.\n",
    "$$\n",
    "\n",
    "We then recall a certain rule about [hospitals from calculus](https://en.wikipedia.org/wiki/L%27H%C3%B4pital%27s_rule) to get\n",
    "\n",
    "$$\n",
    " \\large   \\lim_{h\\to 0} e_h = \\lim_{h\\to 0} \\left|\\frac{1/h}{-1/h^2}\\right| = \\lim_{h\\to 0} |h| = 0.\n",
    "$$\n",
    "\n",
    "Let's play with this numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221503b-913b-48ad-8e1a-19d871967833",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = np.logspace(-14, -1, 10)\n",
    "\n",
    "x_lame = np.abs(hs*np.log(hs))\n",
    "print(x_lame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493ff150-efa1-453a-af41-2c84b34ada33",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.log(x_lame[1:]/x_lame[0:-1])/np.log(hs[1:]/hs[0:-1])\n",
    "print('-'*50)\n",
    "print('Estimated ROC with successive differences: \\n', alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5056ba-712b-483a-8910-24537311293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*50)\n",
    "print('Using regression to estimate ROC: ', np.polyfit(np.log(hs), np.log(x_lame), 1)[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd1b184-50e5-4881-aa31-0e1212fb2e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "fig = plt.figure(4, figsize=(5,5))\n",
    "\n",
    "ax1 = fig.add_subplot(2, 1, 1)\n",
    "ax1.semilogx(hs[1:], alpha)\n",
    "\n",
    "ax2 = fig.add_subplot(2,1,2)\n",
    "ax2.loglog(hs, x_lame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1697d66a-2a0c-4295-8824-73854df59a2d",
   "metadata": {},
   "source": [
    "Note that as $h\\downarrow 0$ the computed $\\alpha$ values from the iterative method appear to be converging to $1$. The regression-based approach of finding a line of best fit was not much better (it is underestimating the ROC). \n",
    "\n",
    "The main lesson here is that we should not turn our minds off when studying ROC. Plots are always a good idea and numerical investigations are no substitute for actual theoretical investigation.\n",
    "\n",
    "All that being said, numerical methods are useful and a good first step to providing insight into how to tackle a problem theoretically or at least point us in the right direction of further mathematically rigorous investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b06df3-d029-42bf-999c-2918ecb71178",
   "metadata": {},
   "source": [
    "---\n",
    "#### Navigation:\n",
    "\n",
    "- [Previous](Chp1Sec0.ipynb)\n",
    "\n",
    "- [Next](Chp1Sec2.ipynb)\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
