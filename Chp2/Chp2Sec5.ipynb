{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c9c3fd-d4c8-483b-a0be-5397833b95ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to Partial Differential Equations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d145bc9d-5db7-4ffe-8e4c-d7fb98fcaed4",
   "metadata": {},
   "source": [
    "## Chapter 2: Elliptic PDEs, Poissonâ€™s Equation, and a Two-Point Boundary Value Problem \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68dbb88-049c-4097-a1e5-5c0f983ab49f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creative Commons License Information\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/80x15.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">Introduction to Partial Differential Equations: Theory and Computations</span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">Troy Butler</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>.<br />Based on a work at <a xmlns:dct=\"http://purl.org/dc/terms/\" href=\"https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations\" rel=\"dct:source\">https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6099d48-8788-4c41-b7aa-ec020f51137a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 2.5: Eigenvalues and Eigenfunctions/Eigenvectors\n",
    "---\n",
    "\n",
    "Students may want to review the material on eigenvalues/eigenvectors in [Section 1.5](./Chp1/Chp1Sec5.ipynb).\n",
    "\n",
    "In this notebook, we consider continuous eigenvalue problems of the form\n",
    "\n",
    "$$\n",
    "    Lu = \\lambda u, \\ u\\in \\mathcal{C}^2_0((0,1))\\backslash \\{0\\}\n",
    "$$\n",
    "\n",
    "and discrete eigenvalue problems of the form\n",
    "\n",
    "$$\n",
    "    L_h v = \\lambda v, \\ v\\in D_{h,0}\\backslash\\{0\\}.\n",
    "$$\n",
    "\n",
    "In other words, we are considering the 2-pt BVPs presented in the operator form studied in [Section 2.3](Chp2Sec3.ipynb) where the forcing function has the form of being a scalar multiple, $\\lambda$, of the solution.\n",
    "\n",
    "**Remarks:**\n",
    "\n",
    "- By \"$\\backslash\\{0\\}$\" we mean that we are looking for functions that are not identically equal to zero. The backslash notation is used to denote \"set subtraction\" meaning that the 0 vector is removed from consideration.\n",
    "\n",
    "- For a given $\\lambda\\in\\mathbb{C}$, if there exists a nonzero (continuous or discrete) function satisfying the above (continuous or discrete) eigenvalue problem, then we call such a $\\lambda$ an **eigenvalue** and the corresponding function an **eigenfunction.**\n",
    "\n",
    "  - We will show that any eigenvalues must actually be real (i.e., have zero imaginary component) due to the symmetry of the operators $L$ and $L_h$.\n",
    "\n",
    "\n",
    "  - For a given eigenvalue $\\lambda\\in\\mathbb{R}$, there is no unique way to represent the eigenfunction because any non-zero multiple of this eigenfunction also defines an eigenfunction associated with the eigenvalue. \n",
    "\n",
    "    In other words, if $u$ is a continuous eigenfunction associated with $\\lambda$, then so is $\\alpha u$ for any $\\alpha\\in\\mathbb{R}\\backslash\\{0\\}$, which is verified by direct substitution into the eigenvalue problem.\n",
    "  \n",
    "**Why are we studying this?**\n",
    "\n",
    "- This is related to Fourier series expansions that prove to be a useful tool for solving PDEs on geometrically \"nice\" domains. This is in fact a critical tool utilized throughout must of the rest of this class.\n",
    "\n",
    "- Some aspects of modern PDE theory rely on analysis of the [spectrum](https://en.wikipedia.org/wiki/Spectrum_(functional_analysis)) of the differential operator. The spectrum is a generalization of the concept of eigenvalues.\n",
    "\n",
    "- Analyzing the eigenvalues/spectrum of an operator provides us a lot of information about the solvability of problems and any conditions on the data that are required for a solution to exist. \n",
    "\n",
    "  - Consider the classic linear algebra $Ax=\\lambda x$ eigenvalue problem (obviously $A$ is a square matrix - why is this obvious?). \n",
    "  \n",
    "    If $\\lambda=0$ is an eigenvalue, then it tells us that the nullspace of $A$ is non-empty. Moreover, this tells us that $Ax=b$ can only be solved for *some* $b$ and that for the $b$ we can choose for which a solution to $Ax=b$ exists, we also know that the solutions are not unique because we can add to these solutions any vector in the nullspace of $A$. The lesson is that linear algebra is a subject too often taken for granted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c4a6b-d647-4a6d-9725-f16540d9d077",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 2.5.0: Peering into the future (a.k.a. motivation)\n",
    "---\n",
    "\n",
    "**The setup:**\n",
    "\n",
    "- Suppose $\\{\\lambda_n\\}_{n\\in\\mathbb{N}}$ is a sequence of eigenvalues with corresponding eigenfunctions $\\{u_n\\}_{n\\in\\mathbb{N}}$ for $L$.\n",
    "\n",
    "- Suppose $f\\in \\text{span}\\{u_n\\}_{n\\in\\mathbb{N}}$ meaning there exists constants $\\{c_n\\}_{n\\in\\mathbb{N}}$ such that\n",
    "\n",
    "$$\n",
    "    f(x) = \\sum_{n=1}^\\infty c_nu_n(x).\n",
    "$$\n",
    "\n",
    "- Further assume that $\\lambda_n\\neq 0$ for all $n$.\n",
    "\n",
    "**The claim:**\n",
    "\n",
    "We claim that\n",
    "\n",
    "$$\n",
    "    u(x) = \\sum_{n=1}^\\infty \\left(\\frac{c_n}{\\lambda_n}\\right) u_n(x)\n",
    "$$\n",
    "\n",
    "solves the continuous BVP with $Lu=f$.\n",
    "\n",
    "**The \"proof\" (missing rigor):**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    Lu(x) &= L\\left(\\sum_{n=1}^\\infty \\left(\\frac{c_n}{\\lambda_n}\\right) u_n(x) \\right) \\\\\n",
    "       \\\\\n",
    "       &= \\sum_{n=1}^\\infty \\left(\\frac{c_n}{\\lambda_n}\\right) Lu_n(x) \\quad\\color{red}{\\text{ (requires justification) }} \\\\\n",
    "       \\\\\n",
    "       &= \\sum_{n=1}^\\infty \\left(\\frac{c_n}{\\lambda_n}\\right) \\lambda_n u_n(x) \\\\\n",
    "       \\\\\n",
    "       &= \\sum_{n=1}^\\infty c_n u_n(x) \\\\\n",
    "       \\\\\n",
    "       &= f(x). \\ \\Box\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**The missing rigor:**\n",
    "\n",
    "Going from the first to second equality above requires justification because the infinite sum is interpreted as the *limit of a sequence of partial sums*.\n",
    "\n",
    "For each $N\\in\\mathbb{N}$, let $g_N:=\\sum_{n=1}^N \\left(\\frac{c_n}{\\lambda_n}\\right) u_n$. Then, the infinite sum is really just $\\lim_{N\\to\\infty} g_N(x)$. \n",
    "\n",
    "By *moving* the differential operator $L$ *through* the summation, we are *interchanging limiting processes* (recall that differentiation is itself defined by a limiting process).\n",
    "\n",
    "We must ***always be careful when interchanging limiting processes because the results are not always what we expect.***\n",
    "\n",
    "A lot of analysis is focused on when interchanging limiting processes leads to the same answers.\n",
    "\n",
    "We have examples where $\\frac{d}{dx} \\lim_{N\\to\\infty} g_N(x)\\neq \\lim_{N\\to\\infty} g'_N(x)$.\n",
    "\n",
    "Consider $g_N(x) = \\sin(Nx)/N$ which converges to the zero function *uniformly* (i.e., in the sup/$\\infty$-norm metric) on $\\mathbb{R}$, so $\\frac{d}{dx} \\lim_{N\\to\\infty} g_N(x) \\equiv 0$. Yet, $g'_N(x) = \\cos(Nx)$ does *not* converge for almost all $x\\in\\mathbb{R}$. \n",
    "\n",
    "**The future:**\n",
    "\n",
    "In chapter 3, we will discuss *why* we can interchange the limiting processes and also define the *game* (so to speak) which is to determine $\\{c_n\\}_{n\\in\\mathbb{N}}$ given an $f$ such that we can write $f$ as a (potentially infinite) linear combination of eigenfunctions.\n",
    "\n",
    "In [Section 2.5.4](#Section2.5.4), we provide an initial glimpse into the process and the approximation properties of truncated summations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e621b5d6-9360-49ff-a988-4ca812f66015",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id='Section2.5.1'>Section 2.5.1: Immediate properties of eigenvalues and eigenfunctions for $L$ and $L_h$</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71899e97-fffa-45ab-a556-cb1c0f168696",
   "metadata": {},
   "source": [
    "Recall from [Section 2.3](Chp2Sec.ipynb) the following lemma.\n",
    "\n",
    "---\n",
    "#### Lemma 2.3.1: Symmetry of Operators\n",
    "\n",
    "The operators $L$ and $L_h$ are symmetric.\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Suppose $\\lambda\\in\\mathbb{C}$ is an eigenvalue for $L$ and consider the associated eigenfunction $u\\in\\mathcal{C}^2_0((0,1))$. By Lemma 2.3.1, \n",
    "\n",
    "$$\n",
    "    \\langle Lu, u\\rangle = \\langle u, Lu\\rangle.\n",
    "$$\n",
    "\n",
    "Of course, $Lu=\\lambda u$, so this means that\n",
    "\n",
    "$$\n",
    "    \\langle\\lambda u, u \\rangle = \\langle u, \\lambda u\\rangle.\n",
    "$$\n",
    "\n",
    "However, if we are taking inner products for complex-valued (instead of real-valued) vector spaces, then the inner product must have the property that for any two vectors $u$ and $v$ and complex number $\\alpha$, \n",
    "\n",
    "$$\n",
    "    \\langle \\alpha u, v\\rangle = \\alpha \\langle u, v \\rangle, \\ \\text{ and } \\ \\langle u, \\alpha v\\rangle = \\overline{\\alpha}\\langle u, v \\rangle\n",
    "$$\n",
    "\n",
    "where $\\overline{\\alpha}$ denotes the complex conjugate. This means that\n",
    "\n",
    "$$\n",
    "    \\langle\\lambda u, u \\rangle = \\langle u, \\lambda u\\rangle \\Rightarrow \\lambda \\langle u, u \\rangle = \\overline{\\lambda} \\langle u, u\\rangle.\n",
    "$$\n",
    "\n",
    "The above is only true if $\\lambda=\\overline{\\lambda}$, and the only complex numbers that are equal to their complex conjugates are those with zero imaginary part. In other words, the symmetry of the operator implies that if any eigenvalues exist, then they must be real-valued. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834b74a6-08b0-442f-b035-e088eb99e70f",
   "metadata": {},
   "source": [
    "A similar argument as above applies to the eigenvalues of $L_h$. We summarize these results in the following lemma.\n",
    "\n",
    "---\n",
    "#### Lemma 2.5.1: Real-valued eigenvalues\n",
    "\n",
    "Any eigenvalues that exist for $L$ or $L_h$ are real-valued.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d55e275-29c3-4d41-9429-acc4727833ce",
   "metadata": {},
   "source": [
    "We now recall another lemma from [Section 2.3](Chp2Sec3.ipynb).\n",
    "\n",
    "---\n",
    "#### Lemma 2.3.2: Positive Defineteness of Operators\n",
    "\n",
    "The operators $L$ and $L_h$ are positive definite.\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "From this lemma, we have that if $\\lambda$ is an eigenvalue for $L$ with the associated eigenfunction $u$, then because $u$ is nonzero, we have\n",
    "\n",
    "$$\n",
    "    \\langle Lu, u \\rangle > 0.\n",
    "$$\n",
    "\n",
    "On the other hand, \n",
    "\n",
    "$$\n",
    "    \\langle Lu, u \\rangle = \\lambda \\langle u, u\\rangle, \n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "    \\lambda \\langle u, u \\rangle >0.\n",
    "$$\n",
    "\n",
    "Since $\\langle u, u \\rangle>0$ (it is the integral of a nonnegative function that must be positive at least somewhere since the function is nonzero and continuous), it follows that $\\lambda>0$ as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b54f72-a99d-4ef9-b328-db76a3daf14d",
   "metadata": {},
   "source": [
    "A similar argument as above applies to the eigenvalues of $L_h$. We summarize these results in the following lemma.\n",
    "\n",
    "---\n",
    "#### Lemma 2.5.2: Positivity of eigenvalues\n",
    "\n",
    "Any eigenvalues that exist for $L$ or $L_h$ are positive.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c5df8-0299-4005-bfe7-b1c320b5e0b5",
   "metadata": {},
   "source": [
    "Recall that inner products impart a *geometric structure* onto a vector space. When the inner product between two vectors is zero, the vectors are said to be orthogonal. \n",
    "\n",
    "Suppose $\\lambda$ and $\\gamma$ are eigenvalues of $L$ such that $\\lambda\\neq \\gamma$, and let $u$ and $v$ denote the eigenfunctions associated with these eigenvalues, respectively.\n",
    "\n",
    "Then, by symmetry of $L$ and properties of inner products, we have\n",
    "\n",
    "$$\n",
    "    \\lambda\\langle u, v\\rangle = \\langle Lu, v \\rangle = \\langle u, Lv \\rangle = \\gamma \\langle u, v\\rangle.\n",
    "$$\n",
    "\n",
    "The only way that $\\lambda \\langle u, v \\rangle = \\gamma \\langle u, v\\rangle$ is if $\\langle u, v \\rangle =0$. Thus, the eigenfunctions associated with distinct eigenvalues are orthogonal to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23b0d50-6e79-4cc1-9427-1a37853c89f0",
   "metadata": {},
   "source": [
    "A similar argument as above applies to $L_h$. We summarize these results in the following lemma.\n",
    "\n",
    "---\n",
    "#### Lemma 2.5.3: Orthogonality of eigenfunctions\n",
    "\n",
    "Eigenfunctions associated with distinct eigenvalues for $L$ or $L_h$ are orthogonal.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Lemmas 2.5.1 and 2.5.2 state that if $\\lambda$ is an eigenvalue for either $L$ or $L_h$, then necessarily $\\lambda\\in\\mathbb{R}$ and $\\lambda>0$. Lemma 2.5.3 states a geometric property of the eigenfunctions.**\n",
    "\n",
    "However, none of these results actually state that eigenvalues exist, what the values actually are, or what the associated eigenfunctions are! These are just statements of properties of things that may or may not exist!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35a0a35-e7fd-4691-856f-5b0727e1cf9a",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id='Section2.5.2'>Section 2.5.2: The eigenvalues and eigenfunctions of $L$</a>\n",
    "---\n",
    "\n",
    "It turns out that we can actually determine what the eigenvalues are for $L$ and $L_h$ because the associated eigenproblems are simple enough for us to analyze with basic ODE and linear algebra techniques. We just have to be clever enough to do so.\n",
    "\n",
    "For the continuous problem, suppose that $\\lambda$ is an eigenvalue. From Lemmas 2.5.1 and 2.5.2, we know that $\\lambda$ is some positive real number, i.e., $\\lambda>0$. This means we can take the square root to get another real number that we denote by $\\beta=\\sqrt{\\lambda}$.\n",
    "\n",
    "Why do we do this? Because it simplifies notation as we will see.\n",
    "\n",
    "Since $\\lambda=\\beta^2$, we have that the associated eigenfunction $u$ satisfies the following equation\n",
    "\n",
    "$$\n",
    "    -u'' = \\beta^2 u \\Rightarrow u'' + \\beta^2 u = 0.\n",
    "$$\n",
    "\n",
    "From elementary ODEs (specifically utilizing [characteristic equations for linear ODEs](https://en.wikipedia.org/wiki/Characteristic_equation_(calculus)), we have that this second-order ODE has general solutions of the form\n",
    "\n",
    "$$\n",
    "    u(x) = c_1 \\cos(\\beta x) + c_2\\sin (\\beta x), \n",
    "$$\n",
    "\n",
    "where $c_1,c_2\\in\\mathbb{R}$. We now apply the first BC, \n",
    "\n",
    "$$\n",
    "    u(0) = 0 \\Rightarrow c_1 = 0.\n",
    "$$\n",
    "\n",
    "This means that the eigenfunction simplifies to the form\n",
    "\n",
    "$$\n",
    "    u(x) = c_2\\sin(\\beta x) = c_2\\sin(\\sqrt{\\lambda} x).\n",
    "$$\n",
    "\n",
    "Now, the other BC gives,\n",
    "\n",
    "$$\n",
    "    u(1) = 0 \\Rightarrow c_2\\sin(\\beta) = 0 \\Rightarrow c_2\\sin(\\sqrt{\\lambda}) = 0.\n",
    "$$\n",
    "\n",
    "Remember, $\\lambda$ can only be an eigenvalue if there is a nonzero eigenfunction, so we need to have $c_2\\neq 0$, and it does not matter what the value of $c_2$ is since we only ever determine an eigenfunction up to a multiplicative constant anyway. This means that we need\n",
    "\n",
    "$$\n",
    "    \\sin(\\beta)=0 = \\sin(\\sqrt{\\lambda}).\n",
    "$$\n",
    "\n",
    "Lucky for us, there are an infinite number of *positive* values of $\\beta$ (and thus of $\\lambda$) such that this is true, meaning that there are an *infinite number of eigenvalues associated with $L$*. Specifically, if\n",
    "\n",
    "$$\n",
    "    \\beta = k\\pi, \\ k\\in\\mathbb{N},\n",
    "$$\n",
    "\n",
    "meaning $\\beta$ is any positive integer multiple of $\\pi$, then $\\lambda=(k\\pi)^2$ defines an eigenvalue with eigenfunction given by $\\sin(k\\pi x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d7ae6-88dc-4d5a-bafd-26c625dda562",
   "metadata": {},
   "source": [
    "Notice that the eigenvalues are countably infinite. We therefore use an index to enumerate them as shown in the following lemma.\n",
    "\n",
    "---\n",
    "#### Lemma 2.5.4: Eigenvalues and eigenfunctions of $L$\n",
    "\n",
    "The eigenvalues of $L$ are given by \n",
    "\n",
    "$$\n",
    "    \\lambda_k = (k\\pi)^2, \\ k\\in\\mathbb{N}, \n",
    "$$\n",
    "\n",
    "and the associated eigenfunctions are given by\n",
    "\n",
    "$$\n",
    "    u_k(x) = \\sin(k\\pi x), \\ k\\in\\mathbb{N}.\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659b801-05dc-416f-aba0-7fa689c19791",
   "metadata": {},
   "source": [
    "We already know from Lemma 2.5.3 that for $k, m\\in\\mathbb{N}$ with $k\\neq m$, the eigenfunctions $u_k$ and $u_m$ are orthogonal with respect to the continuous inner product. It is a good calculus exercise for students to directly verify via integration that this is the case by utilizing a [product to sum trigonometric identity](https://en.wikipedia.org/wiki/List_of_trigonometric_identities#Product-to-sum_and_sum-to-product_identities) to rewrite the product of $\\sin(k\\pi x)\\sin(m\\pi x)$. \n",
    "\n",
    "In other words, if $k\\neq m$, then we have $\\langle u_k, u_m\\rangle =0$.\n",
    "\n",
    "What is the value of $\\langle u_k, u_k\\rangle$?\n",
    "\n",
    "Using either the same trigonomtric identity (or a double angle formula), students should show that $\\langle u_k, u_k\\rangle = \\frac{1}{2}$ for all $k\\in\\mathbb{N}$. \n",
    "\n",
    "We summarize this below.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "#### Lemma 2.5.5: Inner products of eigenfunctions of $L$\n",
    "\n",
    "For $k\\in\\mathbb{N}$, the eigenfunctions $u_k$ of $L$ given in Lemma 2.4.4 have the property that\n",
    "\n",
    "$$\n",
    "    \\langle u_k, u_m \\rangle = \\begin{cases}\n",
    "                                   0, & k\\neq m, \\\\\n",
    "                                   \\frac{1}{2}, & k=m.\n",
    "                               \\end{cases}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f93af6c-0a67-43c5-be94-de55a91b412c",
   "metadata": {},
   "source": [
    "---\n",
    "#### Student Activity\n",
    "\n",
    "Given real numbers $a<b$, consider the eigenvalue problem \n",
    "\n",
    "$$\n",
    "    -u'' = \\lambda u, \\  x\\in (a,b), \\ u(a)=u(b) = 0. \n",
    "$$\n",
    "\n",
    "Find all eigenvalues and eigenvectors.\n",
    "\n",
    "*Students should attempt this before class. We will discuss the solution as an entire class.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea9652-cf43-4c2d-829b-65ba3474d939",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**Activity 2.4.1 Solution**\n",
    "\n",
    "- It is the *same* differential equation as above but with $(a,b)$ instead of $(0,1)$, so the structure of the fundamental set of solutions is the same as before: $\\{\\cos(\\beta x), \\sin(\\beta x)\\}$.\n",
    "\n",
    "- This implies that eigenfunctions are again of the form $c_1\\cos(\\beta x) + c_2\\sin(\\beta x)$ for some $\\beta>0$ that we need to figure out and some $c_1$ and $c_2$ that we need to choose so that these functions are not identically zero. However, we can make things a bit simpler by phase shifting the trig functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a039ae70-e1d9-48ff-84ac-0ee8fdf7b36c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "- Notice that if $c_1\\cos(\\beta x) + c_2\\sin(\\beta x)$ satisfies the differential equation, then so does $c_1\\cos(\\beta (x-\\phi)) + c_2\\sin(\\beta (x-\\phi))$ for any $\\phi\\in\\mathbb{R}$.\n",
    "\n",
    "- What is a good choice of $\\phi$? How about $\\phi=a$ because this at least means that when we evaluate the eigenfunctions at $x=a$, it is like we are evaluating them at $0$, which is more similar to what we did before.\n",
    "\n",
    "- We therefore look for eigenfunctions of the form\n",
    "\n",
    "$$\n",
    "    c_1\\cos(\\beta (x-a)) + c_2\\sin(\\beta (x-a)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf74aab4-e10c-4341-b10e-a08e5b59bbbc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Following the same steps as before, we will get that $c_1=0$ and that we choose $c_2=1$ with $\\beta_k = \\frac{k\\pi}{b-a}$ for $k\\in\\mathbb{N}$. Checking that for each $k\\in\\mathbb{N}$\n",
    "\n",
    "$$\n",
    "    \\lambda_k = \\left(\\frac{k\\pi}{b-a}\\right)^2\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "    u_k(x) = \\sin\\left(\\frac{k\\pi}{b-a}(x-a)\\right)\n",
    "$$\n",
    "\n",
    "are eigenvalue/eigenfunction pairs for the problem is left for students."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b993d-502b-4841-b3d8-9703fa968f36",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id='Section2.5.3'>Section 2.5.3: The eigenvalues and eigenfunctions of $L_h$</a>\n",
    "---\n",
    "\n",
    "Since we know that $\\sin(\\beta x)$ (for suitable choices of $\\beta$) are eigenfunctions of $L$ and that $L_hu \\approx Lu$, we check via direct substitution to see if there are $\\beta$ that make $\\sin(\\beta x)$ an eigenfunction for $L_h$.\n",
    "\n",
    "**Step 1:** Substitution.\n",
    "\n",
    "$$\n",
    "    L_h \\sin(\\beta x) = \\frac{1}{h^2} \\left[ -\\sin(\\beta (x-h)) + 2\\sin(\\beta x) - \\sin(\\beta(x+h))\\right]\n",
    "$$\n",
    "\n",
    "**Step 2:** Utilize a [product to sum trigonometric identity](https://en.wikipedia.org/wiki/List_of_trigonometric_identities#Product-to-sum_and_sum-to-product_identities) to write\n",
    "\n",
    "$$\n",
    "    - \\left[\\sin(\\beta (x-h)) + \\sin(\\beta(x+h))\\right] = -2\\sin(\\beta x)\\cos(-\\beta h). \n",
    "$$\n",
    "\n",
    "We use the evenness of cosine to rewrite this as\n",
    "\n",
    "$$\n",
    "    - \\left[\\sin(\\beta (x-h)) + \\sin(\\beta(x+h))\\right] = -2\\sin(\\beta x)\\cos(\\beta h).\n",
    "$$\n",
    "\n",
    "**Step 3:** Substituting what we have from Step 2 into what we have from Step 1 and factoring gives\n",
    "\n",
    "$$\n",
    "    L_h \\sin(\\beta x) = \\frac{2}{h^2}\\left[1-\\cos(\\beta h)\\right]\\sin(\\beta x).\n",
    "$$\n",
    "\n",
    "**Step 4:** Use a [half-angle formula](https://en.wikipedia.org/wiki/List_of_trigonometric_identities#Half-angle_formulae) to write\n",
    "\n",
    "$$\n",
    "    1-\\cos(\\beta h) = 2\\sin^2(\\beta h/2).\n",
    "$$\n",
    "\n",
    "**Step 5:** Substitute what we have from Step 4 into what we have from Step 3 to give\n",
    "\n",
    "$$\n",
    "    L_h \\sin(\\beta x) = \\underbrace{\\frac{4}{h^2}\\sin^2(\\beta h/2)}_{\\text{Indep. of $x$}} \\sin(\\beta x)\n",
    "$$\n",
    "\n",
    "Observe that this implies $L_h \\sin(\\beta x)$ produces a constant multiple of $\\sin(\\beta x)$. \n",
    "\n",
    "However, this is not yet sufficient to claim that $\\sin(\\beta x)$ is an eigenfunction associated with eigenvalue $\\frac{4}{h^2}\\sin^2(\\beta h/2)$ because we have not yet specified $\\beta$ or checked to see if this candidate for an eigenfunction for certain $\\beta$ values is actually in $D_{h,0}-\\{0\\}$ as required in the specification of the problem. \n",
    "\n",
    "**Step 6:** Recall that to solve $L_h v = \\lambda v$, we still need $v\\in D_{h,0}-\\{0\\}$. \n",
    "\n",
    "We therefore seek values of $\\beta$ that produce an eigenvalue of the form $\\frac{4}{h^2}\\sin^2(\\beta h/2)$ with (nonzero) eigenfunction of the form $\\sin(\\beta x)\\in D_{h,0}$. \n",
    "\n",
    "As in the continuous case, the boundary conditions are only going to be satisfied if $\\beta = k\\pi$ for some integer $k$. Does this mean any integer produces an eigenvalue of the form $\\frac{4}{h^2}\\sin^2(k\\pi h/2)$? \n",
    "\n",
    "Zero is an integer. However, $\\sin (0 x)=0$ for all $x$, so the candidate for the eigenfunction is actually the zero function. So much for zero as a potential eigenvalue.\n",
    "\n",
    "Recall that $D_{h,0}$ is defined in terms of discrete function values at $n$ interior grid points. What happens if $k=1, 2, \\ldots, n$? In this case, $kx_1 = kh = k/(n+1)<1$ and $\\sin(k\\pi x_1) = \\sin(k\\pi h) = \\sin(k\\pi/(n+1))\\neq 0$. In this case, the function $\\sin(k\\pi x)$ is an eigenfunction for $L_h$ with $\\frac{4}{h^2}\\sin^2(k\\pi h/2)$ as its eigenvalue.\n",
    "\n",
    "What happens if $k>n$? \n",
    "\n",
    "If $k=n+1$, then $k\\pi x_j=\\pi j$ and $\\sin(k\\pi x_j)=0$ for all $j$. \n",
    "\n",
    "If $k=n+1+1=n+2$, then $k\\pi x_j = (n+1)\\pi x_j + \\pi x_j$. In this case, $\\sin(k\\pi x_j)=\\sin((n+1)\\pi x_j+\\pi x_j)$. By applying a product-to-sum formula, we can show that this is equal to $-\\sin(n\\pi x_j)$. This means the eigenfunction $\\sin(k\\pi x)$ is just a scalar multiple of the eigenfunction $\\sin(n\\pi x)$. However, we know that eigenfunctions associated with distinct eigenvalues should be orthogonal to each other not linearly dependent upon each other. This means that $\\beta=(n+2)\\pi$ is also not an option.\n",
    "\n",
    "In fact, we can show that for integers $k\\geq n+1$, we run into similar issues.\n",
    "\n",
    "Specifically, for $k=1,2,\\ldots, n$, we have that $\\sin((n+1-k)\\pi x_j) = -\\sin((n+1+k)\\pi x_j)$. This implies that there are no new eigenvalues/eigenfunctions for $n+1\\leq k\\leq 2n+1$.\n",
    "\n",
    "The cycle repeats itself as we continue to increase $k$.\n",
    "\n",
    "We illustrate this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abca91cc-27b1-4ce4-8928-d7191812a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b9f97d-d1cc-4555-9be7-429ad62d15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sines(k, n):\n",
    "    plt.figure(0)\n",
    "    plt.clf()\n",
    "    x = np.linspace(0, 1, n+2)\n",
    "    plt.plot(x, np.sin((n+1-k)*np.pi*x), 'b--', lw=4)  # plot n+1-k case\n",
    "    plt.plot(x, -np.sin((n+1+k)*np.pi*x), 'r:', lw=2)  # plot negative n+1+k case, which matches n+1-k case\n",
    "    plt.plot(x, -np.sin((2*(n+1)-(n+1-k))*np.pi*x), 'k-.')  # plot negative 2*(n+1)+k case, which matches n+1-k case\n",
    "    plt.show()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4924b-336b-44ee-b316-f05220bc07dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "interact_manual(plot_sines, \n",
    "         k = widgets.IntText(value=95),  # The k's are \"counting backwards\" \n",
    "         n = widgets.IntText(value=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f4d707-cbf4-4f2f-9a95-88456df25455",
   "metadata": {},
   "source": [
    "<mark>This means that there are only $n$ eigenvalues and eigenfunctions for this problem.</mark>\n",
    "\n",
    "Why is this so important? \n",
    "\n",
    "Recalling how $L_h$ is defined in [Section 2.3](Chp2Sec3.ipynb), we have that \n",
    "\n",
    "$$\n",
    "    L_hv=\\lambda v \\ \\Longleftrightarrow \\ \\frac{1}{h^2}Av = \\lambda v\n",
    "$$\n",
    "\n",
    "where $A\\in\\mathbb{R}^{n\\times n}$ is given by\n",
    "\n",
    "$$\n",
    "    A = \\begin{pmatrix}\n",
    "                    2 & -1 & 0 & \\cdots & 0 \\\\\n",
    "                    -1 & 2 & -1 & \\ddots & \\vdots \\\\\n",
    "                    0 & \\ddots & \\ddots & \\ddots & 0 \\\\\n",
    "                    \\vdots & \\ddots & -1 & 2 & -1 \\\\\n",
    "                    0 & \\cdots & 0 & -1 & 2\n",
    "                \\end{pmatrix},\n",
    "$$\n",
    "\n",
    "and when we write $L_hv$, we mean $v\\in D_{h,0}$ whereas when we write $Av$ we mean $v\\in\\mathbb{R}^n$ is the vector whose $j$th component is defined by the evaluation of the discrete function at the interior grid points $x_j$ for $1\\leq j\\leq n$.\n",
    "\n",
    "The reason we multiply $\\frac{1}{h^2}$ to $A$ is to make the left-hand side of the matrix-vector eigenvalue problem equivalent to $L_hv$ since $L_h$ involves the $\\frac{1}{h^2}$ term in its definition.\n",
    "\n",
    "The point is this: if the discrete eigenvalue is equivalent to the matrix-vector eigenvalue problem, then there is simply no way that we could ever have more than $n$ eigenvalues and eigenfunctions since that is the maximum number we can get from a matrix of dimension $n\\times n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff033ce-9d47-4993-acd8-ec152d115ea5",
   "metadata": {},
   "source": [
    "We summarize where we are to this point in the lemma below.\n",
    "\n",
    "---\n",
    "#### Lemma 2.5.6: Eigenvalues and eigenfunctions of $L_h$\n",
    "\n",
    "The eigenvalues of $L_h$ are given by\n",
    "\n",
    "$$\n",
    "    \\lambda_k = \\frac{4}{h^2}\\sin^2(k\\pi h/2), \\ k=1, 2, \\ldots, n, \n",
    "$$\n",
    "\n",
    "and the associated eigenfunctions are given by\n",
    "\n",
    "$$\n",
    "    v_k(x) = \\sin(k\\pi x), \\ k=1, 2, \\ldots, n.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Remarks:**\n",
    "\n",
    "- Observe that $\\lambda_k$ is a function of $h$, so we may write $\\lambda_k(h)$ to either make this dependence explicit or to distinguish it from the continuous eigenvalues that we also denote by $\\lambda_k$ but are obviously not functions of the discretization parameter $h$.\n",
    "\n",
    "  - Note that even though $\\lambda_k$ is a function of $h$, $v_k$ is *not*.\n",
    "\n",
    "\n",
    "- For $\\theta$ near zero, we know that $\\sin(\\theta)\\approx \\theta$, which means that for $k=1$ and \"small $h$\" that $\\lambda_1$ is approximately given by\n",
    "<br><br>\n",
    "$$\n",
    "    \\lambda_1(h) \\approx \\frac{4}{h^2}(\\pi h/2)^2 = \\pi^2\n",
    "$$\n",
    "<br>\n",
    "   In other words, the smallest eigenvalue for $L_h$ is close to the smallest eigenvalue for $L$. \n",
    "<br><br> \n",
    "\n",
    "- It is possible to prove that $\\lim_{h\\downarrow 0} \\lambda_1(h)$ (where we make the functional dependence of $\\lambda_1$ on $h$ explicit) exists and is equal to $\\pi^2$. One way to prove this is to manipulate the formula for $\\lambda_1(h)$ and apply L'Hospital's rule a couple of times. *Students should verify this.*\n",
    "\n",
    " \n",
    "- Clearly $\\sin^2(k\\pi h/2) = \\sin^2\\left(\\frac{k\\pi}{2(n+1)}\\right)$, so as $k$ increases from $1$ to $n$, the eigenvalues increase, monotonically, but are bounded above by $4/h^2$ (the factor in front of this squared sine function). In other words,\n",
    "<br><br>\n",
    "$$\n",
    "    0 < (\\lambda_1\\approx \\pi^2) < \\lambda_2 < \\cdots <\\lambda_n < \\frac{4}{h^2}. \n",
    "$$\n",
    "\n",
    "- Are $\\lambda_2$, $\\lambda_3$, etc. close approximations to $(2\\pi)^2$, $(3\\pi)^2$, etc.? \n",
    "\n",
    "  Consider what it means for $k\\pi h/2$ to be \"close to zero\". This quantity depends on both $h$ being small (which depends on $n$ being large) and $k$ being not too large relative to the smallness of $h$. If this is true, then just like we showed for $\\lambda_1$ above, $\\lambda_k\\approx (k\\pi)^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019c905-aa90-4f74-aeef-2b18de8e1492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigenvalues(h, k):\n",
    "    cont_e_val = (k*np.pi)**2  # the continuous e.val\n",
    "    print('='*50)\n",
    "    print('kth e.val for L is     ', cont_e_val)\n",
    "    print()\n",
    "    disc_e_val = 4/h**2 * np.sin(k*np.pi*h/2)**2  # the discrete e.val\n",
    "    print('kth e.val for L_h is   ', disc_e_val)\n",
    "    print()\n",
    "    error =  cont_e_val - disc_e_val\n",
    "    print('approx error is        ', error)\n",
    "    print()\n",
    "    rel_error = error / cont_e_val*100 \n",
    "    print('rel. approx. error is   {:3.2f}%'.format(rel_error) )\n",
    "    print('='*50)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8516dad-7600-4efa-8549-4a046150c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(eigenvalues, \n",
    "             h = widgets.FloatLogSlider(value=0.1, base=10, min=-7, max=-1, step=1),\n",
    "             k = widgets.IntText(value=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23f852-d14d-4091-916c-bb8612b10ab8",
   "metadata": {},
   "source": [
    "**Bounds on first discrete eigenvalue**\n",
    "\n",
    "Here, we show that for $h\\in(0,1]$, $\\lambda_1(h)\\in[4, \\pi^2]$ using basic calculus.\n",
    "\n",
    "First, we differentiate $\\lambda_1(h)$ with respect to $h$ to get \n",
    "\n",
    "$$\n",
    "    \\frac{d}{dh} \\lambda_1(h) = -\\frac{8}{h^3}\\sin^2(\\pi h /2) + \\frac{4\\pi}{h^2}\\sin(\\pi h/2)\\cos(\\pi h/2).\n",
    "$$\n",
    "\n",
    "Since this looks like a somewhat complicated function, we analyze its plot for $h\\in(0,1]$ below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba3603-4fd0-4e84-a984-7a235f7531b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.linspace(0.001,1,100)\n",
    "%matplotlib widget\n",
    "plt.figure(0, figsize=(3,3))\n",
    "plt.plot(h,-8/h**3*np.sin(np.pi*h/2)**2+4*np.pi/h**2*np.sin(np.pi*h/2)*np.cos(np.pi*h/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deb32eb-949d-4797-8212-1887de1ef79b",
   "metadata": {},
   "source": [
    "From the plot above, we see that \n",
    "$$\n",
    "    \\frac{d}{dh}\\lambda_1(h)\\leq 0\n",
    "$$\n",
    "for all $0<h\\leq 1$. *Note, we could also argue this based on knowledge of the functions, but the plot does all the hard work for us.*\n",
    "\n",
    "Recall from one of the above remarks that $\\lim_{h\\downarrow 0} \\lambda_1(h)=\\pi^2$. The negativity of the derivative implies that this is an *upper* bound for $\\lambda_1(h)$ for $0<h\\leq 1$.\n",
    "\n",
    "Also, the negativity of the derivative implies that the minimum value of $\\lambda_1(h)$ occurs at $h=1$, and $\\lambda_1(1)=4$.\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\n",
    "   \\large \\boxed{ 4\\leq \\lambda_1(h)\\leq \\pi^2 \\ \\text{ for } \\ 0<h\\leq 1.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447370b3-938a-4887-90f4-0bb32d462d71",
   "metadata": {},
   "source": [
    "**What about inner products between discrete eigenfunctions?**\n",
    "\n",
    "As before, we know that $\\langle v_k, v_m \\rangle_h = 0$ for integers $k\\neq m$.\n",
    "\n",
    "What about when $k=m$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335c46b8-8c4f-4536-9fc1-895e61b17f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_h(u, v, h):\n",
    "    z = h * (u[0]*v[0] + u[-1]*v[-1])/2.0 + h*np.dot(u[1:-1],v[1:-1])\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85eb5d7-bd05-4425-80ec-efa4bf2b63c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = lambda x, k: np.sin(k*np.pi*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c277dc-7538-4ac4-842f-28ef44643496",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 19\n",
    "x = np.linspace(0, 1, n+2)\n",
    "h = x[1]-x[0]\n",
    "\n",
    "k = 3\n",
    "inner_h(v(x, k), v(x, k), h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d22f1ec-6953-4aed-8ff7-14d43c2c937e",
   "metadata": {},
   "source": [
    "It appears that $\\langle v_k, v_k\\rangle_h = 1/2$ just as in the continuous case. Can we prove it?\n",
    "\n",
    "First, we write\n",
    "$$\n",
    "    \\langle v_k, v_k\\rangle_h = h\\left[ \\frac{v_k^2(x_0)+v_k^2(x_{n+1})}{2} + \\sum_{j=1}^n v_k^2(x_j)\\right]\n",
    "$$\n",
    "\n",
    "The first term on the right hand side is equal to zero since $v_k\\in D_{h,0}$.\n",
    "\n",
    "*We claim that*\n",
    "\n",
    "$$\n",
    "     \\sum_{j=1}^n v_k^2(x_j) = \\sum_{j=1}^n \\sin^2(k\\pi x_j) = \\frac{1}{2h}. \n",
    "$$\n",
    "\n",
    "This is a bit tricky to see, so we provide a full proof of this claim below.\n",
    "\n",
    "---\n",
    "***Proof of claim:***\n",
    "\n",
    "We recall that \n",
    "\n",
    "$$\n",
    "    \\sin(x) = \\frac{e^{ix}-e^{-ix}}{2i}. \n",
    "$$\n",
    "\n",
    "This implies that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\sum_{j=1}^n \\sin^2(k\\pi x_j) &= \\sum_{j=1}^n \\left( \\frac{e^{ik\\pi x_j}-e^{-ik\\pi x_j}}{2i } \\right)^2 \\\\\n",
    "                                  \\\\\n",
    "                                  &= -\\frac{1}{4} \\sum_{j=1}^n \\left(e^{i2k\\pi x_j} - \\underbrace{2e^0}_{=2} + e^{-i2k\\pi x_j} \\right) \\\\\n",
    "                                  \\\\\n",
    "                                  &= \\frac{n}{2} - \\frac{1}{4} \\sum_{j=1}^n \\left(e^{i2k\\pi x_j} + e^{-i2k\\pi x_j} \\right).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Use the fact that $x_j=jh$ so that $kx_j = jx_k$ to rewrite\n",
    "\n",
    "$$\n",
    "    \\sum_{j=1}^n \\left(e^{i2k\\pi x_j} + e^{-i2k\\pi x_j} \\right) = \\sum_{j=1}^n \\left(e^{i2\\pi x_k}\\right)^j + \\sum_{j=1}^n \\left(e^{-i2\\pi x_k}\\right)^j.\n",
    "$$\n",
    "\n",
    "Recall that the partial sum of a geometric series $S_n:= \\sum_{j=1}^n r^j$ can be written as $S_n = \\frac{r-r^{n+1}}{1-r}$. Use this to rewrite the above sums as\n",
    "\n",
    "$$\n",
    "    \\sum_{j=1}^n \\left(e^{i2\\pi x_k}\\right)^j = \\large \\frac{e^{i2\\pi x_k} - e^{i2\\pi x_k(n+1)}}{1-e^{i2\\pi x_k}}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "    \\sum_{j=1}^n \\left(e^{-i2\\pi x_k}\\right)^j = \\large \\frac{e^{-i2\\pi x_k} - e^{-i2\\pi x_k(n+1)}}{1-e^{-i2\\pi x_k}}.\n",
    "$$\n",
    "\n",
    "Recall that $h=1/(n+1)$ to get that $2\\pi x_k(n+1) = 2\\pi kh(n+1) = 2\\pi k$ so that by Euler's formula we have $e^{i2\\pi x_k(n+1)} = e^{-i2\\pi x_k(n+1)} = 1$, which implies that each of the above sums are just fancy ways of writing $-1$. In other words,\n",
    "\n",
    "$$\n",
    "    \\sum_{j=1}^n \\sin^2(k\\pi x_j) = \\frac{n}{2} - \\frac{1}{4} (-1 + -1) = \\frac{n}{2} + \\frac{1}{2} = \\frac{n+1}{2} = \\frac{1}{2h}. \\ \\Box\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d411189e-3b61-409b-9d36-d13e14f3d6d0",
   "metadata": {},
   "source": [
    "We summarize this as the following lemma.\n",
    "\n",
    "---\n",
    "#### Lemma 2.5.7: Inner products of eigenfunctions of $L_h$\n",
    "\n",
    "For $1\\leq k\\leq n$, the eigenfunctions $v_k$ of $L_h$ given in Lemma 2.4.6 have the property that\n",
    "\n",
    "$$\n",
    "    \\langle v_k, v_m \\rangle_h = \\begin{cases}\n",
    "                                   0, & k\\neq m, \\\\\n",
    "                                   \\frac{1}{2}, & k=m.\n",
    "                               \\end{cases}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb97299-410a-4767-a625-d8745432eb4c",
   "metadata": {},
   "source": [
    "Before we look at plots of the numerically estimated eigenfunctions and their errors due to numerical approximation of eigenvectors from the matrix $\\frac{1}{h^2}A$, *we need to understand a few key points.*\n",
    "\n",
    "<span style=\"background-color:rgba(255, 0, 255, 0.5)\">***Key points:***</span>\n",
    "\n",
    "- Using a linear algebra package to estimate the eigenvalues/eigenfunctions of a matrix will typically produce results that are normalized with respect to the Euclidean norm (i.e., the 2-norm).\n",
    "\n",
    "- This implies that the $v_k$ computed from a linear algebra package will likely have the property that $\\langle v_k, v_k \\rangle = 1$.\n",
    "\n",
    "- The sign of an eigenvector is completely arbitrary, and we could have just as easily stated that the discrete eigenfunctions were computed using $-\\sin$ in place of $\\sin$ in the lemma above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00225e-be3e-4f4c-9ddc-8d7d38d20c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_A(n):\n",
    "    A = np.zeros((n,n))\n",
    "    np.fill_diagonal(A,2)\n",
    "    A += np.diag(-np.ones(n-1),k=1)\n",
    "    A += np.diag(-np.ones(n-1),k=-1)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464d6138-0c54-4e17-971f-f599d8566a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "x = np.linspace(0,1,n+2)\n",
    "h = x[1]-x[0]\n",
    "\n",
    "A = make_A(n)\n",
    "\n",
    "lambda_approx, v = np.linalg.eigh(1/h**2 * A)  # eigh returns eigs of symmetric matrix in increasing order\n",
    "\n",
    "lambda_exact = np.zeros(n)\n",
    "for k in range(0,n):\n",
    "    lambda_exact[k] = 4/(h**2) * (np.sin((k+1)*np.pi*h/2))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d65599-c6eb-48ee-aa7d-6e8096f8757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lambda_approx)\n",
    "print('-'*50) \n",
    "print(np.max(np.abs(lambda_approx-lambda_exact)))  # Largest error made in estimating an eigenvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d20e6-3f26-422b-887a-069727d1eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_exact = np.zeros((n,n))\n",
    "for k in range(0,n):\n",
    "    v_exact[:,k] = np.sin((k+1)*np.pi*x[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d14abac-03bb-4df5-8c60-ba033be21a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the eig functions in numpy will normalize eigenvectors \n",
    "# with respect to the usual 2-norm and the sign is arbitrary\n",
    "\n",
    "# Choose k between 1 and 30\n",
    "k = 3\n",
    "\n",
    "print('Component #' + str(k) + ' should be 1 and the others\\n' +\\\n",
    "      'should be approximately zero for estimated eigenvectors')\n",
    "print('-'*50)\n",
    "print(v.T.dot(v)[k-1, :])\n",
    "\n",
    "print('\\n\\n')\n",
    "print('-'*50)\n",
    "print('Component #' + str(k) + ' should be ' + str(1/(2*h)) + ' and the others\\n' +\\\n",
    "      'should be approximately zero for eigenvectors from Lemma')\n",
    "print(v_exact.T.dot(v_exact)[k-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a77a31-16da-4904-a325-acb44b35fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# Pick k between 1 and 30\n",
    "k = 3\n",
    "\n",
    "ind = np.argmax(np.abs(v[:,k-1]))\n",
    "\n",
    "plt.figure(4)\n",
    "plt.plot(x[1:-1],v[:,k-1]/v[ind,k-1])\n",
    "plt.title('$L^\\infty$ normalized estimated numerical e.func #' + str(k))\n",
    "\n",
    "plt.figure(5)\n",
    "plt.plot(x[1:-1],v[:,k-1]/v[ind,k-1]-np.sin(k*np.pi*x[1:-1]))\n",
    "plt.title('Difference from exact to estimated numerical e.func #' + str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d32216-254a-496f-b619-6c55130171e6",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id='Section2.5.4'>Section 2.5.4: Spectral approximation of functions</a>\n",
    "---\n",
    "\n",
    "As previously mentioned in [Section 2.3](#Chp2Sec3.ipynb), $D_{h,0}$ is an $n$-dimensional vector space, which means that any set of $n$ linearly independent vectors in $D_{h,0}$ forms a basis for $D_{h,0}$. Since there are $n$ eigenfunctions of $L_h$ that are linearly independent (they are pairwise orthogonal to each other which implies linear independence), this implies we can express any $g\\in D_{h,0}$ as a linear combination of these eigenfunctions. \n",
    "\n",
    "Using the notation above where $\\{v_k\\}_{k=1}^n$ are the eigenfunctions of $L_h$, we thave that for any $g\\in D_{h,0}$ there exists *unique* $\\{c_k\\}_{k=1}^n$ such that \n",
    "\n",
    "$$\n",
    "    g(x) = \\sum_{i=1}^n c_k v_k(x).\n",
    "$$\n",
    "\n",
    "**Question:** What are the $\\{c_k\\}_{k=1}^n$?\n",
    "\n",
    "We use Lemma 2.5.7 to answer this question. Specifically, we compute the discrete inner product of both sides of the above equation with respect to a fixed $v_m$ (and use the bilinearity of inner products) to give\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\langle g, v_m\\rangle_h &= \\underbrace{\\large \\left\\langle\\sum_{k=1}^n c_kv_k, v_m\\right\\rangle_h}_{\\text{Bilinearity gives next equality}} \\\\\n",
    "                            \\\\\n",
    "                            &= {\\large \\sum_{k=1}^n c_k}\\underbrace{\\large \\langle v_k, v_m\\rangle_h}_{=0 \\text{ if } k\\neq m, \\text{ else } 1/2} \\\\\n",
    "                            \\\\\n",
    "                            &=\\large \\frac{c_m}{2}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This implies\n",
    "\n",
    "$$\n",
    "    \\large c_m = 2\\langle g, v_m\\rangle_h.\n",
    "$$\n",
    "\n",
    "Hence, if $g\\in D_{h,0}$, then\n",
    "\n",
    "$$\n",
    "    \\large \\boxed{ g(x) = \\sum_{k=1}^n 2\\langle g, v_k\\rangle_h  v_k(x). }\n",
    "$$\n",
    "\n",
    "The $v_k=\\sin(k\\pi x)$ for each $1\\leq k\\leq n$, and the above defines a finite Fourier series. We return to this in chapter 3 and explore how this is related to the infinite Fourier series expansion of continuous functions.\n",
    "\n",
    "In some places, you will find this referred to more generally as a spectral representation, expansion, or approximation (depending on the context or if sums, finite or infinite, are truncated) of a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48527569-e398-4336-9291-3c0518b7bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function assuming an array of discrete v are given from either \n",
    "# an eigenanalysis of a matrix or using the exact eigenfunctions.\n",
    "\n",
    "def g_spectral(g, v, n, trunc, norm_const=1):\n",
    "\n",
    "    g_sum = np.zeros(n+2)\n",
    "    \n",
    "    h = 1/(n+1)\n",
    "    \n",
    "    for k in range(trunc):\n",
    "        g_sum[1:-1] += inner_h(g[1:-1],v[:,k], h)*v[:,k] / norm_const\n",
    "    \n",
    "    return g_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1187af3-b4c5-42ea-8f12-80fe29fd2349",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = x*(1-x)*np.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f6a111-01a9-4d6d-9d56-5a51694fc0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plt.figure(6)\n",
    "\n",
    "plt.plot(x, g)\n",
    "\n",
    "# Below, we use the numerically approximated discrete eigenfunctions v\n",
    "\n",
    "norm_const = inner_h(v[:,0], v[:,0], 1/(n+1))\n",
    "plt.plot(x, g_spectral(g, v, n, trunc=2, norm_const=norm_const))\n",
    "plt.title('Normalizing constant used = ' + str(norm_const))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d982f6-d337-4929-b7c5-e31b14454f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.figure(7)\n",
    "plt.plot(x, g)\n",
    "\n",
    "# Below, we use the exact discrete eigenfunctions\n",
    "\n",
    "norm_const = 0.5\n",
    "plt.plot(x, g_spectral(g, v_exact, n, trunc=3, norm_const = norm_const))\n",
    "plt.title('Normalizing constant used = ' + str(norm_const))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce13461-8ca8-4e54-bbcc-64d8edf86064",
   "metadata": {},
   "source": [
    "---\n",
    "## Navigation:\n",
    "\n",
    "- [Previous](Chp2Sec4.ipynb)\n",
    "\n",
    "- [Next](Chp2Sec6.ipynb)\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
