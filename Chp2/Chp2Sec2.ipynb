{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2880a34c-d2fc-45c5-b5b7-c00de0ddb37a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to Partial Differential Equations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb03bb22-d502-4417-b9cd-cac8671ba0f2",
   "metadata": {},
   "source": [
    "## Chapter 2: Elliptic PDEs, Poissonâ€™s Equation, and a Two-Point Boundary Value Problem \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b37f86-d67f-4707-8b94-3807d6234671",
   "metadata": {},
   "source": [
    "## Want to use Colab? [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations/blob/main/Chp2/Chp2Sec2.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a099d20a-9ac2-4349-9ce2-75845a609321",
   "metadata": {},
   "source": [
    "## Prepping the environment for interactive plots in Colab\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592c246-ec8a-4038-8feb-e6de2ed7731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab - installing missing packages')\n",
    "    !pip install ipympl\n",
    "    from IPython.display import clear_output\n",
    "    clear_output()\n",
    "    exit()\n",
    "else:\n",
    "    print('Not running on CoLab - assuming environment has necessary packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18140c49-b78e-4c53-8d2d-1702ee696af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6273fef7-f40f-47df-b90e-92a3197343b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creative Commons License Information\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/80x15.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">Introduction to Partial Differential Equations: Theory and Computations</span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">Troy Butler</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>.<br />Based on a work at <a xmlns:dct=\"http://purl.org/dc/terms/\" href=\"https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations\" rel=\"dct:source\">https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b36aa-f74e-4d6a-8a3d-402bfc8f56e5",
   "metadata": {},
   "source": [
    "## Section 2.2:  A Finite Difference Approximation in 1-D\n",
    "---\n",
    "\n",
    "In this section, we will use Taylor series to derive a centered finite difference scheme leading to a finite difference approximation of the 2-pt BVP  $-u''(x)=f(x)$ for $x\\in(0,1)$ with $u(0)=u(1)=0$. Specifically, we will derive an $A\\in\\mathbb{R}^{n\\times n}$ of the form\n",
    "\n",
    "$$\n",
    "\\large A = \\begin{pmatrix}\n",
    "                    2 & -1 & 0 & \\cdots & 0 \\\\\n",
    "                    -1 & 2 & -1 & \\ddots & \\vdots \\\\\n",
    "                    0 & \\ddots & \\ddots & \\ddots & 0 \\\\\n",
    "                    \\vdots & \\ddots & -1 & 2 & -1 \\\\\n",
    "                    0 & \\cdots & 0 & -1 & 2\n",
    "           \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and a data vector $b$ of the form\n",
    "\n",
    "$$\n",
    "\\large b = h^2\\begin{pmatrix}\n",
    "                        f(x_1) \\\\\n",
    "                        f(x_2) \\\\\n",
    "                        \\vdots \\\\\n",
    "                        f(x_n)\n",
    "               \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "that gives us the discrete problem $Av=b$ where $v\\in\\mathbb{R}^n$ has the property that $v_j\\approx u(x_j)$.\n",
    "\n",
    "- The $h^2$ comes from discretizing the differential operator $L=\\frac{d^2}{dx^2}$ with the centered finite difference scheme. \n",
    "\n",
    "  - It is perhaps slightly strange that we put this on the data vector instead of on $A$.\n",
    "\n",
    "  - In [Section 2.3](Chp2Sec3.ipynb), we see that $\\frac{1}{h^2}A$ defines the difference operator $L_h$ that is the discrete counterpart to $L$.\n",
    "  \n",
    "  - So, why don't we \"attach\" this $h^2$ on $A$ in the form of a multiplicative factor of $1/h^2$? \n",
    "  \n",
    "    - Well, remember that we need to *solve* $Av=b$ for $v$. So, even if we multiplied $A$ by $1/h^2$, we then need to \"undo\" this when solving for $v$. This saves us computational effort.\n",
    "\n",
    "\n",
    "- How do we *solve* $Av=b$ in practice? Gaussian elimination is a method that should be familiar from undergraduate linear algebra.\n",
    "\n",
    "  - If not, it is straightforward if not a bit messy (you may want to review this here https://en.wikipedia.org/wiki/Gaussian_elimination). \n",
    "  \n",
    "  - The idea is to reduce the system of linear equations into row-echelon form (https://en.wikipedia.org/wiki/Row_echelon_form) and use back-substitution to then determine the vector $v$ such that $Av=b$.\n",
    "  \n",
    "\n",
    "**Key question:** Since formally we can write $v=A^{-1}b$ (assuming $A^{-1}$ exists), why we do not simply just construct the inverse of the matrix?\n",
    "\n",
    "**The answer:** Because that is ***expensive*** in terms of [FLOPS](https://en.wikipedia.org/wiki/FLOPS) and thanks to Gaussian elimination, completely unnecessary in determining $v$ with less FLOPS.\n",
    "\n",
    "Think of FLOPS as a computational currency that you must pay in order to solve a problem. If we can solve the same problem with less FLOPS in method A vs. method B, then we say method A is *computationally cheaper* than method B.\n",
    "\n",
    "**The algorithm:** While you could code up your own version of Gaussian elimination, there are many available computational libraries that will do this for you efficiently. \n",
    "We show how using [`numpy.linalg.solve`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html) below on an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c9be5b-a27d-4e56-84dd-3ff40cda4520",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id='Section2.2.1'>Section 2.2.1: A centered finite difference approximation to second-order derivatives</a>\n",
    "---\n",
    "\n",
    "Students may find reviewing [Section 1.2.3 in the Section 1.2 notebook](https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations/blob/main/Chp1/Chp1Sec2.ipynb) useful before proceeding. \n",
    "\n",
    "Here, assume that for a given $x\\in\\mathbb{R}$ that $g\\in\\mathcal{C}^4([x-h_\\max, x+h_\\max])$, then for any $0<h\\leq h_\\max$, Taylor's theorem gives\n",
    "\n",
    "$$\n",
    "    g(x+h) = g(x) + g'(x)h + \\frac{h^2}{2}g''(x) + \\frac{h^3}{6}g^{(3)}(x) + \\frac{h^4}{24}g^{(4)}(\\xi_1), \n",
    "$$\n",
    "\n",
    "where $\\xi_1\\in[x,x+h]$. Similarly, for the same $h>0$, we have\n",
    "\n",
    "$$\n",
    "    g(x-h) = g(x) - g'(x)h + \\frac{h^2}{2}g''(x) - \\frac{h^3}{6}g^{(3)}(x) + \\frac{h^4}{24}g^{(4)}(\\xi_2), \n",
    "$$\n",
    "\n",
    "where $\\xi_2\\in[x-h, x]$. If we *add* these two expressions together, the terms associated with $h$ and $h^3$ cancel out due to the opposing signs, and we get\n",
    "\n",
    "$$\n",
    "    g(x+h) + g(x-h) = 2g(x) + h^2 g''(x) + \\frac{h^4}{24}\\left[g^{(4)}(\\xi_1) + g^{(4)}(\\xi_2) \\right].\n",
    "$$\n",
    "\n",
    "Now, we solve for $g''(x)$ but subtracting $2g(x)$ and dividing through by $h^2$ to get\n",
    "\n",
    "$$\n",
    "    g''(x) + E_h(x) = \\frac{g(x+h)-2g(x) + g(x-h)}{h^2}, \n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    E_h(x) := \\frac{h^2}{24}\\left[g^{(4)}(\\xi_1) + g^{(4)}(\\xi_2) \\right].\n",
    "$$\n",
    "\n",
    "**Remarks:**\n",
    "\n",
    "- $E_h(x)$ is at least $\\mathcal{O}(h^2)$ because we assumed $g\\in\\mathcal{C}^4(\\mathbb{R})$, which means that for any finite $h>0$, $|g^{(4)}|$ is a continuous function on $[x-h, x+h]$, so it must obtain a maximum value, i.e., \n",
    "<br><br>\n",
    "$$\n",
    "    |E_h(x)| \\leq \\frac{M_g h^2}{12}, \n",
    "$$\n",
    "<br>\n",
    "where \n",
    "<br><br>\n",
    "$$\n",
    "    M_g := \\sup_{\\xi \\in [x-h_\\max,x+h_\\max]} | g^{(4)} |\n",
    "$$\n",
    "<br><br>\n",
    "and we can actually replace the $\\sup$ with $\\max$.\n",
    "\n",
    "\n",
    "- Given this information about $E_h(x)$, the following approximation is \"reasonable\" for any $g$ that is four-times continuously differentiable on an interval as long as $h$ is sufficiently small:\n",
    "\n",
    "$$\n",
    "    g''(x) \\approx \\frac{g(x+h)-2g(x) + g(x-h)}{h^2}.\n",
    "$$\n",
    "\n",
    "\n",
    "- If $g$ is a function such that $g^{(4)}\\equiv 0$, then $E_h(x)=0$ and the above approximation becomes an equality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bcbe28-e554-4571-9a82-3f2dbd670688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c11a90-935f-422e-81a8-68047e52b056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_approx(g, x, h=0.1):\n",
    "    g_pp = (g(x+h) - 2*g(x) + g(x-h)) / h**2\n",
    "    return g_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83449e1b-c786-4b9f-ad2e-c64084a89669",
   "metadata": {},
   "source": [
    "Let $g(x) = e^{-x^2}\\sin(\\pi x)$, which produces a somewhat interesting plot. Let's play with this below to study the difference approximation and the practical limitations on making $h$ too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632611c-6fca-49ec-b9c8-f7c50dfd54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c9cde-23fb-422d-9880-7dc349769a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sp.symbols('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13fd82c-8261-429c-a6aa-af8ec8522c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sp.exp(-x**2)*sp.sin(sp.pi * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73aafef-452b-4510-9bae-de93c36d3e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920140d5-2f66-4e12-87bf-6cf31504e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pp = g.diff(x, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ccc4df-660e-43e8-a2d5-4bae287b226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb09959-fda2-4baa-b37d-c19229aa1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy.utilities.lambdify import lambdify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cda8fa-0ccd-4740-8533-ed536ff96e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_eval = lambdify(x, g)\n",
    "\n",
    "g_pp_eval = lambdify(x, g_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc0999-8673-4686-bd4a-5297529439fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gpp(g, g_pp, n, h=0.1):\n",
    "    \n",
    "    plt.figure(0)\n",
    "    plt.clf()\n",
    "    \n",
    "    x = np.linspace(-3, 3, n)\n",
    "    \n",
    "    plt.plot(x, g_pp(x), ls=':', c='b', label=\"exact $g''$\")\n",
    "    plt.plot(x, pp_approx(g, x, h), ls='-.', c='r', label=r\"$\\approx$ $g''$\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c4e0b3-8b87-4828-bc28-00c129b8b4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "%reset -f out \n",
    "\n",
    "# Check out what happens when $h<1e-7$\n",
    "\n",
    "interact_manual(plot_gpp, \n",
    "            g = fixed(g_eval),\n",
    "            g_pp = fixed(g_pp_eval),\n",
    "            h=widgets.FloatLogSlider(value=0.01, min=-9, max=-1, base=10, step=1),\n",
    "            n=widgets.IntSlider(value=100, min=50, max=1000, step=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b75fb9-b6e8-4d87-a5f2-547970144b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pppp = g.diff(x, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682fa287-c5c6-4b8c-a78c-fe9e81aa20a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pppp_eval = lambdify(x, g_pppp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97506a04-2d64-4822-b92a-f7c606e3d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, 1000)\n",
    "g_pppp_max = np.abs(np.max(g_pppp_eval(x)))\n",
    "print(g_pppp_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d875c088-1a3f-4699-8b07-c91e643635c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gpp_error_and_bounds(g, g_pp, g_pppp_max, n, h=0.1):\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    \n",
    "    x = np.linspace(-3, 3, n)\n",
    "    \n",
    "    plt.plot(x, np.abs( g_pp(x) - pp_approx(g, x, h) ), ls=':', c='b', label=\"Error\")\n",
    "    \n",
    "    plt.plot([-3, 3], np.array([g_pppp_max, g_pppp_max])*h**2/12, ls='-.', c='r', label=r\"$M_gh^2/12$\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00edaf2a-0f0f-4860-aaeb-cfa3cd5648ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "%reset -f out \n",
    "\n",
    "# Check out what happens when $h<1e-4$\n",
    "\n",
    "interact_manual(plot_gpp_error_and_bounds, \n",
    "            g = fixed(g_eval),\n",
    "            g_pp = fixed(g_pp_eval),\n",
    "            g_pppp_max = fixed(g_pppp_max),\n",
    "            h=widgets.FloatLogSlider(value=0.01, min=-9, max=-1, base=10, step=1),\n",
    "            n=widgets.IntSlider(value=100, min=50, max=1000, step=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e21a18-20cf-4548-90e5-d933c1cf9643",
   "metadata": {},
   "source": [
    "<mark>Students are encouraged to use the approach shown in [Section 1.2](https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations/blob/main/Chp1/Chp1Sec2.ipynb) for estimating rates of convergence for $0.1>h>1e-7$ (and also $0.1>h>1e-4$). </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e643db7-5b2a-4509-9554-591f4b81e9a5",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id='Section2.2.2'>Section 2.2.2: The finite difference approximation of $-u''(x)=f(x)$ for $x\\in(0,1)$ with $u(0)=u(1)=0$</a>\n",
    "---\n",
    "\n",
    "The steps/ideas for deriving the \"prototypical\" finite difference approximation we seek are as follows.\n",
    "\n",
    "1. Partition $[0,1]$ into $n+1$ subintervals of equal length $h:=1/(n+1)$. \n",
    "\n",
    "   This gives $n+2$ equally spaced points, denoted by $\\{x_j\\}_{j=0}^{n+1}$, where $x_j=jh$.\n",
    "   \n",
    "   **Why use $n+1$ subintervals instead of $n$ subintervals?** Remember, we know $u(x_0)=u(0)=0$ and $u(x_{n+1})=u(1)=1$. Thus, this is just another way of saying that we want to consider $n$ interior points $x_1,\\ldots, x_n$ corresponding to the unknown values of $u(x_j)$ that we seek to approximate. If we used $n$ subintervals, then we would have $n-1$ unknown values. Thus, using $n+1$ subintervals is just an elegant way of not having to write $n-1$ to describe the number of unknowns.\n",
    "   \n",
    "   Let $\\{v_j\\}_{j=0}^{n+1}$ define the approximation of $u$ at the points $\\{x_j\\}_{j=0}^{n+1}$. \n",
    "   \n",
    "2. How should we determine $v\\in\\mathbb{R}^{n+1}$ so that $v_j \\approx u(x_j)$?\n",
    "\n",
    "   By requiring that $v_0=0=v_{n+1}$, there is no approximation error at the endpoints. Fantastic. \n",
    "   \n",
    "   While we don't know what $u$ is at $x_1,\\ldots, x_n$, we do know that $-u''=f$ at all of these points. So, we use the centered finite difference approximation derived above in [Section 2.2.1](#Section2.2.1) and require that\n",
    "   \n",
    "   $$\n",
    "       - \\frac{v_{j-1} - 2v_j + v_{j+1}}{h^2} = f(x_j), \\ \\text{ for } \\ j=1,\\ldots, n.\n",
    "   $$\n",
    "\n",
    "   Note that this gives $n$ equations for $n$ unknowns and since we required $v_0=0=v_n$, then for the $j=1$ and $j=n$ equations, we actually have\n",
    "   \n",
    "   $$\n",
    "       - \\frac{-2v_1 + v_2}{h^2} = f(x_1), \\ \\text{ and } \\ - \\frac{v_{n-1} - 2v_n }{h^2} = f(x_n).\n",
    "   $$\n",
    "   \n",
    "   <mark>An important note is that the above two questions are clearly the ones to change if we have different boundary conditions or values applied to the problem.</mark>\n",
    "\n",
    "3. We can organize the system of $n$ equations for $n$ unknowns as the matrix-vector equation\n",
    "<br><br>\n",
    "$$\n",
    "\\large Av=b, \\ \\text{ where } \\ A = \\begin{pmatrix}\n",
    "                    2 & -1 & 0 & \\cdots & 0 \\\\\n",
    "                    -1 & 2 & -1 & \\ddots & \\vdots \\\\\n",
    "                    0 & \\ddots & \\ddots & \\ddots & 0 \\\\\n",
    "                    \\vdots & \\ddots & -1 & 2 & -1 \\\\\n",
    "                    0 & \\cdots & 0 & -1 & 2\n",
    "                \\end{pmatrix},\n",
    "            \\\n",
    "            \\\n",
    "             b = h^2\\begin{pmatrix} \n",
    "                        f(x_1) \\\\\n",
    "                        f(x_2) \\\\\n",
    "                        \\vdots \\\\\n",
    "                        f(x_n)\n",
    "                    \\end{pmatrix}\n",
    "$$\n",
    "<br><br>\n",
    "and $v\\in\\mathbb{R}^n$ represents $v=(v_1, v_2, \\ldots, v_n)^\\top\\approx (u(x_1), u(x_2), \\ldots, u(x_n))^\\top$.\n",
    "\n",
    "4. We analyze the accuracy and convergence rate of this method using the method of manufactured solutions and the error function \n",
    "<br><br>\n",
    "$$\n",
    "    \\large E_h := \\max_{1\\leq j\\leq n} |u(x_j) - v_j|.\n",
    "$$\n",
    "<br><br>\n",
    "Note that if we change one of the boundary condition types (e.g., to Neumann or Robin), then we would adjust the system of equations by adding either one or two more equations that we need to solve to determine $v_0$ or $v_{n+1}$ (possibly both), and we should similarly adjust the definition of $E_h$ to include the approximation error we make at these endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfafa445-d577-42b0-8622-f8fb6870c1fd",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 2.2.3: General code implementation and the method of manufactured solutions\n",
    "---\n",
    "\n",
    "- We create a function `make_A` for making the matrix $A$. \n",
    "\n",
    "  We could use sparse matrix packages for this since most of the components of $A$ are zero, but we eschew that for now to keep things as \"familiar\" as possible (at least initially) by using standard `numpy` arrays. This is less memory efficient, but we are not going to deal with $n$ values that make this problematic.\n",
    "\n",
    "- We do *not* create a function for making $b$ because we assume we can evaluate $f$ on an array to get an array. In other words, the function $f$ serves to make $b$ directly.\n",
    "\n",
    "- As discussed previously in other notebooks, the method of manufactured solutions refers to the approach where we verify and study code by starting with a solution to the problem (i.e., start with some $u$ that we \"manufacture\" essentially out of thin air), derive the problem conditions that would lead to such a manufactured solution (i.e., plug the solution into the problem to determine what the problem needs to look like to give that solution), and then we *know* how to compute the error to check the code/approach because we actually have the *exact* solution.\n",
    "\n",
    "We show all of this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e97b21-a025-4ded-bcb3-8ed7b74d2619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_A(n):\n",
    "    A = np.zeros((n,n))\n",
    "    np.fill_diagonal(A,2)\n",
    "    A += np.diag(-np.ones(n-1),k=1)\n",
    "    A += np.diag(-np.ones(n-1),k=-1)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cff491-f656-4bc2-af06-2c7760968ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5  # just for illustrative purposes\n",
    "A = make_A(n)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a7612b-1e50-43a1-a390-9f08cd132e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we make $u$ the $g$ we saw in Section 2.1.1 above\n",
    "x = sp.symbols('x')\n",
    "u = sp.exp(-x**2)*sp.sin(sp.pi * x)\n",
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9972552b-33a7-414b-86e7-be076c7ba6ad",
   "metadata": {},
   "source": [
    "The $\\sin(\\pi x)$ is zero at both $x=0$ and $x=1$, so $u(0)=u(1)=0$, which we can also verify below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f426b3c6-baad-4de2-bbcf-f5b14c6711d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "u.subs({x:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870111ed-cef9-42bc-92f7-a2b943017466",
   "metadata": {},
   "outputs": [],
   "source": [
    "u.subs({x:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584cfb86-4f89-4b1a-b0b7-80029a4286c0",
   "metadata": {},
   "source": [
    "We need $-u''=f$, so let's figure out what $f$ is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9599451b-190a-4d7e-b3e1-39416b2eb0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = -u.diff(x, 2)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45df5a4b-b2c1-4c4f-9f9d-e0605b450640",
   "metadata": {},
   "source": [
    "Now we can `lambdify` this $f$ to construct our $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be283a-963c-4ad6-87a6-ba7b62dcb8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_eval = lambdify(x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a80cf35-2cbd-4fe9-a831-c23ee9141e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_Av_b(n, f):\n",
    "    \n",
    "    A = make_A(n)\n",
    "    \n",
    "    x = np.linspace(0, 1, n+2)\n",
    "    h = x[1]-x[0]\n",
    "    b = h**2*f(x[1:-1])\n",
    "    \n",
    "    v = np.zeros(n+2)\n",
    "    v[1:-1] = np.linalg.solve(A, b)\n",
    "    \n",
    "    return v, x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0604f1-617e-4431-bc68-0f6b993af6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_v_u(n, f, u=None, fignum=2):\n",
    "    \n",
    "    plt.figure(fignum)\n",
    "    plt.clf()\n",
    "    \n",
    "    v, x, h = solve_Av_b(n, f)\n",
    "    \n",
    "    if u is not None:\n",
    "        plt.plot(x, u(x), c='b', label='$u$')\n",
    "    \n",
    "    plt.plot(x, v, ls=':', c='r', label='$v$')\n",
    "    \n",
    "    E_h = np.linalg.norm(np.abs(u(x) - v))  # Two wasted computations here, oh well.\n",
    "    \n",
    "    plt.title(r'$h\\approx${:1.3f} for $n=${:d} gives $E_h\\approx$ {:1.2e}'.format(h, n, E_h) )\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4828c1-6aae-4238-aecd-479e5b4e2d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "%reset -f out \n",
    "\n",
    "# Check out what happens when $h<1e-4$\n",
    "\n",
    "interact_manual(plot_v_u, \n",
    "            n=widgets.IntSlider(value=5, min=3, max=100, step=1),\n",
    "            f = fixed(f_eval),\n",
    "            u = fixed(lambdify(x, u)),\n",
    "            fignum = fixed(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2820acf6-fefd-4163-998a-df9b8ad3a5e4",
   "metadata": {},
   "source": [
    "---\n",
    "#### Student Activity\n",
    "---\n",
    "\n",
    "What if a function that we want to use as a manufactured solution does not satisfy the BCs? \n",
    "\n",
    "We can adjust any $w\\in\\mathcal{C}^2((0,1))$ that has finite $w(0)$ and $w(1)$ to satisfy the BCs by simply defining $u(x)=w(x)-\\ell(x)$ where $\\ell(x)$ is the linear function whose graph corresponds to the line connecting $w(0)$ to $w(1)$. \n",
    "\n",
    "Conceptually, this defines a $u$ that is essentially $w$ except the $x$-axis can be thought of as being \"re-defined\" to be the line given by the graph of $\\ell(x)$. \n",
    "\n",
    "1. For the following $w$, define the appropriate $u$ as a manufactured solution and compare $u$ to $v$ as was done above. (We will do a few in class.)\n",
    "\n",
    "2. Make loglog-plots of $E_h$ vs. $h$ and compute the rate of convergence. (Students are left to do this on their own.)\n",
    "\n",
    "The $w$ are:\n",
    "\n",
    "(a) $w(x)=x^3$.\n",
    "\n",
    "(b) $w(x)=e^x\\sin(3\\pi x/2)$.\n",
    "\n",
    "(c) $w(x)=\\tan(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee03882-597e-4fa4-8133-a7baa4c34dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318e451-4664-4317-8921-b036762a84a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1536e1b1-36a3-43b5-b248-34c36128f768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db803ea0-058c-415d-b793-f934c63ce277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7d805f8-982f-4136-9f13-0386774bc0e2",
   "metadata": {},
   "source": [
    "**A suggestion (and possibly a homework problem)**\n",
    "\n",
    "<mark>Students are encouraged to build upon the `solve_2pt_BVP_Dirichlet` class from [Section 2.1](Chp2Sec1.ipynb) to include numerical estimates of $u$ as well. This should include new data attributes (e.g., $n$, $h$, $v$ and $A$) and method attributes such as `make_A` and `solve_Av_b`.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab475eb-71b5-4229-bbab-23c0ef62b65b",
   "metadata": {},
   "source": [
    "---\n",
    "### <a id='Section2.2.3'>Section 2.2.4: Some properties of $A$</a>\n",
    "---\n",
    "\n",
    "Here, we simply list some important and useful properties of $A$ for the particular problem considered in this section. Students may want to refer to [Section 1.5](https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations/blob/main/Chp1/Chp1Sec5.ipynb) for the linear algebra details.\n",
    "\n",
    "- $A$ is symmetric positive definite so is nonsingular and has all positive real eigenvalues.\n",
    "\n",
    "  - This implies that $v$ always exist and is unique for any $b\\in\\mathbb{R}^n$. \n",
    "  \n",
    "  - This also implies that $v$ exists even when $b$ is not associated with any meaningful/practical $f$ for which a solution $u$ can even be considered.\n",
    "  \n",
    "Wait, what does it mean that $v$ exists even if $f$ is complete nonsense?\n",
    "\n",
    "Imagine we define $f$ as follows:\n",
    "\n",
    "- For any given $x$, take the first 3 decimals to set the random seed and generate $f(x)$ as the first normally distributed random number generated fromtaht seed. This $f$ is nonsense and \"arbitrarily rough\" (i.e., not smooth or even continuous in the slightest). Yet, $f(x)$ is well-defined and we can produce a $v$ for any given $n$ even though $u$ fails to exist in any meaningful way.\n",
    "\n",
    "- For any given $x$, define $f(x)=0$ if $x$ is rational and $f(x)=1$ if $x$ is irrational (or vice versa). Then $f$ is *mostly* $1$, but is discontinuous everywhere. Yet, on a computer, $f(x)=0$ for all $x$ we can \"compute\" so $b$ is always the zero vector and $v$ will also always be the zero vector despite $u$ failing to exist as a function in $\\mathcal{C}^2((0,1))$. However, we can define a $u\\in\\mathcal{C}^2((0,1))$ such that $-u''=f$ almost everywhere. Consider $u(x)=\\frac{1}{2}x(1-x)$, which is very clearly not zero anywhere in $(0,1)$ despite $v$ always being that way.\n",
    "\n",
    "What are the lessons here?\n",
    "\n",
    "- Just because we solve a discrete problem and get a nice answer does not mean that the continuous problem is reasonable, or at least it may mean that what it means to solve the continuous problem has to be considered from a more sophisticated perspective. (Consider this an advertisement for a more modern PDEs course.)\n",
    "\n",
    "- We should really consider solutions to the continuous problems from a theoretical perspective *first* and then consider numerical approximations *second*. By this, I mean it is actually practically important to have theoretical justifications for the solution to the *exact* problem existing and being unique before we go about numerically approximating a solution that either does not exist or does not exist in the sense that we designed the numerical method to consider. \n",
    "\n",
    "  - Using finite element methods and modern PDE theory we can handle both of the scenarios \"imagined\" above. \n",
    "  \n",
    "    In the first case, the modern PDE theory would basically tell us, \"This is a stupid problem with no solution.\" This is good to know. Not all problems have solutions. This probably means that if this was a model for some application, then you either have something fundamentally wrong with the model or you have some fundamental misunderstanding of the application. Whatever it is, the application and model clearly require further study.\n",
    "    \n",
    "    In the second case, modern PDE theory says the solution exists and is unique in the sense I described it above, but it won't tell us what it is. This is where finite element methods come in that use a Lebesgue integral to \"smooth out\" the roughness of the function $f$ by actually identifying it as equivalent to $f(x)\\equiv 1$ (there is a delicate interplay between the finite element method and the \"weak\" or \"variational\" form of the PDE that we would study in modern PDE theory) and the numerical implementation would reveal a new type of $v$ that actually approximates the $u$ quite well. \n",
    "    \n",
    "- The meta-lesson here is that your journey is not over with this class. There is still much to learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a48084-b8e2-4966-b2de-cc91e78faf99",
   "metadata": {},
   "source": [
    "---\n",
    "#### Navigation:\n",
    "\n",
    "- [Previous](https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations/blob/main/Chp2/Chp2Sec1.ipynb)\n",
    "\n",
    "- [Next](https://github.com/CU-Denver-MathStats-OER/Intro-PDEs-Theory-and-Computations/blob/main/Chp2/Chp2Sec3.ipynb)\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
